#!/usr/bin/env python3
import argparse, json, os, shutil, subprocess, sys, pathlib, datetime, re, tempfile, textwrap, tarfile, time, shlex, hashlib, io, contextlib, traceback, statistics
import xml.etree.ElementTree as ET
from typing import Any, Dict, Iterable, List, Optional, Sequence, Set, Tuple
from urllib.request import pathname2url

# Import template engine
sys.path.insert(0, str(pathlib.Path(__file__).parent.parent / "modules"))
try:
    from template_engine import TemplateEngine
    TEMPLATE_ENGINE_AVAILABLE = True
except ImportError:
    TEMPLATE_ENGINE_AVAILABLE = False

try:
    from strict_build import (
        DEFAULT_BUILD_TYPE as STRICT_DEFAULT_BUILD_TYPE,
        DEFAULT_GENERATOR as STRICT_DEFAULT_GENERATOR,
        DEFAULT_STD as STRICT_DEFAULT_STD,
        DEFAULT_TIDY_CHECKS as STRICT_DEFAULT_TIDY_CHECKS,
        SANITIZER_FLAGS,
        STRICT_WARNING_FLAGS,
        strict_build_command,
        strict_configure_command,
        strict_test_command,
    )
except ImportError:
    STRICT_DEFAULT_STD = "23"
    STRICT_DEFAULT_BUILD_TYPE = "Debug"
    STRICT_DEFAULT_GENERATOR = "Ninja"
    STRICT_DEFAULT_TIDY_CHECKS = "cppcoreguidelines-*"
    STRICT_WARNING_FLAGS = ["-Wall", "-Wextra", "-Wconversion", "-Wshadow", "-Werror"]
    SANITIZER_FLAGS = ["-fsanitize=address", "-fsanitize=undefined"]

    def strict_configure_command(*args, **kwargs):  # type: ignore
        raise RuntimeError("strict_build module unavailable")

    def strict_build_command(*args, **kwargs):  # type: ignore
        raise RuntimeError("strict_build module unavailable")

    def strict_test_command(*args, **kwargs):  # type: ignore
        raise RuntimeError("strict_build module unavailable")

ROOT = pathlib.Path(__file__).resolve().parent.parent
PROJECT_ROOT = pathlib.Path.cwd().resolve()

def get_available_presets():
    """Get list of available presets from template engine or fallback to legacy"""
    if TEMPLATE_ENGINE_AVAILABLE:
        try:
            template_dir = ROOT / "templates"
            manifest_dir = ROOT / "manifest"
            if template_dir.exists() and manifest_dir.exists():
                engine = TemplateEngine(template_dir, manifest_dir)
                return engine.get_available_presets()
        except Exception as e:
            print(f"Warning: Failed to load template engine: {e}")

    # Fallback to legacy presets
    return ["minimal", "full", "library"]

def get_version() -> str:
    try:
        vfile = ROOT / "VERSION"
        if vfile.exists():
            return vfile.read_text().strip()
    except Exception:
        pass
    try:
        res = subprocess.run(["git", "-C", str(ROOT), "describe", "--tags", "--always"], text=True, capture_output=True)
        if res.returncode == 0 and res.stdout.strip():
            return res.stdout.strip()
    except Exception:
        pass
    return "0.0.0+unknown"
# Always write artifacts under the current working directory
EXPORTS = pathlib.Path.cwd() / "exports"
MODULES = ROOT / "modules"
EXPORTS.mkdir(exist_ok=True)

def run(cmd, cwd=None, check=True, env=None):
    return subprocess.run(cmd, cwd=cwd, text=True, capture_output=True, check=check, env=env)

def write_json(path: pathlib.Path, data):
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, indent=2))

def validate_cmake_guidance(project_dir=None):
    """Validate CMakeLists.txt compliance with CMAKE_GUIDANCE.md patterns"""
    if project_dir is None:
        project_dir = pathlib.Path.cwd()
    else:
        project_dir = pathlib.Path(project_dir)

    cmake_file = project_dir / "CMakeLists.txt"

    result = {
        "cmake_file_exists": cmake_file.exists(),
        "compliance": {
            "compile_commands_export": False,
            "project_warnings_interface": False,
            "project_sanitizers_interface": False,
            "json_diagnostics": False,
            "cxx_standard_set": False,
            "lint_target": False
        },
        "suggestions": [],
        "overall_score": 0
    }

    if not cmake_file.exists():
        result["suggestions"].append("Create CMakeLists.txt with 'llmtk init'")
        return result

    try:
        cmake_content = cmake_file.read_text()

        # Check for required patterns from CMAKE_GUIDANCE.md
        patterns = {
            "compile_commands_export": "CMAKE_EXPORT_COMPILE_COMMANDS ON",
            "project_warnings_interface": "add_library(project_warnings INTERFACE)",
            "project_sanitizers_interface": "add_library(project_sanitizers INTERFACE)",
            "json_diagnostics": "fdiagnostics-format=json",
            "cxx_standard_set": "CMAKE_CXX_STANDARD",
            "lint_target": "add_custom_target(lint"
        }

        for key, pattern in patterns.items():
            if pattern in cmake_content:
                result["compliance"][key] = True
            else:
                # Add specific suggestions based on missing patterns
                if key == "compile_commands_export":
                    result["suggestions"].append("Add 'set(CMAKE_EXPORT_COMPILE_COMMANDS ON)' for clangd support")
                elif key == "project_warnings_interface":
                    result["suggestions"].append("Create project_warnings INTERFACE library for consistent warnings")
                elif key == "json_diagnostics":
                    result["suggestions"].append("Add JSON diagnostics support for LLM-friendly error output")
                elif key == "lint_target":
                    result["suggestions"].append("Add custom lint target for syntax-only checking")

        # Calculate overall compliance score
        compliant_count = sum(result["compliance"].values())
        total_checks = len(result["compliance"])
        result["overall_score"] = (compliant_count / total_checks) * 100

    except Exception as e:
        result["suggestions"].append(f"Error reading CMakeLists.txt: {e}")

    return result

def load_yaml(path: pathlib.Path):
    # Try PyYAML, then yq, else return None
    try:
        import yaml  # type: ignore
        with open(path, 'r') as f:
            return yaml.safe_load(f)
    except Exception:
        pass
    if shutil.which("yq"):
        try:
            conv = subprocess.run(["yq", "-o=json", str(path)], text=True, capture_output=True)
            if conv.returncode == 0 and conv.stdout:
                return json.loads(conv.stdout)
        except Exception:
            pass
    return None

def generate_reference_md(out_path: pathlib.Path):
    tools_manifest = ROOT/"manifest"/"tools.yaml"
    commands_manifest = ROOT/"manifest"/"commands.yaml"
    tools = load_yaml(tools_manifest)
    commands = load_yaml(commands_manifest)

    now = datetime.datetime.now(datetime.UTC).isoformat()
    lines = []
    lines.append("# Toolkit Reference")
    lines.append("")
    lines.append(f"Generated from manifests on {now}.")
    lines.append("")

    # Tools
    lines.append("## Tools")
    if tools and isinstance(tools, dict) and "tools" in tools:
        for name in sorted(tools["tools"].keys()):
            t = tools["tools"][name] or {}
            version = t.get("version")
            provides = t.get("provides") or []
            check = None
            if isinstance(t.get("check"), dict):
                check = " ".join(map(str, t["check"].get("cmd", [])))
            lines.append(f"- {name}")
            if version: lines.append(f"  - version: {version}")
            if provides: lines.append(f"  - provides: {', '.join(provides)}")
            if check: lines.append(f"  - check: `{check}`")
    else:
        # Fallback: embed raw YAML for visibility
        try:
            raw = tools_manifest.read_text()
            lines.append("````yaml")
            lines.append(raw.rstrip())
            lines.append("````")
        except Exception:
            lines.append("(tools manifest not found)")

    lines.append("")
    lines.append("## Commands")
    if commands and isinstance(commands, dict) and "commands" in commands:
        for name in sorted(commands["commands"].keys()):
            c = commands["commands"][name] or {}
            lines.append(f"- {name}")
            if c.get("description"): lines.append(f"  - description: {c['description']}")
            args = c.get("args") or []
            if args:
                arg_summaries = []
                for a in args:
                    if isinstance(a, dict):
                        nm = a.get("name")
                        req = a.get("required", False)
                        var = a.get("variadic", False)
                        frag = nm or "arg"
                        if req: frag += " (required)"
                        if var: frag += " (variadic)"
                        arg_summaries.append(frag)
                if arg_summaries:
                    lines.append(f"  - args: {', '.join(arg_summaries)}")
            runs = c.get("runs") or []
            if runs: lines.append(f"  - runs: {', '.join(runs)}")
            outs = c.get("outputs") or []
            if outs:
                for o in outs:
                    if isinstance(o, str):
                        lines.append(f"  - output: {o}")
                    elif isinstance(o, dict):
                        p = o.get("path") or o.get("file") or "(unknown)"
                        lines.append(f"  - output: {p}")
                        schema = o.get("schema")
                        if schema is not None:
                            try:
                                rendered = json.dumps(schema, indent=2)
                            except Exception:
                                rendered = str(schema)
                            lines.append("    schema:")
                            lines.append("    ```json")
                            for l in rendered.splitlines():
                                lines.append("    " + l)
                            lines.append("    ```")
            if c.get("json_summary"): lines.append(f"  - json_summary: {c['json_summary']}")
    else:
        try:
            raw = commands_manifest.read_text()
            lines.append("````yaml")
            lines.append(raw.rstrip())
            lines.append("````")
        except Exception:
            lines.append("(commands manifest not found)")

    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("\n".join(lines) + "\n")



def generate_capabilities_json(out_path: pathlib.Path):
    """Emit a machine-readable capabilities summary for agents."""
    tools_manifest = ROOT/"manifest"/"tools.yaml"
    commands_manifest = ROOT/"manifest"/"commands.yaml"
    tools = load_yaml(tools_manifest) or {}
    commands = load_yaml(commands_manifest) or {}

    data = {
        "$schema": f"https://llmtk.ai/schemas/capabilities-v1.json",
        "_meta": {
            "generated_at": datetime.datetime.now(datetime.UTC).isoformat(),
            "toolkit_version": get_version(),
            "tools_manifest": str(tools_manifest),
            "commands_manifest": str(commands_manifest),
        },
        "tools": {},
        "commands": {},
    }

    tools_section = tools.get("tools") if isinstance(tools, dict) else None
    if isinstance(tools_section, dict):
        for name, entry in tools_section.items():
            entry = entry or {}
            data["tools"][name] = {
                "version": entry.get("version"),
                "min_version": entry.get("min_version"),
                "provides": entry.get("provides") or [],
                "role": entry.get("role", "optional"),
                "invocation": entry.get("invocation") or {},
                "install": entry.get("install") or {},
                "check": entry.get("check") or {},
                "fallbacks": entry.get("fallbacks") or [],
                "local_install": entry.get("local_install") or None,
            }

    commands_section = commands.get("commands") if isinstance(commands, dict) else None
    if isinstance(commands_section, dict):
        for name, entry in commands_section.items():
            entry = entry or {}
            data["commands"][name] = {
                "description": entry.get("description"),
                "args": entry.get("args") or [],
                "runs": entry.get("runs") or [],
                "outputs": entry.get("outputs") or [],
                "json_summary": entry.get("json_summary"),
                "examples": entry.get("examples") or [],
            }

    write_json(out_path, data)
    return out_path


def _ctest_filter_args(args: argparse.Namespace) -> List[str]:
    filters: List[str] = []
    if getattr(args, "regex", None):
        filters.extend(["-R", args.regex])
    if getattr(args, "exclude", None):
        filters.extend(["-E", args.exclude])
    if getattr(args, "label", None):
        filters.extend(["-L", args.label])
    return filters


def _resolve_optional_path(default: pathlib.Path, override: Optional[str]) -> pathlib.Path:
    if override is None or override == "":
        return default
    return pathlib.Path(override)


def _describe_ctest_preview(stdout: str) -> Dict[str, Any]:
    try:
        data = json.loads(stdout)
    except json.JSONDecodeError:
        return {"raw": stdout.strip()}

    tests = data.get("tests") or []
    summary = {
        "total": len(tests),
        "by_label": {},
        "tests": [],
    }

    for entry in tests:
        labels = entry.get("labels") or []
        for label in labels:
            summary["by_label"].setdefault(label, 0)
            summary["by_label"][label] += 1
        summary["tests"].append({
            "name": entry.get("name"),
            "command": entry.get("command"),
            "timeout": entry.get("properties", {}).get("timeout")
        })

    return summary


def _find_latest_ctest_xml(build_dir: pathlib.Path) -> Optional[pathlib.Path]:
    testing_dir = build_dir / "Testing"
    if not testing_dir.exists():
        return None

    latest_xml: Optional[pathlib.Path] = None
    latest_mtime = -1.0

    for candidate in testing_dir.iterdir():
        if not candidate.is_dir():
            continue
        xml_path = candidate / "Test.xml"
        if not xml_path.exists():
            continue
        try:
            mtime = xml_path.stat().st_mtime
        except OSError:
            continue
        if mtime > latest_mtime:
            latest_mtime = mtime
            latest_xml = xml_path

    return latest_xml


def _normalize_status(raw_status: Optional[str]) -> str:
    if not raw_status:
        return "unknown"
    status = raw_status.strip().lower()
    if status in {"passed", "success", "completed"}:
        return "passed"
    if status in {"failed", "fail"}:
        return "failed"
    if status in {"timeout"}:
        return "timeout"
    if status in {"notrun", "not run", "disabled"}:
        return "notrun"
    if status in {"skipped"}:
        return "skipped"
    return status.replace(" ", "_")


def _parse_ctest_xml(xml_path: pathlib.Path) -> Dict[str, Any]:
    try:
        tree = ET.parse(xml_path)
    except ET.ParseError as exc:
        return {"error": f"Failed to parse {xml_path}: {exc}"}

    root = tree.getroot()
    tests: List[Dict[str, Any]] = []
    stats: Dict[str, int] = {"passed": 0, "failed": 0, "timeout": 0, "notrun": 0, "skipped": 0, "unknown": 0}
    total_duration = 0.0

    for test_el in root.findall('.//Test'):
        status = _normalize_status(test_el.attrib.get('Status'))
        name = test_el.findtext('Name') or test_el.attrib.get('Name') or "(unknown)"
        full_name = test_el.findtext('FullName') or name
        path_text = test_el.findtext('Path')
        command = test_el.findtext('FullCommandLine')
        labels = [lab.text.strip() for lab in test_el.findall('./Labels/Label') if lab.text]

        results = test_el.find('Results')
        measurements: Dict[str, Any] = {}
        output_snippets: List[str] = []
        execution_time: Optional[float] = None
        completion_status: Optional[str] = None

        if results is not None:
            for named in results.findall('NamedMeasurement'):
                key = named.attrib.get('name') or ''
                value = named.findtext('Value') or ''
                measurements[key] = value
                key_lower = key.lower()
                if key_lower == 'execution time':
                    try:
                        execution_time = float(value)
                        total_duration += execution_time
                    except ValueError:
                        pass
                elif key_lower == 'completion status':
                    completion_status = value
                elif key_lower in {'test output', 'exit condition', 'stdout', 'stderr'}:
                    if value:
                        output_snippets.append(value)
                elif key_lower in {'command'} and not command:
                    command = value

            for measurement in results.findall('Measurement'):
                value = measurement.findtext('Value')
                if value:
                    output_snippets.append(value)

            for failure in results.findall('.//Failure'):
                parts = [part.strip() for part in failure.itertext() if part and part.strip()]
                if parts:
                    output_snippets.append(' '.join(parts))

        if status == 'unknown' and completion_status:
            status = _normalize_status(completion_status)

        if status not in stats:
            stats[status] = 0
        stats[status] += 1

        fail_reason = None
        if status not in {'passed'}:
            fail_reason = measurements.get('Completion Status') or measurements.get('Exit Value') or measurements.get('Exit Code')
            if not fail_reason and output_snippets:
                fail_reason = output_snippets[0]

        pass_reason = None
        if status == 'passed':
            pass_reason = measurements.get('Completion Status') or 'Completed'

        tests.append({
            'name': name,
            'full_name': full_name,
            'status': status,
            'path': path_text,
            'command': command,
            'labels': labels,
            'duration': execution_time,
            'measurements': measurements,
            'output_snippets': output_snippets[:5],
            'fail_reason': fail_reason,
            'pass_reason': pass_reason,
        })

    total = sum(stats.values())
    failures = [test for test in tests if test['status'] not in {'passed'}]

    return {
        'tests': tests,
        'stats': {
            'total': total,
            'passed': stats.get('passed', 0),
            'failed': stats.get('failed', 0),
            'timeout': stats.get('timeout', 0),
            'notrun': stats.get('notrun', 0),
            'skipped': stats.get('skipped', 0),
            'unknown': stats.get('unknown', 0),
            'duration_seconds': round(total_duration, 4),
        },
        'failures': failures,
    }


def _fallback_parse_ctest_output(output: str) -> Dict[str, Any]:
    failed_tests: List[Dict[str, Any]] = []
    total = passed = failed = 0
    summary_re = re.compile(r"(\d+)% tests passed, (\d+) tests failed out of (\d+)")
    listing_re = re.compile(r"\s*(\d+)\s*-\s*(.+?)\s+\(")

    in_failed_section = False
    for line in output.splitlines():
        stripped = line.strip()
        match = summary_re.search(stripped)
        if match:
            _, failed_str, total_str = match.groups()
            total = int(total_str)
            failed = int(failed_str)
            passed = total - failed
        if 'The following tests FAILED:' in stripped:
            in_failed_section = True
            continue
        if in_failed_section:
            if not stripped:
                in_failed_section = False
                continue
            match = listing_re.match(line)
            if match:
                idx, name = match.groups()
                failed_tests.append({'name': name.strip(), 'index': int(idx)})

    tests = [{
        'name': item.get('name'),
        'full_name': item.get('name'),
        'status': 'failed',
        'path': None,
        'command': None,
        'labels': [],
        'duration': None,
        'measurements': {},
        'output_snippets': [],
        'fail_reason': 'CTest reported failure (XML unavailable)',
        'pass_reason': None,
    } for item in failed_tests]

    return {
        'tests': tests,
        'stats': {
            'total': total,
            'passed': passed,
            'failed': failed,
            'timeout': 0,
            'notrun': 0,
            'skipped': 0,
            'unknown': max(total - (passed + failed), 0),
            'duration_seconds': 0.0,
        },
        'failures': tests,
        'warning': 'Structured XML not found; derived summary from stdout only.'
    }


def _load_json_lines(path: pathlib.Path) -> List[Dict[str, Any]]:
    records: List[Dict[str, Any]] = []
    if not path.exists():
        return records
    for raw in path.read_text().splitlines():
        raw = raw.strip()
        if not raw:
            continue
        try:
            obj = json.loads(raw)
        except json.JSONDecodeError:
            continue
        if isinstance(obj, dict):
            records.append(obj)
    return records


def _ninja_log_raw_lines(path: pathlib.Path) -> Set[str]:
    lines: Set[str] = set()
    if not path.exists():
        return lines
    with open(path, 'r', encoding='utf-8', errors='replace') as fh:
        for line in fh:
            stripped = line.rstrip()
            if not stripped or stripped.startswith('#'):
                continue
            lines.add(stripped)
    return lines


def _ninja_log_new_entries(path: pathlib.Path, baseline: Optional[Set[str]] = None) -> Tuple[List[Dict[str, Any]], Set[str]]:
    entries: List[Dict[str, Any]] = []
    seen: Set[str] = set()
    if not path.exists():
        return entries, seen

    with open(path, 'r', encoding='utf-8', errors='replace') as fh:
        for line in fh:
            raw = line.rstrip()
            if not raw or raw.startswith('#'):
                continue
            seen.add(raw)
            if baseline and raw in baseline:
                continue
            parts = raw.split('\t')
            if len(parts) < 4:
                continue
            try:
                start = int(parts[0])
                end = int(parts[1])
            except ValueError:
                continue
            duration = max(end - start, 0)
            output = parts[3] if len(parts) >= 4 else parts[-1]
            entries.append({
                'start': start,
                'end': end,
                'duration_ms': duration,
                'output': output,
                'raw': raw,
            })
    entries.sort(key=lambda item: item['end'])
    return entries, seen


def _compute_parallelism(entries: Sequence[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    if not entries:
        return None

    min_start = min(item['start'] for item in entries)
    max_end = max(item['end'] for item in entries)
    makespan_ms = max(max_end - min_start, 0)
    total_duration_ms = sum(item.get('duration_ms', 0) for item in entries)

    average_parallelism = None
    makespan_seconds = None
    total_duration_seconds = None
    if makespan_ms > 0:
        makespan_seconds = makespan_ms / 1000.0
        total_duration_seconds = total_duration_ms / 1000.0
        if makespan_seconds:
            average_parallelism = total_duration_seconds / makespan_seconds

    events: List[Tuple[int, int]] = []
    for item in entries:
        events.append((int(item.get('start', 0)), 1))
        events.append((int(item.get('end', 0)), -1))
    events.sort()

    concurrency = 0
    max_concurrency = 0
    for _, delta in events:
        concurrency += delta
        if concurrency > max_concurrency:
            max_concurrency = concurrency

    return {
        'entries': len(entries),
        'makespan_seconds': round(makespan_seconds, 4) if makespan_seconds is not None else None,
        'total_duration_seconds': round(total_duration_seconds, 4) if total_duration_seconds is not None else None,
        'average_parallelism': round(average_parallelism, 3) if average_parallelism is not None else None,
        'max_parallelism': max_concurrency,
    }


def _load_compile_command_index(path_hint: Optional[pathlib.Path] = None) -> Dict[str, str]:
    compile_db_path = _find_compile_database(path_hint)
    if not compile_db_path or not compile_db_path.exists():
        return {}
    try:
        data = json.loads(compile_db_path.read_text())
    except Exception:
        return {}
    mapping: Dict[str, str] = {}
    if isinstance(data, list):
        for entry in data:
            if not isinstance(entry, dict):
                continue
            file_path = entry.get('file')
            if not file_path:
                continue
            name = pathlib.Path(file_path).name
            mapping.setdefault(name, file_path)
    return mapping


def _slow_translation_units(
    entries: Sequence[Dict[str, Any]],
    compile_index: Dict[str, str],
    *,
    top_n: int = 10,
) -> List[Dict[str, Any]]:
    if not entries:
        return []
    ranked = sorted(entries, key=lambda item: item.get('duration_ms', 0), reverse=True)[:top_n]
    results: List[Dict[str, Any]] = []
    for item in ranked:
        output = item.get('output') or ''
        output_name = pathlib.Path(output).name
        source_name = pathlib.Path(output_name).stem
        source_path = None
        candidate_names = []
        if source_name:
            if pathlib.Path(source_name).suffix:
                candidate_names.append(source_name)
            else:
                candidate_names.extend(source_name + ext for ext in ('.cpp', '.cc', '.cxx', '.c', '.mm', '.m'))
        for candidate in candidate_names:
            if candidate in compile_index:
                source_path = compile_index[candidate]
                break
        results.append({
            'output': output,
            'source': source_path,
            'duration_seconds': round(item.get('duration_ms', 0) / 1000.0, 4),
        })
    return results


def _ctest_status_to_level(status: str) -> str:
    status = status.lower()
    if status in {'failed', 'timeout', 'exception'}:
        return 'error'
    if status in {'notrun', 'skipped', 'disabled'}:
        return 'warning'
    return 'note'


def _write_ctest_sarif(
    sarif_path: pathlib.Path,
    tests: Iterable[Dict[str, Any]],
    command: List[str],
    build_dir: pathlib.Path,
    ctest_version: Optional[str],
    returncode: int,
    duration: float,
) -> None:
    failing = [test for test in tests if test.get('status') not in {'passed'}]
    run = {
        'tool': {
            'driver': {
                'name': 'ctest',
                'version': ctest_version,
            }
        },
        'results': [],
        'invocations': [{
            'executionSuccessful': returncode == 0,
            'commandLine': ' '.join(command),
            'workingDirectory': {'uri': str(build_dir.resolve())},
            'endTimeUtc': datetime.datetime.now(datetime.UTC).isoformat(),
            'durationInSeconds': duration,
        }]
    }

    for test in failing:
        message = test.get('fail_reason') or test.get('pass_reason') or f"Test {test.get('name')} reported status {test.get('status')}"
        level = _ctest_status_to_level(test.get('status', 'unknown'))
        artifact_path = test.get('path')
        if artifact_path:
            uri = pathname2url(str(artifact_path))
        else:
            fallback = test.get('full_name') or test.get('name') or 'unknown'
            uri = f"test://{pathname2url(fallback)}"
        run['results'].append({
            'ruleId': f"ctest::{test.get('full_name')}",
            'level': level,
            'message': {'text': message},
            'locations': [{
                'physicalLocation': {
                    'artifactLocation': {
                        'uri': uri,
                        'uriBaseId': '%SRCROOT%'
                    }
                }
            }],
            'properties': {
                'duration': test.get('duration'),
                'command': test.get('command'),
            }
        })

    sarif_doc = {
        '$schema': 'https://schemastore.azurewebsites.net/schemas/json/sarif-2.1.0.json',
        'version': '2.1.0',
        'runs': [run]
    }
    write_json(sarif_path, sarif_doc)

_TEMPLATE_CHAIN_PATTERNS = [
    re.compile(r"\b(in|note: in) instantiation of", re.IGNORECASE),
    re.compile(r"\brequired from", re.IGNORECASE),
    re.compile(r"\binstantiated from", re.IGNORECASE),
    re.compile(r"\bsubstitution failure", re.IGNORECASE),
]

_SEVERITY_REGEX = re.compile(r"\b(error|warning|note|remark|fatal error)\b", re.IGNORECASE)

_DEFAULT_CONTEXT_BUDGETS = {
    "summary": 1500,
    "focused": 6000,
    "detailed": 16000,
}


def _collapse_template_traces(lines):
    collapsed = []
    omitted = 0
    i = 0
    total = len(lines)
    while i < total:
        line = lines[i]
        if any(pat.search(line) for pat in _TEMPLATE_CHAIN_PATTERNS):
            group = [line]
            i += 1
            while i < total and any(pat.search(lines[i]) for pat in _TEMPLATE_CHAIN_PATTERNS):
                group.append(lines[i])
                i += 1
            if len(group) > 3:
                collapsed.append(group[0])
                collapsed.append(f"... ({len(group) - 2} template frames trimmed)")
                collapsed.append(group[-1])
                omitted += len(group) - 3
            else:
                collapsed.extend(group)
            continue
        collapsed.append(line)
        i += 1
    return collapsed, omitted


def _dedupe_preserve(seq):
    seen = set()
    out = []
    for item in seq:
        if item not in seen and item is not None:
            out.append(item)
            seen.add(item)
    return out


def _normalize_note(note):
    if isinstance(note, str):
        return {"message": note, "file": None, "line": None, "column": None, "level": "note"}
    if isinstance(note, dict):
        loc = note.get("location") or {}
        return {
            "message": note.get("message") or note.get("text") or "",
            "file": note.get("file") or note.get("path") or loc.get("file"),
            "line": note.get("line") or loc.get("line"),
            "column": note.get("column") or loc.get("column"),
            "level": note.get("level") or note.get("severity") or "note",
        }
    return {"message": str(note), "file": None, "line": None, "column": None, "level": "note"}


def _normalize_diag(diag):
    message = diag.get("message") or diag.get("text") or diag.get("description")
    loc = diag.get("location") or {}
    return {
        "level": diag.get("level") or diag.get("severity") or loc.get("level"),
        "message": message or "",
        "file": diag.get("file") or diag.get("path") or loc.get("file"),
        "line": diag.get("line") or loc.get("line"),
        "column": diag.get("column") or loc.get("column"),
        "category": diag.get("category") or diag.get("option"),
        "option": diag.get("option"),
        "notes": [_normalize_note(n) for n in diag.get("notes") or diag.get("children") or []],
        "raw": diag,
    }


def _extract_json_objects(blob):
    decoder = json.JSONDecoder()
    idx = 0
    length = len(blob)
    results = []
    while idx < length:
        ch = blob[idx]
        if ch not in "{[":
            idx += 1
            continue
        try:
            obj, end = decoder.raw_decode(blob, idx)
        except json.JSONDecodeError:
            idx += 1
            continue
        results.append(obj)
        idx = end
    return results


def _parse_structured_diagnostics(text):
    diagnostics = []
    for obj in _extract_json_objects(text):
        if isinstance(obj, dict):
            if "diag" in obj and isinstance(obj["diag"], dict):
                diagnostics.append(_normalize_diag(obj["diag"]))
            elif "diagnostics" in obj and isinstance(obj["diagnostics"], list):
                for item in obj["diagnostics"]:
                    if isinstance(item, dict):
                        diagnostics.append(_normalize_diag(item))
            elif {"level", "message"}.issubset(obj.keys()):
                diagnostics.append(_normalize_diag(obj))
        elif isinstance(obj, list):
            for entry in obj:
                if isinstance(entry, dict) and {"level", "message"}.issubset(entry.keys()):
                    diagnostics.append(_normalize_diag(entry))
    return diagnostics


_TEXT_DIAG_PATTERN = re.compile(
    r"^(?P<file>[^:\n]+)?(?::(?P<line>\d+))?(?::(?P<column>\d+))?:\s*(?P<level>error|warning|note|remark|fatal error)\s*:\s*(?P<message>.*)",
    re.IGNORECASE,
)


def _parse_text_diagnostics(lines):
    diagnostics = []
    for line in lines:
        match = _TEXT_DIAG_PATTERN.match(line.strip())
        if not match:
            continue
        gd = match.groupdict()
        try:
            line_no = int(gd.get("line")) if gd.get("line") else None
        except ValueError:
            line_no = None
        try:
            col_no = int(gd.get("column")) if gd.get("column") else None
        except ValueError:
            col_no = None
        diagnostics.append({
            "level": (gd.get("level") or "").lower(),
            "message": gd.get("message") or "",
            "file": gd.get("file") or None,
            "line": line_no,
            "column": col_no,
            "category": None,
            "option": None,
            "notes": [],
            "raw": line,
        })
    return diagnostics


def _format_location(file, line, column):
    if not file:
        return None
    location = str(file)
    if line is not None:
        location += f":{line}"
        if column is not None:
            location += f":{column}"
    return location


def _format_highlights(diagnostics):
    highlights = []
    for diag in diagnostics:
        level = (diag.get("level") or "info").upper()
        location = _format_location(diag.get("file"), diag.get("line"), diag.get("column"))
        category = diag.get("category") or diag.get("option")
        base = f"{level}: {diag.get('message', '').strip()}"
        if location:
            base = f"{base} ({location})"
        if category:
            base = f"{base} [{category}]"
        highlights.append(base.strip())
        for note in diag.get("notes", []):
            note_level = (note.get("level") or "note").lower()
            note_loc = _format_location(note.get("file"), note.get("line"), note.get("column"))
            note_msg = note.get("message", "").strip()
            frag = f"  {note_level}: {note_msg}" if note_msg else f"  {note_level}"
            if note_loc:
                frag = f"{frag} ({note_loc})"
            highlights.append(frag.strip())
    return _dedupe_preserve(highlights)


def _focused_view_lines(collapsed_lines):
    focus = []
    for idx, line in enumerate(collapsed_lines):
        if _SEVERITY_REGEX.search(line):
            if idx > 0:
                focus.append(collapsed_lines[idx - 1])
            focus.append(line)
            if idx + 1 < len(collapsed_lines):
                focus.append(collapsed_lines[idx + 1])
    focus = _dedupe_preserve(focus)
    return focus if focus else collapsed_lines[:200]


def _apply_context_budget(lines, budget):
    text = "\n".join(line.rstrip() for line in lines if line is not None)
    text = text.strip()
    full_len = len(text)
    if budget and full_len > budget:
        trimmed = text[:budget]
        cut = trimmed.rfind("\n")
        if cut > budget * 0.6:
            trimmed = trimmed[:cut]
        truncated = full_len - len(trimmed)
        text = trimmed.rstrip() + f"\n... [truncated {truncated} chars]"
        used = len(text)
        return text, used, full_len, truncated
    return text, full_len, full_len, 0


def _collect_counts(diagnostics):
    counts = {"error": 0, "warning": 0, "note": 0, "remark": 0, "other": 0}
    for diag in diagnostics:
        level = (diag.get("level") or "other").lower()
        if level in counts:
            counts[level] += 1
        else:
            counts["other"] += 1
    return counts


def _find_compile_database(path_hint=None):
    if path_hint:
        candidate = pathlib.Path(path_hint)
        if candidate.exists():
            return candidate
        return candidate  # allow caller to report missing path
    candidates = [
        EXPORTS / "compile_commands.json",
        pathlib.Path.cwd() / "compile_commands.json",
    ]
    for cand in candidates:
        if cand.exists():
            return cand
    common_build_dirs = ["build", "cmake-build-debug", "cmake-build-release"]
    for build_dir in common_build_dirs:
        candidate = pathlib.Path(build_dir) / "compile_commands.json"
        if candidate.exists():
            return candidate
    return None


def _prepare_compile_command(entry):
    args = []
    if isinstance(entry.get("arguments"), list):
        args = list(entry["arguments"])
    elif entry.get("command"):
        args = shlex.split(entry["command"])
    sanitized = []
    skip_next = False
    drop_prefixes = ("-o", "-MF", "-MT", "-MQ")
    for part in args:
        if skip_next:
            skip_next = False
            continue
        if part in drop_prefixes:
            skip_next = True
            continue
        if any(part.startswith(prefix) for prefix in drop_prefixes):
            continue
        sanitized.append(part)
    if "-fsyntax-only" not in sanitized:
        sanitized.append("-fsyntax-only")
    if not any(arg.startswith("-fdiagnostics-format") for arg in sanitized):
        sanitized.append("-fdiagnostics-format=json")
    if not any(arg.startswith("-fdiagnostics-color") for arg in sanitized) and not any(arg.startswith("-fcolor-diagnostics") for arg in sanitized):
        sanitized.append("-fdiagnostics-color=never")
    if "-fno-caret-diagnostics" not in sanitized:
        sanitized.append("-fno-caret-diagnostics")
    return sanitized


def cmd_docs(_):
    out = ROOT/"docs"/"REFERENCE.md"
    generate_reference_md(out)
    print(str(out))

def cmd_capabilities(_):
    path = generate_capabilities_json(EXPORTS/"capabilities.json")
    print(str(path))

def adopt_existing_project(project_dir: pathlib.Path, project_name: str) -> int:
    """Generate an adoption report for an existing project without mutating sources."""
    print(f"🤝 Adopting existing project in {project_dir}")

    cmake_validation = validate_cmake_guidance(project_dir)
    compile_db = project_dir / "compile_commands.json"
    exported_compile_db = None
    compile_db_copy_error = None
    build_dir_candidates = []
    for candidate in ["build", "cmake-build-debug", "cmake-build-release"]:
        bdir = project_dir / candidate
        if bdir.exists() and bdir.is_dir():
            build_dir_candidates.append(str(bdir.resolve()))

    preset_names = [".clang-tidy", ".clang-format", "cmake-format.yaml", "pre-commit-config.yaml"]
    presets_status = []
    recommendations = list(cmake_validation.get("suggestions", []))
    for preset in preset_names:
        present = (project_dir / preset).exists()
        presets_status.append({"name": preset, "present": present})
        if not present:
            recommendations.append(f"Consider copying preset '{preset}' from llm-cpp-toolkit/presets")

    if not compile_db.exists():
        recommendations.append("Generate compile_commands.json via 'llmtk context export --build <build-dir>'")
    else:
        dest = EXPORTS / "compile_commands.json"
        try:
            dest.parent.mkdir(parents=True, exist_ok=True)
            if dest.exists() and dest.resolve() == compile_db.resolve():
                exported_compile_db = str(dest.resolve())
            else:
                shutil.copy2(compile_db, dest)
                exported_compile_db = str(dest.resolve())
        except Exception as exc:  # best effort copy
            compile_db_copy_error = str(exc)
            recommendations.append(
                "Copy existing compile_commands.json into exports/ (e.g. 'cp compile_commands.json exports/') for agents"
            )

    deduped_recs = []
    seen = set()
    for item in recommendations:
        if item and item not in seen:
            deduped_recs.append(item)
            seen.add(item)

    adoption_summary = {
        "mode": "existing",
        "project_name": project_name,
        "project_dir": str(project_dir.resolve()),
        "timestamp": datetime.datetime.now(datetime.UTC).isoformat(),
        "cmake_validation": cmake_validation,
        "artifacts": {
            "compile_commands": str(compile_db.resolve()) if compile_db.exists() else None,
            "build_directories": build_dir_candidates,
            "exports_compile_commands": exported_compile_db,
        },
        "presets": presets_status,
        "recommendations": deduped_recs,
    }

    out = EXPORTS / "init-existing.json"
    write_json(out, adoption_summary)
    capabilities_path = generate_capabilities_json(EXPORTS/"capabilities.json")

    print("📄 Adoption summary written to", out)
    print("📚 Capabilities manifest available at", capabilities_path)
    if exported_compile_db:
        print(f"📦 Copied existing compile_commands.json to {exported_compile_db}")
    elif compile_db_copy_error:
        print(f"⚠️ Failed to copy compile_commands.json: {compile_db_copy_error}")
    if cmake_validation["overall_score"] >= 80:
        print(f"✅ Existing CMake setup scores {cmake_validation['overall_score']:.0f}% against CMAKE_GUIDANCE.md")
    else:
        print(f"💡 CMake compliance is {cmake_validation['overall_score']:.0f}% – see recommendations above.")

    print("\n🚀 Suggested follow-ups:")
    for rec in deduped_recs[:5]:
        print(f"   • {rec}")
    if len(deduped_recs) > 5:
        print(f"   • ...and {len(deduped_recs) - 5} more (see adoption summary)")

    return 0


def cmd_init(args):
    """Initialize a new C++ project with CMAKE_GUIDANCE.md compliance"""
    project_name = args.name or pathlib.Path.cwd().name
    project_dir = pathlib.Path.cwd() if not args.name else pathlib.Path(args.name)

    # Create directory if it doesn't exist
    if args.name:
        if project_dir.exists():
            print(f"📂 Using existing project directory: {project_dir}")
        else:
            project_dir.mkdir(parents=True, exist_ok=True)
            print(f"📁 Created project directory: {project_dir}")

    # Check if CMakeLists.txt already exists
    cmake_file = project_dir / "CMakeLists.txt"
    if args.existing and not cmake_file.exists():
        print("❌ --existing specified but no CMakeLists.txt found in the target directory.")
        return 1

    if cmake_file.exists() and not args.force:
        return adopt_existing_project(project_dir, project_name)

    # Use template engine if available, otherwise fall back to legacy generation
    if TEMPLATE_ENGINE_AVAILABLE:
        return init_with_template_engine(args, project_name, project_dir)
    else:
        return init_with_legacy_generation(args, project_name, project_dir)

def init_with_template_engine(args, project_name, project_dir):
    """Initialize project using the template engine"""
    try:
        template_dir = ROOT / "templates"
        manifest_dir = ROOT / "manifest"
        engine = TemplateEngine(template_dir, manifest_dir)

        # Build user overrides from command line arguments
        user_overrides = {}

        # Handle legacy arguments
        if getattr(args, 'pic', False):
            user_overrides['pic'] = True
        if getattr(args, 'no_sanitizers', False):
            user_overrides['sanitizers'] = False
        if getattr(args, 'no_rtti', False):
            user_overrides['rtti'] = False
        if getattr(args, 'no_exceptions', False):
            user_overrides['exceptions'] = False
        if getattr(args, 'enable_simd', False):
            user_overrides['simd'] = 'native'
        if getattr(args, 'openmp', False):
            user_overrides['concurrency'] = 'openmp'
        if getattr(args, 'static_linking', False):
            user_overrides['static_linking'] = True

        # Resolve template
        preset = getattr(args, 'preset', 'full')
        template = engine.resolve_template(preset, user_overrides)

        print(f"📋 Using template: {template.name}")
        if template.description:
            print(f"   {template.description}")

        # Generate CMakeLists.txt
        cmake_min_version = getattr(args, 'cmake_min', '3.28')
        cxx_standard = getattr(args, 'std', '23')

        cmake_content = engine.generate_cmake_content(
            template, project_name, cmake_min_version, cxx_standard
        )

        # Write CMakeLists.txt
        cmake_file = project_dir / "CMakeLists.txt"
        cmake_file.write_text(cmake_content)

        # Create additional files from template
        for file_spec in template.files:
            file_path = project_dir / file_spec['path']
            file_path.parent.mkdir(parents=True, exist_ok=True)
            try:
                content = file_spec['content'].format(project_name=project_name)
            except (KeyError, ValueError) as e:
                # Fallback: use simple replacement for problematic templates
                content = file_spec['content'].replace('{project_name}', project_name)
            file_path.write_text(content)
            print(f"📄 Created {file_spec['path']}")

        # Generate capabilities.json
        capabilities_path = project_dir / "exports" / "capabilities.json"
        capabilities_path.parent.mkdir(parents=True, exist_ok=True)
        generate_capabilities_json(capabilities_path)

        print(f"✅ Project '{project_name}' initialized successfully with preset '{preset}'")
        if template.documentation:
            print(f"\n📖 Template documentation:")
            print(textwrap.indent(template.documentation, "   "))

        return 0

    except Exception as e:
        print(f"❌ Failed to initialize with template engine: {e}")
        print("   Falling back to legacy generation...")
        return init_with_legacy_generation(args, project_name, project_dir)

def init_with_legacy_generation(args, project_name, project_dir):
    """Initialize project using legacy hardcoded generation"""
    # Generate CMAKE_GUIDANCE.md compliant CMakeLists.txt with user options
    cmake_min_version = getattr(args, 'cmake_min', '3.28')
    cxx_standard = getattr(args, 'std', '23')
    enable_pic = getattr(args, 'pic', False)
    disable_sanitizers = getattr(args, 'no_sanitizers', False)
    preset = getattr(args, 'preset', 'full')

    # Handle sanitizers section based on user preference
    sanitizers_section = ""
    if not disable_sanitizers:
        sanitizers_section = f'''
# 2) Sanitizer function for creating multiple sanitized variants
function(llmtk_add_sanitized_target base_target sanitizer_name sanitizer_flags)
  if (NOT TARGET "${{base_target}}")
    message(FATAL_ERROR "llmtk_add_sanitized_target: unknown target '${{base_target}}'")
  endif()

  set(sanitized "${{base_target}}_${{sanitizer_name}}")

  # Collect the base target's sources so we can clone the build
  get_target_property(srcs "${{base_target}}" SOURCES)
  if (NOT srcs)
    message(FATAL_ERROR "Target '${{base_target}}' has no SOURCES property; cannot clone for sanitizer build")
  endif()

  # Determine target type (executable vs library)
  get_target_property(target_type "${{base_target}}" TYPE)
  if(target_type STREQUAL "EXECUTABLE")
    add_executable("${{sanitized}}" EXCLUDE_FROM_ALL ${{srcs}})
  elseif(target_type STREQUAL "STATIC_LIBRARY" OR target_type STREQUAL "SHARED_LIBRARY")
    add_library("${{sanitized}}" EXCLUDE_FROM_ALL ${{srcs}})
  else()
    message(FATAL_ERROR "Unsupported target type '${{target_type}}' for sanitizer cloning")
  endif()

  # Keep the strict warnings and any other link deps from the original target
  target_link_libraries("${{sanitized}}" PRIVATE
    $<TARGET_PROPERTY:${{base_target}},LINK_LIBRARIES>
    project_warnings
  )

  # Mirror include dirs / compile definitions / options so the TU is identical
  target_include_directories("${{sanitized}}" PRIVATE
    $<TARGET_PROPERTY:${{base_target}},INCLUDE_DIRECTORIES>
  )
  target_compile_definitions("${{sanitized}}" PRIVATE
    $<TARGET_PROPERTY:${{base_target}},COMPILE_DEFINITIONS>
  )
  target_compile_options("${{sanitized}}" PRIVATE
    $<TARGET_PROPERTY:${{base_target}},COMPILE_OPTIONS>
    ${{sanitizer_flags}}
  )
  target_link_options("${{sanitized}}" PRIVATE ${{sanitizer_flags}})

  # Optional: share the same output directory, and make the sanitized target depend on the base one
  set_target_properties("${{sanitized}}" PROPERTIES
    RUNTIME_OUTPUT_DIRECTORY $<TARGET_PROPERTY:${{base_target}},RUNTIME_OUTPUT_DIRECTORY>
  )
  add_dependencies("${{sanitized}}" "${{base_target}}")
endfunction()

# Create sanitized variants for non-MSVC compilers
if (NOT MSVC)
  # AddressSanitizer + UBSan combo (most common for development)
  llmtk_add_sanitized_target({project_name} asan_ubsan "-fsanitize=address,undefined")

  # ThreadSanitizer (mutually exclusive with AddressSanitizer)
  llmtk_add_sanitized_target({project_name} tsan "-fsanitize=thread")

  # MemorySanitizer (requires special build setup, uncomment if needed)
  # llmtk_add_sanitized_target({project_name} msan "-fsanitize=memory")
endif()

# Legacy compatibility: empty interface library for existing code
add_library(project_sanitizers INTERFACE)'''
    else:
        sanitizers_section = '''
# 2) Sanitizers disabled per user preference
add_library(project_sanitizers INTERFACE)'''

    # Handle different presets for target and lint sections
    if preset == "minimal":
        target_section = f'''
# 3) Your target(s) - replace with your actual source files
add_executable({project_name} main.cpp)
target_link_libraries({project_name} PRIVATE project_warnings project_sanitizers)'''
        lint_section = ""
    elif preset == "library":
        target_section = f'''
# 3) Library target - adjust as needed
add_library({project_name} src/{project_name}.cpp)
target_include_directories({project_name} PUBLIC include)
target_link_libraries({project_name} PRIVATE project_warnings project_sanitizers)

# Optional: also build example executable
add_executable({project_name}_example examples/main.cpp)
target_link_libraries({project_name}_example PRIVATE {project_name} project_warnings project_sanitizers)'''

        # For library preset, add sanitized variants of both library and example
        if not disable_sanitizers:
            target_section += f'''

# Create sanitized variants of library and example (non-MSVC only)
if (NOT MSVC)
  # Library sanitized variants
  llmtk_add_sanitized_target({project_name} asan_ubsan "-fsanitize=address,undefined")
  llmtk_add_sanitized_target({project_name} tsan "-fsanitize=thread")

  # Example sanitized variants
  llmtk_add_sanitized_target({project_name}_example asan_ubsan "-fsanitize=address,undefined")
  llmtk_add_sanitized_target({project_name}_example tsan "-fsanitize=thread")
endif()'''
        lint_section = f'''
# 4) LLM-focused "lint" that compiles TUs with tight diagnostics (no linking)
get_target_property(LIB_SOURCES {project_name} SOURCES)
get_target_property(LIB_INCLUDES {project_name} INCLUDE_DIRECTORIES)
get_target_property(LIB_DEFS     {project_name} COMPILE_DEFINITIONS)
get_target_property(LIB_OPTS     {project_name} COMPILE_OPTIONS)

# Compose include and define flags portably
set(_lint_includes "")
if(LIB_INCLUDES)
  foreach(inc ${{LIB_INCLUDES}})
    list(APPEND _lint_includes -I${{inc}})
  endforeach()
endif()
set(_lint_defines "")
if(LIB_DEFS)
  foreach(def ${{LIB_DEFS}})
    list(APPEND _lint_defines -D${{def}})
  endforeach()
endif()

# Build a response file so the command stays short
set(_lint_rsp "${{CMAKE_BINARY_DIR}}/lint_args.rsp")
file(WRITE  "${{_lint_rsp}}" "")
if(LIB_OPTS)
  foreach(opt ${{LIB_OPTS}})
    file(APPEND "${{_lint_rsp}}" "${{opt}}\\n")
  endforeach()
endif()
foreach(def ${{_lint_defines}})
  file(APPEND "${{_lint_rsp}}" "${{def}}\\n")
endforeach()
foreach(inc ${{_lint_includes}})
  file(APPEND "${{_lint_rsp}}" "${{inc}}\\n")
endforeach()

# Pick flags per compiler
if (CMAKE_CXX_COMPILER_ID STREQUAL "Clang")
  set(LINT_CORE_FLAGS
    -std=c++{cxx_standard} -fsyntax-only
    -Wfatal-errors -ferror-limit=1
    -ftemplate-backtrace-limit=6 -fconstexpr-backtrace-limit=3 -fmacro-backtrace-limit=2
    -fno-caret-diagnostics -fdiagnostics-color=never -fdiagnostics-show-option
    -fdiagnostics-format=json
  )
elseif (CMAKE_CXX_COMPILER_ID STREQUAL "GNU")
  set(LINT_CORE_FLAGS
    -std=c++{cxx_standard} -fsyntax-only
    -Wfatal-errors -fmax-errors=1
    -fconcepts-diagnostics-depth=2
    -fno-diagnostics-show-caret -fdiagnostics-color=never
    -fdiagnostics-format=json
  )
else()
  # MSVC: fall back to /analyze JSON? For now: no-op lint with success.
  set(LINT_CORE_FLAGS "")
endif()

# One lint target that iterates all TU's; writes a single JSON per TU into build/lint/
add_custom_target(lint
  COMMENT "Running tight JSON diagnostics per translation unit..."
)
foreach(src ${{LIB_SOURCES}})
  get_filename_component(src_name "${{src}}" NAME_WE)
  set(out_json "${{CMAKE_BINARY_DIR}}/lint/${{src_name}}.json")
  file(MAKE_DIRECTORY "${{CMAKE_BINARY_DIR}}/lint")
  add_custom_command(TARGET lint POST_BUILD
    COMMAND ${{CMAKE_CXX_COMPILER}} "@${{_lint_rsp}}" ${{LINT_CORE_FLAGS}} "${{src}}" 2> "${{out_json}}"
    BYPRODUCTS "${{out_json}}"
    COMMENT "Lint ${{src_name}}"
    VERBATIM
  )
endforeach()'''
    else:  # preset == "full"
        target_section = f'''
# 3) Your target(s) - replace with your actual source files
add_executable({project_name} main.cpp)
target_link_libraries({project_name} PRIVATE project_warnings project_sanitizers)'''
        lint_section = f'''
# 4) LLM-focused "lint" that compiles TUs with tight diagnostics (no linking)
#    Important: use the *same* include dirs/defs/opts as {project_name} so headers resolve.
get_target_property(APP_SOURCES {project_name} SOURCES)
get_target_property(APP_INCLUDES {project_name} INCLUDE_DIRECTORIES)
get_target_property(APP_DEFS     {project_name} COMPILE_DEFINITIONS)
get_target_property(APP_OPTS     {project_name} COMPILE_OPTIONS)

# Compose include and define flags portably for non-MSVC compilers.
set(_lint_includes "")
if(APP_INCLUDES)
  foreach(inc ${{APP_INCLUDES}})
    list(APPEND _lint_includes -I${{inc}})
  endforeach()
endif()
set(_lint_defines "")
if(APP_DEFS)
  foreach(def ${{APP_DEFS}})
    list(APPEND _lint_defines -D${{def}})
  endforeach()
endif()

# Build a response file so the command stays short (token-cheap logs)
set(_lint_rsp "${{CMAKE_BINARY_DIR}}/lint_args.rsp")
file(WRITE  "${{_lint_rsp}}" "")
if(APP_OPTS)
  foreach(opt ${{APP_OPTS}})
    file(APPEND "${{_lint_rsp}}" "${{opt}}\\n")
  endforeach()
endif()
foreach(def ${{_lint_defines}})
  file(APPEND "${{_lint_rsp}}" "${{def}}\\n")
endforeach()
foreach(inc ${{_lint_includes}})
  file(APPEND "${{_lint_rsp}}" "${{inc}}\\n")
endforeach()

# Pick flags per compiler
if (CMAKE_CXX_COMPILER_ID STREQUAL "Clang")
  set(LINT_CORE_FLAGS
    -std=c++{cxx_standard} -fsyntax-only
    -Wfatal-errors -ferror-limit=1
    -ftemplate-backtrace-limit=6 -fconstexpr-backtrace-limit=3 -fmacro-backtrace-limit=2
    -fno-caret-diagnostics -fdiagnostics-color=never -fdiagnostics-show-option
    -fdiagnostics-format=json
  )
elseif (CMAKE_CXX_COMPILER_ID STREQUAL "GNU")
  set(LINT_CORE_FLAGS
    -std=c++{cxx_standard} -fsyntax-only
    -Wfatal-errors -fmax-errors=1
    -fconcepts-diagnostics-depth=2
    -fno-diagnostics-show-caret -fdiagnostics-color=never
    -fdiagnostics-format=json
  )
else()
  # MSVC: fall back to /analyze JSON? For now: no-op lint with success.
  set(LINT_CORE_FLAGS "")
endif()

# One lint target that iterates all TU's; writes a single JSON per TU into build/lint/
add_custom_target(lint
  COMMENT "Running tight JSON diagnostics per translation unit..."
)
foreach(src ${{APP_SOURCES}})
  get_filename_component(src_name "${{src}}" NAME_WE)
  set(out_json "${{CMAKE_BINARY_DIR}}/lint/${{src_name}}.json")
  file(MAKE_DIRECTORY "${{CMAKE_BINARY_DIR}}/lint")
  add_custom_command(TARGET lint POST_BUILD
    COMMAND ${{CMAKE_CXX_COMPILER}} "@${{_lint_rsp}}" ${{LINT_CORE_FLAGS}} "${{src}}" 2> "${{out_json}}"
    BYPRODUCTS "${{out_json}}"
    COMMENT "Lint ${{src_name}}"
    VERBATIM
  )
endforeach()'''

    # Handle Position Independent Code
    pic_section = ""
    if enable_pic:
        pic_section = "\n# Enable position independent code\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\n"

    cmake_template = f'''cmake_minimum_required(VERSION {cmake_min_version})
project({project_name} LANGUAGES CXX)

# Always declare the language level once
set(CMAKE_CXX_STANDARD {cxx_standard})
set(CMAKE_CXX_STANDARD_REQUIRED ON)
set(CMAKE_CXX_EXTENSIONS OFF){pic_section}

# Export compile_commands.json (handy for your reducer or tools)
set(CMAKE_EXPORT_COMPILE_COMMANDS ON)

# 1) Warnings-as-errors & friends — per-compiler, per-config, target-attachable
add_library(project_warnings INTERFACE)

if (MSVC)
  target_compile_options(project_warnings INTERFACE
    /W4 /WX /permissive- /Zc:preprocessor /Zc:__cplusplus /EHsc
    # optional noise trims:
    # /wd4244 /wd4267
  )
else()
  target_compile_options(project_warnings INTERFACE
    -Wall -Wextra -Wconversion -Wshadow -Werror
    -Wnon-virtual-dtor -Woverloaded-virtual -Wimplicit-fallthrough
    -fdiagnostics-show-option
  )
endif()

# 2) Sanitizers toggles (don't hardcode CMAKE_CXX_FLAGS_*; compose options instead)
option(ENABLE_ASAN "Enable AddressSanitizer" $<NOT:$<CXX_COMPILER_ID:MSVC>>)
option(ENABLE_UBSAN "Enable UBSan"           $<NOT:$<CXX_COMPILER_ID:MSVC>>)
add_library(project_sanitizers INTERFACE)
if (NOT MSVC)
  if (ENABLE_ASAN)
    target_compile_options(project_sanitizers INTERFACE -fsanitize=address)
    target_link_options(project_sanitizers    INTERFACE -fsanitize=address)
  endif()
  if (ENABLE_UBSAN)
    target_compile_options(project_sanitizers INTERFACE -fsanitize=undefined)
    target_link_options(project_sanitizers    INTERFACE -fsanitize=undefined)
  endif()
endif()

# 3) Your target(s) - replace with your actual source files
add_executable({project_name} main.cpp)
target_link_libraries({project_name} PRIVATE project_warnings project_sanitizers)

# 4) LLM-focused "lint" that compiles TUs with tight diagnostics (no linking)
#    Important: use the *same* include dirs/defs/opts as {project_name} so headers resolve.
get_target_property(APP_SOURCES {project_name} SOURCES)
get_target_property(APP_INCLUDES {project_name} INCLUDE_DIRECTORIES)
get_target_property(APP_DEFS     {project_name} COMPILE_DEFINITIONS)
get_target_property(APP_OPTS     {project_name} COMPILE_OPTIONS)

# Compose include and define flags portably for non-MSVC compilers.
set(_lint_includes "")
if(APP_INCLUDES)
  foreach(inc ${{APP_INCLUDES}})
    list(APPEND _lint_includes -I${{inc}})
  endforeach()
endif()

set(_lint_defines "")
if(APP_DEFS)
  foreach(def ${{APP_DEFS}})
    list(APPEND _lint_defines -D${{def}})
  endforeach()
endif()

# Build a response file so the command stays short (token-cheap logs)
set(_lint_rsp "${{CMAKE_BINARY_DIR}}/lint_args.rsp")
file(WRITE  "${{_lint_rsp}}" "")
if(APP_OPTS)
  foreach(opt ${{APP_OPTS}})
    file(APPEND "${{_lint_rsp}}" "${{opt}}\\n")
  endforeach()
endif()
foreach(def ${{_lint_defines}})
  file(APPEND "${{_lint_rsp}}" "${{def}}\\n")
endforeach()
foreach(inc ${{_lint_includes}})
  file(APPEND "${{_lint_rsp}}" "${{inc}}\\n")
endforeach()

# Pick flags per compiler
if (CMAKE_CXX_COMPILER_ID STREQUAL "Clang")
  set(LINT_CORE_FLAGS
    -std=c++23 -fsyntax-only
    -Wfatal-errors -ferror-limit=1
    -ftemplate-backtrace-limit=6 -fconstexpr-backtrace-limit=3 -fmacro-backtrace-limit=2
    -fno-caret-diagnostics -fdiagnostics-color=never -fdiagnostics-show-option
    -fdiagnostics-format=json
  )
elseif (CMAKE_CXX_COMPILER_ID STREQUAL "GNU")
  set(LINT_CORE_FLAGS
    -std=c++20 -fsyntax-only
    -Wfatal-errors -fmax-errors=1
    -fconcepts-diagnostics-depth=2
    -fno-diagnostics-show-caret -fdiagnostics-color=never
    -fdiagnostics-format=json
  )
else()
  # MSVC: fall back to /analyze JSON? For now: no-op lint with success.
  set(LINT_CORE_FLAGS "")
endif()

# One lint target that iterates all TU's; writes a single JSON per TU into build/lint/
add_custom_target(lint
  COMMENT "Running tight JSON diagnostics per translation unit..."
)
foreach(src ${{APP_SOURCES}})
  get_filename_component(src_name "${{src}}" NAME_WE)
  set(out_json "${{CMAKE_BINARY_DIR}}/lint/${{src_name}}.json")
  file(MAKE_DIRECTORY "${{CMAKE_BINARY_DIR}}/lint")
  add_custom_command(TARGET lint POST_BUILD
    COMMAND ${{CMAKE_CXX_COMPILER}} "@${{_lint_rsp}}" ${{LINT_CORE_FLAGS}} "${{src}}" 2> "${{out_json}}"
    BYPRODUCTS "${{out_json}}"
    COMMENT "Lint ${{src_name}}"
    VERBATIM
  )
endforeach()
'''

    # Write CMakeLists.txt
    cmake_file.write_text(cmake_template)
    print(f"✅ Created CMAKE_GUIDANCE.md compliant CMakeLists.txt")

    # Create main.cpp if it doesn't exist
    main_file = project_dir / "main.cpp"
    if not main_file.exists():
        main_template = f'''#include <iostream>

int main() {{
    std::cout << "Hello from {project_name}!\\n";
    return 0;
}}
'''
        main_file.write_text(main_template)
        print(f"✅ Created sample main.cpp")

    # Validate the created setup
    cmake_validation = validate_cmake_guidance(project_dir)
    print(f"🏗️ CMake compliance: ✅ {cmake_validation['overall_score']:.0f}% compliant with CMAKE_GUIDANCE.md")

    print(f"\n🚀 Next steps:")
    print(f"   • cd {project_dir if args.name else '.'}")
    print(f"   • llmtk context export    # Generate build context")
    print(f"   • cmake -B build -G Ninja  # Configure build")
    print(f"   • cmake --build build      # Build project")
    print(f"   • cmake --build build --target lint  # Run LLM-optimized linting")

    capabilities_path = generate_capabilities_json(EXPORTS/"capabilities.json")
    print(f"\n📚 Capabilities manifest available at {capabilities_path}")

    return 0

def cmd_doctor(args):
    # Load from manifest to get complete tool list
    tools_manifest = ROOT / "manifest" / "tools.yaml"
    if tools_manifest.exists():
        tools_config = load_yaml(tools_manifest)
        if tools_config and "tools" in tools_config:
            # Get all tools from manifest, prioritizing core and recommended
            all_tools = tools_config["tools"]
            tools = []
            for name, config in all_tools.items():
                role = config.get("role", "optional")
                if role in ["core", "recommended"]:
                    tools.append(name)
            # Add any remaining tools
            for name in all_tools.keys():
                if name not in tools:
                    tools.append(name)
        else:
            # Fallback list
            tools = [
                "cmake","ninja","clangd","clang-tidy","clang-format",
                "include-what-you-use","cppcheck","rg","fd","jq","yq","bear","ccache","mold"
            ]
    else:
        # Fallback list
        tools = [
            "cmake","ninja","clangd","clang-tidy","clang-format",
            "include-what-you-use","cppcheck","rg","fd","jq","yq","bear","ccache","mold"
        ]

    report = {"_meta": {"generated_at": datetime.datetime.now(datetime.UTC).isoformat()}}

    found_tools = []
    missing_core = []
    missing_recommended = []
    missing_optional = []

    # Add local bin to PATH for tool discovery (use same path as install command)
    local_bin = ROOT / ".llmtk" / "bin"
    old_path = os.environ.get("PATH", "")
    if local_bin.exists():
        os.environ["PATH"] = f"{local_bin}:{old_path}"

    for t in tools:
        # Get the actual command to check from the manifest
        actual_cmd = t
        if tools_manifest.exists() and tools_config and "tools" in tools_config:
            tool_config = tools_config["tools"].get(t, {})
            check_config = tool_config.get("check", {})
            if isinstance(check_config, dict) and "cmd" in check_config:
                actual_cmd = check_config["cmd"][0]  # First element is the command name

        path = shutil.which(actual_cmd)
        info = {"found": bool(path), "path": path or None}
        if path:
            try:
                out = run([actual_cmd, "--version"], check=False).stdout.splitlines()
                info["version_line"] = out[0] if out else None
            except Exception:
                info["version_line"] = None
            found_tools.append(t)
        else:
            # Categorize missing tools by role
            if tools_manifest.exists() and tools_config and "tools" in tools_config:
                tool_config = tools_config["tools"].get(t, {})
                role = tool_config.get("role", "optional")
                if role == "core":
                    missing_core.append(t)
                elif role == "recommended":
                    missing_recommended.append(t)
                else:
                    missing_optional.append(t)
            else:
                missing_core.append(t)  # Default to core for fallback
        report[t] = info

    # Restore original PATH
    os.environ["PATH"] = old_path

    # Add CMake validation to the report
    cmake_validation = validate_cmake_guidance()
    report["_cmake"] = cmake_validation

    # Add summary to report
    report["_summary"] = {
        "total_tools": len(tools),
        "found": len(found_tools),
        "missing": len(tools) - len(found_tools),
        "missing_core": missing_core,
        "missing_recommended": missing_recommended,
        "missing_optional": missing_optional,
        "cmake_compliance_score": cmake_validation["overall_score"]
    }

    out = EXPORTS / "doctor.json"
    write_json(out, report)

    # Print user-friendly summary if not being called from install
    if not hasattr(args, '_from_install'):
        # Check if this is CMake-focused or full health check
        cmake_only = hasattr(args, 'cmake') and args.cmake

        if cmake_only:
            print()
            print("🏗️ CMAKE COMPLIANCE CHECK")
            print("=" * 40)
        else:
            print()
            print("🏥 HEALTH CHECK SUMMARY")
            print("=" * 40)
            print(f"✅ Found: {len(found_tools)}/{len(tools)} tools")

        # Show different content based on mode
        if not cmake_only:
            if missing_core:
                print(f"\n❌ Missing core tools ({len(missing_core)}):")
                for tool in missing_core:
                    print(f"   • {tool}")

            if missing_recommended:
                print(f"\n⚠️ Missing recommended tools ({len(missing_recommended)}):")
                for tool in missing_recommended:
                    print(f"   • {tool}")

        # Show CMake compliance status (always shown)
        cmake_score = cmake_validation["overall_score"]
        if cmake_validation["cmake_file_exists"]:
            if cmake_score >= 80:
                print(f"\n🏗️ CMake setup: ✅ {cmake_score:.0f}% compliant with CMAKE_GUIDANCE.md")
            elif cmake_score >= 50:
                print(f"\n🏗️ CMake setup: ⚠️ {cmake_score:.0f}% compliant with CMAKE_GUIDANCE.md")
            else:
                print(f"\n🏗️ CMake setup: ❌ {cmake_score:.0f}% compliant with CMAKE_GUIDANCE.md")

            # Show detailed compliance breakdown in CMake-only mode
            if cmake_only:
                print(f"\n📋 Compliance Details:")
                for key, value in cmake_validation["compliance"].items():
                    status = "✅" if value else "❌"
                    readable_name = key.replace("_", " ").title()
                    print(f"   {status} {readable_name}")

            if cmake_validation["suggestions"]:
                suggestions_to_show = cmake_validation["suggestions"] if cmake_only else cmake_validation["suggestions"][:3]
                print("   CMake improvements needed:")
                for suggestion in suggestions_to_show:
                    print(f"   • {suggestion}")
        else:
            print(f"\n🏗️ CMake setup: ❌ No CMakeLists.txt found")
            print("   • Run 'llmtk init' to create LLM-optimized project structure")

        # Recommendations section
        needs_tools = missing_core or missing_recommended
        needs_cmake = cmake_score < 80

        if (not cmake_only and needs_tools) or needs_cmake:
            print(f"\n💡 RECOMMENDED ACTIONS:")
            if not cmake_only and needs_tools:
                print(f"   • Run 'llmtk install' to install missing tools")
                print(f"   • Use 'llmtk install --local' for non-sudo installation")
            if needs_cmake:
                print(f"   • Run 'llmtk init' to upgrade CMake setup for LLM workflows")
                print(f"   • See CMAKE_GUIDANCE.md for manual setup instructions")

        if not cmake_only:
            print(f"\n📄 Detailed report: {out}")

    print(str(out))

def cmd_context_export(args):
    # Check CMake compliance if requested
    if hasattr(args, 'require_cmake') and args.require_cmake:
        cmake_validation = validate_cmake_guidance()
        if not cmake_validation["cmake_file_exists"]:
            print("❌ Error: No CMakeLists.txt found. Use --require-cmake only in CMake projects.")
            print("💡 Run 'llmtk init' to create a compliant CMake project")
            return 1

        if cmake_validation["overall_score"] < 80:
            print(f"❌ Error: CMake compliance too low ({cmake_validation['overall_score']:.0f}% < 80%)")
            print("💡 Missing CMAKE_GUIDANCE.md patterns:")
            for suggestion in cmake_validation["suggestions"][:3]:
                print(f"   • {suggestion}")
            print("💡 Run 'llmtk init' to upgrade your CMake setup")
            return 1

        print(f"✅ CMake compliance verified ({cmake_validation['overall_score']:.0f}%)")

    build = pathlib.Path(args.build)

    if getattr(args, "preview", False):
        steps = [
            f"Configure: cmake -S . -B {build} -G Ninja -DCMAKE_EXPORT_COMPILE_COMMANDS=ON",
            f"Copy compile_commands.json into exports/",
        ]
        if args.deep:
            steps.append(f"Query CMake File API (codemodel, cache, toolchains) in {build}")
        else:
            steps.append(f"Query CMake File API (codemodel) in {build}")
        steps.append(f"cmake --build {build} (best-effort to refresh targets)")
        steps.append("Summarize artifacts to exports/context.json")

        print("📋 Context export preview:")
        for step in steps:
            print(f"   • {step}")
        return 0

    build.mkdir(exist_ok=True)

    # Compile DB
    try:
        run(["cmake","-S",".","-B",str(build),"-G","Ninja","-DCMAKE_EXPORT_COMPILE_COMMANDS=ON"], check=True)
        if (build/"compile_commands.json").exists():
            (EXPORTS/"compile_commands.json").write_bytes((build/"compile_commands.json").read_bytes())
    except Exception as e:
        # If compile_commands missing, try bear as a fallback
        bear = shutil.which("bear")
        if bear:
            try:
                run([bear, "--", "cmake", "--build", str(build)], check=False)
                if (build/"compile_commands.json").exists():
                    (EXPORTS/"compile_commands.json").write_bytes((build/"compile_commands.json").read_bytes())
            except Exception:
                pass
    # If still missing but project root has a compile_commands.json, copy it
    if not (EXPORTS/"compile_commands.json").exists() and (pathlib.Path.cwd()/"compile_commands.json").exists():
        (EXPORTS/"compile_commands.json").write_bytes((pathlib.Path.cwd()/"compile_commands.json").read_bytes())

    # CMake File API codemodel
    q = build/".cmake"/"api"/"v1"/"query"; q.mkdir(parents=True, exist_ok=True)
    (q/"codemodel-v2").write_text("")
    if args.deep:
        (q/"cache-v2").write_text("")
        (q/"toolchains-v1").write_text("")
    run(["cmake","--build",str(build)], check=False)
    reply = build/".cmake"/"api"/"v1"/"reply"
    if reply.exists():
        (EXPORTS/"cmake-file-api").mkdir(exist_ok=True)
        for p in reply.iterdir():
            (EXPORTS/"cmake-file-api"/p.name).write_bytes(p.read_bytes())

    summary = {
        "deep_export": args.deep,
        "compile_commands": "exports/compile_commands.json" if (EXPORTS/"compile_commands.json").exists() else None,
        "cmake_file_api": {
            "dir": "exports/cmake-file-api/",
            "files": sorted([p.name for p in (EXPORTS/"cmake-file-api").iterdir()]) if (EXPORTS/"cmake-file-api").exists() else []
        } if (EXPORTS/"cmake-file-api").exists() else None,
        "generated_at": datetime.datetime.now(datetime.UTC).isoformat()
    }

    if args.deep and (EXPORTS/"cmake-file-api").exists():
        summary["deep_info"] = {}
        # Try to find the codemodel file
        codemodel_file = next((p for p in (EXPORTS/"cmake-file-api").iterdir() if p.name.startswith("codemodel-v2-")), None)
        if codemodel_file:
            try:
                codemodel = json.loads(codemodel_file.read_text())
                summary["deep_info"]["codemodel"] = {
                    "configurations": [conf.get("name") for conf in codemodel.get("configurations", [])],
                    "targets": [t.get("name") for conf in codemodel.get("configurations", []) for t in conf.get("targets", [])]
                }
            except Exception:
                pass # ignore malformed json

        # Try to find the cache file
        cache_file = next((p for p in (EXPORTS/"cmake-file-api").iterdir() if p.name.startswith("cache-v2-")), None)
        if cache_file:
            try:
                cache = json.loads(cache_file.read_text())
                summary["deep_info"]["cache"] = {
                    "CMAKE_CXX_COMPILER": next((item.get("value") for item in cache.get("entries", []) if item.get("name") == "CMAKE_CXX_COMPILER"), None),
                    "CMAKE_CXX_STANDARD": next((item.get("value") for item in cache.get("entries", []) if item.get("name") == "CMAKE_CXX_STANDARD"), None),
                }
            except Exception:
                pass

        # Try to find the toolchains file
        toolchains_file = next((p for p in (EXPORTS/"cmake-file-api").iterdir() if p.name.startswith("toolchains-v1-")), None)
        if toolchains_file:
            try:
                toolchains = json.loads(toolchains_file.read_text())
                if toolchains.get("toolchains"):
                    summary["deep_info"]["toolchains"] = toolchains.get("toolchains")
            except Exception:
                pass

    bench_latest = EXPORTS / "perf" / "bench.json"
    if bench_latest.exists():
        try:
            bench_data = json.loads(bench_latest.read_text())
            stage_summaries = {}
            for name, info in (bench_data.get("stages") or {}).items():
                if not isinstance(info, dict):
                    continue
                stage_summaries[name] = {
                    "duration_seconds": info.get("duration_seconds"),
                    "peak_memory_kib": info.get("peak_memory_kib"),
                    "return_code": info.get("return_code"),
                }
            performance_summary: Dict[str, Any] = {
                "bench": "exports/perf/bench.json",
                "generated_at": bench_data.get("_meta", {}).get("generated_at"),
                "parallelism": bench_data.get("parallelism"),
                "slow_translation_units": bench_data.get("slow_translation_units"),
                "stages": stage_summaries,
            }
            summary["performance"] = performance_summary
        except Exception:
            summary["performance"] = {"bench": "exports/perf/bench.json"}

    write_json(EXPORTS/"context.json", summary)
    print(str(EXPORTS/"context.json"))

def cmd_context_pack(args):
    """Pack the context export into a tarball."""
    if args.redact:
        print("Note: Redaction is not yet implemented.")

    output_filename = EXPORTS.parent / "context-export.tar.gz"
    with tarfile.open(output_filename, "w:gz") as tar:
        tar.add(EXPORTS, arcname=EXPORTS.name)

    print(f"Context export packed to: {output_filename}")

def cmd_analyze(args):
    reports = EXPORTS/"reports"; reports.mkdir(exist_ok=True)
    cache_dir = ROOT / ".llmtk" / "cache" / "analyze"; cache_dir.mkdir(parents=True, exist_ok=True)
    compile_db = EXPORTS/"compile_commands.json"
    paths = args.paths or []

    def get_file_hash(path):
        h = hashlib.sha256()
        with open(path, 'rb') as f:
            while True:
                data = f.read(65536)
                if not data:
                    break
                h.update(data)
        return h.hexdigest()

    def get_changed_files():
        try:
            result = run(["git", "diff", "--name-only", "HEAD"], check=True)
            return [pathlib.Path(p).resolve() for p in result.stdout.splitlines()]
        except Exception as e:
            print(f"Warning: Could not get changed files from git: {e}")
            return None

    def read_compile_db_files():
        files = []
        if compile_db.exists():
            try:
                data = json.loads(compile_db.read_text())
                files = [entry.get("file") for entry in data if entry.get("file")]
            except Exception:
                files = []
        
        # Convert to absolute paths for comparison
        files = [pathlib.Path(f).resolve() for f in files]

        # Filter by user-provided paths if any
        if paths:
            keep = []
            for f in files:
                for p in paths:
                    p_abs = pathlib.Path(p).resolve()
                    if p_abs.is_dir():
                        if f.is_relative_to(p_abs):
                            keep.append(f)
                            break
                    elif f == p_abs:
                        keep.append(f)
                        break
            files = keep
        
        # Incremental filtering
        if args.incremental:
            changed_files = get_changed_files()
            if changed_files is not None:
                files = [f for f in files if f in changed_files]

        # Dedup while preserving order
        seen = set(); uniq = []
        for f in files:
            if f not in seen:
                seen.add(f); uniq.append(str(f))
        return uniq

    files_in_db = read_compile_db_files()

    # Caching logic
    cache_key_str = args.cache_key or ""
    if files_in_db:
        files_hash = hashlib.sha256("".join(sorted(files_in_db)).encode()).hexdigest()
        content_hash = hashlib.sha256()
        for f in files_in_db:
            try:
                content_hash.update(get_file_hash(f).encode())
            except Exception:
                pass # Ignore files that can't be read
        cache_key_str += files_hash + content_hash.hexdigest()

    cache_key = hashlib.sha256(cache_key_str.encode()).hexdigest()
    cache_file = cache_dir / f"{cache_key}.json"

    if cache_file.exists():
        print(f"Using cached analysis results from {cache_file}")
        cached_data = json.loads(cache_file.read_text())
        for report_name, report_data in cached_data.items():
            write_json(reports/f"{report_name}.json", report_data)
        print(str(reports))
        return

    def run_proc(cmd, cwd=None):
        return subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)

    # clang-tidy with export-fixes
    clang_tidy_path = shutil.which("clang-tidy")
    clang_tidy_report = {"available": bool(clang_tidy_path), "diagnostics": [], "fixes": [], "version": None, "inputs": []}
    if clang_tidy_path:
        try:
            v = run_proc([clang_tidy_path, "--version"]).stdout.splitlines()
            clang_tidy_report["version"] = v[0] if v else None
        except Exception:
            pass
        # Choose files to analyze
        target_files = files_in_db
        clang_tidy_report["inputs"] = target_files[:50]  # cap in report
        tidy_diags = []
        tidy_fixes = []
        diag_re = re.compile(r"^(?P<file>[^:]+):(?P<line>\d+):(?!\d+:)\s*(?P<col>\d+): (?P<severity>warning|error|note): (?P<msg>.*?)(?: \[(?P<check>[A-Za-z0-9_.\-]+)\])?$")
        # If no compile DB, still try on explicit paths
        if not target_files and paths:
            # restrict to common C/C++ extensions
            exts = (".c", ".cc", ".cxx", ".cpp", ".h", ".hh", ".hpp", ".hxx")
            for p in paths:
                pp = pathlib.Path(p)
                if pp.is_file() and pp.suffix in exts:
                    target_files.append(str(pp))
                elif pp.is_dir():
                    for f in pp.rglob("*"):
                        if f.suffix in exts:
                            target_files.append(str(f))
        # Analyze (best-effort, sequential to keep simple)
        for f in target_files[:200]:  # prevent runaway
            fixes_yaml = pathlib.Path(tempfile.gettempdir())/f"llmtk_tidy_{abs(hash(f))}.yaml"
            if fixes_yaml.exists():
                try: fixes_yaml.unlink()
                except Exception: pass
            cmd = [clang_tidy_path, f, "-quiet", f"-export-fixes={fixes_yaml}"]
            if compile_db.exists():
                # clang-tidy accepts directory or file to -p; pass dir containing the DB
                cmd.extend(["-p", str(compile_db.parent.resolve())])
            res = run_proc(cmd)
            for line in (res.stdout + "\n" + res.stderr).splitlines():
                m = diag_re.match(line.strip())
                if m:
                    d = m.groupdict()
                    d["line"] = int(d["line"]); d["col"] = int(d["col"])
                    tidy_diags.append(d)
            # Parse export-fixes YAML (if any)
            if fixes_yaml.exists():
                content = load_yaml(fixes_yaml)
                if content and isinstance(content, dict):
                    # content has Diagnostics: [{DiagnosticMessage:{Message,FilePath,FileOffset}, Replacements:[{FilePath,Offset,Length,ReplacementText}]}]
                    diags = content.get("Diagnostics") or []
                    for item in diags:
                        dm = item.get("DiagnosticMessage") or {}
                        repl = item.get("Replacements") or []
                        tidy_fixes.append({
                            "file": dm.get("FilePath"),
                            "message": dm.get("Message"),
                            "file_offset": dm.get("FileOffset"),
                            "replacements": [
                                {
                                    "file": r.get("FilePath"),
                                    "offset": r.get("Offset"),
                                    "length": r.get("Length"),
                                    "replacement": r.get("ReplacementText"),
                                } for r in repl if isinstance(r, dict)
                            ],
                        })
                try: fixes_yaml.unlink()
                except Exception: pass
        clang_tidy_report["diagnostics"] = tidy_diags
        clang_tidy_report["fixes"] = tidy_fixes
    write_json(reports/"clang-tidy.json", clang_tidy_report)

    # include-what-you-use (IWYU)
    iwyu_bin = shutil.which("include-what-you-use")
    iwyu_tool = shutil.which("iwyu-tool")
    iwyu_report = {"available": bool(iwyu_bin or iwyu_tool), "version": None, "suggestions": []}
    if iwyu_bin or iwyu_tool:
        # Try to get version line
        try:
            if iwyu_bin:
                vv = run_proc([iwyu_bin, "--version"]).stdout.splitlines()
                iwyu_report["version"] = vv[0] if vv else None
        except Exception:
            pass
        out = None
        if iwyu_tool and compile_db.exists():
            # Run across the compilation database
            res = run_proc([iwyu_tool, "-p", str(compile_db.parent.resolve())])
            out = res.stdout or res.stderr
        elif iwyu_bin and files_in_db:
            # Try first few files using compile DB path; IWYU will discover flags via -Xiwyu?
            # We fallback to plain invocation which may be noisy
            collected = []
            for f in files_in_db[:20]:
                res = run_proc([iwyu_bin, f])
                collected.append((res.stdout or "") + "\n" + (res.stderr or ""))
            out = "\n".join(collected)
        def parse_iwyu(text: str):
            suggestions = {}
            current = None
            mode = None  # 'add' or 'remove'
            for raw in text.splitlines():
                line = raw.strip()
                if not line:
                    mode = None
                    continue
                m_add = re.search(r"^(.*) should add these lines:", line)
                m_rem = re.search(r"^(.*) should remove these lines:", line)
                if m_add or m_rem:
                    current = (m_add or m_rem).group(1).strip()
                    entry = suggestions.setdefault(current, {"add": [], "remove": []})
                    mode = 'add' if m_add else 'remove'
                    continue
                if line.startswith("The full include-list for "):
                    current = None
                    mode = None
                    continue
                if mode in ('add','remove') and (line.startswith('#include') or line.startswith('namespace') or line.startswith('using')):
                    # IWYU annotates with comments; keep the include text as-is
                    suggestions.setdefault(current, {"add": [], "remove": []})[mode].append(line)
            return [{"file": f, **v} for f, v in suggestions.items()]
        if out:
            iwyu_report["suggestions"] = parse_iwyu(out)[:200]
    write_json(reports/"iwyu.json", iwyu_report)

    # cppcheck
    cppcheck_bin = shutil.which("cppcheck")
    cppcheck_report = {"available": bool(cppcheck_bin), "version": None, "diagnostics": []}
    if cppcheck_bin:
        try:
            vv = run_proc([cppcheck_bin, "--version"]).stdout.splitlines()
            cppcheck_report["version"] = vv[0] if vv else None
        except Exception:
            pass
        xml_tmp = pathlib.Path(tempfile.gettempdir())/"llmtk_cppcheck.xml"
        cmd = [cppcheck_bin, "--enable=all", "--inconclusive", "--quiet", "--xml", "--xml-version=2"]
        if compile_db.exists():
            cmd.extend(["--project", str(compile_db)])
        else:
            # Fall back to paths list or current dir
            search_paths = paths or ["."]
            cmd.extend(search_paths)
        res = subprocess.run(cmd, text=True, capture_output=True)
        # cppcheck writes XML to stderr
        xml_data = res.stderr
        diags = []
        try:
            import xml.etree.ElementTree as ET
            root = ET.fromstring(xml_data)
            for error in root.iterfind('.//errors/error'):
                ed = error.attrib
                locs = []
                for loc in error.iterfind('location'):
                    locs.append({
                        "file": loc.attrib.get("file"),
                        "line": int(loc.attrib.get("line", "0")),
                        "column": int(loc.attrib.get("column", "0"))
                    })
                diags.append({
                    "id": ed.get("id"),
                    "severity": ed.get("severity"),
                    "msg": ed.get("msg"),
                    "verbose": ed.get("verbose"),
                    "locations": locs
                })
        except Exception:
            # If XML parsing fails, include raw for inspection
            diags = [{"raw": xml_data[:200000]}]
        cppcheck_report["diagnostics"] = diags
    write_json(reports/"cppcheck.json", cppcheck_report)

    # Save results to cache
    all_reports = {
        "clang-tidy": clang_tidy_report,
        "iwyu": iwyu_report,
        "cppcheck": cppcheck_report
    }
    write_json(cache_file, all_reports)
    print(f"Saved analysis results to cache: {cache_file}")

    # Generate SARIF output if requested
    if getattr(args, 'sarif', False):
        try:
            # Import the SARIF converter
            sarif_converter_path = MODULES / "sarif_converter.py"
            if sarif_converter_path.exists():
                # Run the SARIF converter
                cmd = [
                    sys.executable, str(sarif_converter_path),
                    str(reports / "analysis.sarif"),
                    str(reports / "clang-tidy.json"),
                    str(reports / "cppcheck.json"),
                    str(reports / "iwyu.json")
                ]
                result = subprocess.run(cmd, capture_output=True, text=True, cwd=ROOT)
                if result.returncode == 0:
                    print(f"SARIF report: {reports / 'analysis.sarif'}")
                else:
                    print(f"Warning: SARIF conversion failed: {result.stderr}")
            else:
                print("Warning: SARIF converter not found")
        except Exception as e:
            print(f"Warning: SARIF conversion failed: {e}")

    print(str(reports))


def cmd_test(args):
    ctest_bin = shutil.which("ctest")
    if not ctest_bin:
        print("Error: ctest not found on PATH. Install CMake or ensure ctest is available.")
        return 1

    build_dir = pathlib.Path(args.build_dir).resolve()
    if not build_dir.exists():
        print(f"Error: build directory not found: {build_dir}")
        return 1

    tests_dir = EXPORTS / "tests"
    tests_dir.mkdir(parents=True, exist_ok=True)

    json_path = _resolve_optional_path(tests_dir / "ctest_results.json", getattr(args, "json", None))
    sarif_path = _resolve_optional_path(tests_dir / "ctest_results.sarif", getattr(args, "sarif", None))
    preview_path = tests_dir / "ctest_preview.json"

    ctest_version = None
    try:
        version_result = run([ctest_bin, "--version"], check=False)
        if version_result.stdout:
            ctest_version = version_result.stdout.splitlines()[0].strip()
    except Exception:
        pass

    filter_args = _ctest_filter_args(args)

    if getattr(args, "preview", False):
        preview_cmd = [ctest_bin, "--test-dir", str(build_dir), "--show-only=json-v1"] + filter_args
        if args.config:
            preview_cmd.extend(["--build-config", args.config])
        result = subprocess.run(preview_cmd, text=True, capture_output=True, cwd=build_dir)
        if result.returncode != 0 or not result.stdout.strip():
            # Fallback to plain text listing
            preview_cmd = [ctest_bin, "--test-dir", str(build_dir), "--show-only"] + filter_args
            if args.config:
                preview_cmd.extend(["--build-config", args.config])
            result = subprocess.run(preview_cmd, text=True, capture_output=True, cwd=build_dir)

        preview_summary = _describe_ctest_preview(result.stdout)
        write_json(preview_path, preview_summary)

        total = preview_summary.get("total")
        if isinstance(total, int):
            print(f"📋 Preview: {total} tests would run. Summary: {preview_path}")
        else:
            print(f"📋 Preview saved to {preview_path}")
        tests_list = preview_summary.get("tests") or []
        if isinstance(tests_list, list) and tests_list:
            print("   Sample tests:")
            for sample in tests_list[:5]:
                name = sample.get("name") or "(unknown)"
                cmd = sample.get("command") or ""
                print(f"   • {name}{' — ' + cmd if cmd else ''}")
        return 0

    command = [ctest_bin, "--test-dir", str(build_dir), "--output-on-failure", "--no-tests=error"] + filter_args
    if args.parallel is not None:
        command.extend(["-j", str(args.parallel)])
    if args.config:
        command.extend(["--build-config", args.config])
    if args.timeout:
        command.extend(["--timeout", str(args.timeout)])
    if args.rerun_failed:
        command.append("--rerun-failed")
    if args.stop_on_failure:
        command.append("--stop-on-failure")

    start = time.time()
    result = subprocess.run(command, text=True, capture_output=True, cwd=build_dir)
    duration = time.time() - start

    stdout_path = tests_dir / "ctest_stdout.txt"
    stderr_path = tests_dir / "ctest_stderr.txt"
    stdout_path.write_text(result.stdout)
    stderr_path.write_text(result.stderr)

    xml_path = _find_latest_ctest_xml(build_dir)
    parsed = None
    copied_xml = None
    if xml_path and xml_path.exists():
        parsed = _parse_ctest_xml(xml_path)
        copied_xml = tests_dir / "Test.xml"
        shutil.copyfile(xml_path, copied_xml)
    else:
        parsed = _fallback_parse_ctest_output(result.stdout)

    summary = {
        "_meta": {
            "generated_at": datetime.datetime.now(datetime.UTC).isoformat(),
            "ctest_command": " ".join(command),
            "build_dir": str(build_dir),
            "ctest_version": ctest_version,
            "return_code": result.returncode,
            "duration_seconds": round(duration, 3),
            "stdout": str(stdout_path),
            "stderr": str(stderr_path),
            "xml": str(copied_xml) if copied_xml else None,
        },
        "stats": parsed.get("stats", {}),
        "tests": parsed.get("tests", []),
        "failures": parsed.get("failures", []),
        "stdout_preview": result.stdout.splitlines()[:10],
        "stderr_preview": result.stderr.splitlines()[:10],
    }
    if parsed.get("warning"):
        summary["warning"] = parsed["warning"]

    write_json(json_path, summary)

    try:
        _write_ctest_sarif(sarif_path, summary["tests"], command, build_dir, ctest_version, result.returncode, duration)
    except Exception as exc:
        print(f"Warning: failed to write SARIF report: {exc}")

    stats = summary["stats"]
    total = stats.get("total", 0)
    passed = stats.get("passed", 0)
    failing_count = stats.get("failed", 0) + stats.get("timeout", 0) + stats.get("notrun", 0)

    print(f"🧪 CTest: {passed}/{total} passed ({failing_count} flagged). JSON: {json_path}")
    if summary.get("failures"):
        print("❌ Failed tests:")
        for failure in summary["failures"][:10]:
            name = failure.get("full_name") or failure.get("name")
            reason = failure.get("fail_reason") or failure.get("Completion Status") or "(no reason captured)"
            print(f"   • {name}: {reason}")
    if copied_xml:
        print(f"📄 Raw CTest XML: {copied_xml}")
    if sarif_path:
        print(f"🧾 SARIF: {sarif_path}")

    return result.returncode

def cmd_bench(args):
    bench_runner = ROOT / "scripts" / "bench_runner.py"
    if not bench_runner.exists():
        print(f"Error: bench runner not found: {bench_runner}")
        return 1

    hyperfine_path = shutil.which("hyperfine")
    if not hyperfine_path:
        print("⚠️ hyperfine not found; falling back to single-run measurements without statistical summaries.")

    perf_dir = EXPORTS / "perf"
    perf_dir.mkdir(parents=True, exist_ok=True)
    run_id = datetime.datetime.now(datetime.UTC).strftime("%Y%m%d_%H%M%S")
    run_dir = perf_dir / run_id
    logs_dir = run_dir / "logs"
    metrics_dir = run_dir / "metrics"
    hf_dir = run_dir / "hyperfine"
    for directory in (run_dir, logs_dir, metrics_dir, hf_dir):
        directory.mkdir(parents=True, exist_ok=True)

    build_dir = pathlib.Path(args.build_dir).resolve()
    source_dir = pathlib.Path.cwd()

    if args.fresh and not args.skip_configure and build_dir.exists():
        print(f"🧹 Removing existing build directory for fresh configure: {build_dir}")
        shutil.rmtree(build_dir)

    def _rel(path: pathlib.Path) -> str:
        try:
            return str(path.relative_to(EXPORTS))
        except ValueError:
            return str(path)

    def _collect_ccache_stats() -> Optional[Dict[str, Any]]:
        stats: Dict[str, Any] = {}
        ccache = shutil.which("ccache")
        if ccache:
            try:
                proc = subprocess.run([ccache, "--show-stats", "--format=json"], text=True, capture_output=True)
                if proc.returncode == 0 and proc.stdout.strip():
                    stats["ccache"] = json.loads(proc.stdout)
                else:
                    fallback = subprocess.run([ccache, "-s"], text=True, capture_output=True)
                    if fallback.returncode == 0:
                        stats["ccache_raw"] = fallback.stdout.strip()
            except Exception as exc:
                stats["ccache_error"] = str(exc)
        sccache = shutil.which("sccache")
        if sccache:
            try:
                proc = subprocess.run([sccache, "--show-stats"], text=True, capture_output=True)
                if proc.returncode == 0:
                    stats["sccache"] = proc.stdout.strip()
            except Exception as exc:
                stats.setdefault("sccache_errors", []).append(str(exc))
        return stats or None

    compile_index: Dict[str, str] = {}
    stage_results: Dict[str, Dict[str, Any]] = {}
    stage_order: List[str] = []
    exit_code = 0

    is_ninja = "ninja" in args.generator.lower()
    ninja_log_path = build_dir / ".ninja_log"
    ninja_entries: List[Dict[str, Any]] = []

    def run_stage(stage: str, command: List[str]) -> Tuple[Dict[str, Any], int]:
        metrics_file = metrics_dir / f"{stage}.jsonl"
        if metrics_file.exists():
            metrics_file.unlink()
        log_path = logs_dir / f"{stage}.log"

        wrapper_cmd: List[str] = [
            sys.executable,
            str(bench_runner),
            "--metrics", str(metrics_file),
            "--stage", stage,
            "--log", str(log_path),
        ]
        if args.keep_output:
            wrapper_cmd.append("--keep-output")
        wrapper_cmd.append("--")
        wrapper_cmd.extend(command)

        hf_json = hf_dir / f"{stage}.json"
        if hf_json.exists():
            hf_json.unlink()

        start_wall = time.perf_counter()
        if hyperfine_path:
            hf_cmd = [
                hyperfine_path,
                "--runs", str(max(args.runs, 1)),
                "--warmup", str(max(args.warmup, 0)),
                "--export-json", str(hf_json),
                "--style", "basic",
                "--command-name", stage,
                shlex.join(wrapper_cmd),
            ]
            proc = subprocess.run(hf_cmd, text=True)
            return_code = proc.returncode
        else:
            proc = subprocess.run(wrapper_cmd)
            return_code = proc.returncode
        elapsed = time.perf_counter() - start_wall

        run_metrics = _load_json_lines(metrics_file)
        durations = [float(m.get("duration_seconds", 0.0)) for m in run_metrics]
        peak_memory = None
        if run_metrics:
            peak_memory = max((float(m.get("peak_rss_kib", 0.0)) for m in run_metrics), default=0.0)

        hyperfine_summary = None
        if hyperfine_path and hf_json.exists():
            try:
                hf_data = json.loads(hf_json.read_text())
                if isinstance(hf_data, dict) and hf_data.get("results"):
                    res = hf_data["results"][0]
                    times = res.get("times") or []
                    hyperfine_summary = {
                        "mean_seconds": res.get("mean"),
                        "stddev_seconds": res.get("stddev"),
                        "min_seconds": res.get("min"),
                        "max_seconds": res.get("max"),
                        "median_seconds": res.get("median"),
                        "runs": len(times) if isinstance(times, list) else None,
                        "user_seconds": res.get("user"),
                        "system_seconds": res.get("system"),
                        "exit_codes": res.get("exit_codes"),
                        "json": _rel(hf_json),
                    }
            except Exception as exc:
                hyperfine_summary = {"error": str(exc)}

        duration_avg = statistics.mean(durations) if durations else elapsed
        stage_result: Dict[str, Any] = {
            "command": command,
            "command_str": shlex.join(command),
            "log": _rel(log_path),
            "metrics": _rel(metrics_file) if metrics_file.exists() else None,
            "hyperfine": hyperfine_summary,
            "runs": run_metrics,
            "return_code": int(run_metrics[-1].get("returncode", return_code)) if run_metrics else int(return_code),
            "duration_seconds": round(duration_avg, 4),
            "peak_memory_kib": round(peak_memory, 2) if peak_memory is not None else None,
        }
        return stage_result, stage_result["return_code"]

    # Configure stage
    if not args.skip_configure:
        configure_cmd = strict_configure_command(
            source_dir,
            build_dir,
            std=args.std,
            build_type=args.build_type,
            generator=args.generator,
            use_ccache=not args.no_ccache,
            enable_tidy=not args.no_tidy,
            tidy_checks=args.tidy_checks,
            extra_defines=args.cmake_arg,
            extra_args=args.cmake_extra,
        )
        cfg_result, cfg_rc = run_stage("configure", configure_cmd)
        stage_results["configure"] = cfg_result
        stage_order.append("configure")
        exit_code = cfg_rc
        if cfg_rc != 0:
            summary_path = run_dir / "bench.json"
            summary = {
                "_meta": {
                    "generated_at": datetime.datetime.now(datetime.UTC).isoformat(),
                    "run_id": run_id,
                    "runs": args.runs,
                    "warmup": args.warmup,
                    "std": args.std,
                    "build_type": args.build_type,
                    "generator": args.generator,
                    "strict_warnings": STRICT_WARNING_FLAGS,
                    "sanitizers": SANITIZER_FLAGS,
                },
                "stage_order": stage_order,
                "stages": stage_results,
                "artifacts": {
                    "run_dir": _rel(run_dir),
                    "logs": _rel(logs_dir),
                    "metrics": _rel(metrics_dir),
                },
            }
            write_json(summary_path, summary)
            write_json(perf_dir / "bench.json", summary)
            print(f"📊 Benchmark summary: {summary_path}")
            return exit_code
        compile_index = _load_compile_command_index(build_dir / "compile_commands.json")

    # Build stage
    if not args.skip_build:
        baseline = _ninja_log_raw_lines(ninja_log_path) if is_ninja else set()
        build_cmd = strict_build_command(
            build_dir,
            target=args.target,
            jobs=args.jobs,
            build_tool_args=args.build_arg,
        )
        build_result, build_rc = run_stage("build", build_cmd)
        stage_results["build"] = build_result
        stage_order.append("build")
        if build_rc != 0:
            exit_code = build_rc
        if is_ninja:
            new_entries, _ = _ninja_log_new_entries(ninja_log_path, baseline)
            ninja_entries = new_entries
        if not compile_index:
            compile_index = _load_compile_command_index(build_dir / "compile_commands.json")

    # Test stage
    if not args.skip_test and exit_code == 0:
        test_cmd = strict_test_command(
            build_dir,
            label=args.label,
            regex=args.regex,
            jobs=args.ctest_jobs,
            extra_args=args.ctest_arg,
        )
        test_result, test_rc = run_stage("test", test_cmd)
        stage_results["test"] = test_result
        stage_order.append("test")
        if test_rc != 0:
            exit_code = test_rc

    ccache_stats = _collect_ccache_stats()
    parallelism = _compute_parallelism(ninja_entries) if ninja_entries else None
    slow_units = _slow_translation_units(ninja_entries, compile_index, top_n=args.top_slowest)

    summary = {
        "_meta": {
            "generated_at": datetime.datetime.now(datetime.UTC).isoformat(),
            "run_id": run_id,
            "runs": args.runs,
            "warmup": args.warmup,
            "std": args.std,
            "build_type": args.build_type,
            "generator": args.generator,
            "strict_warnings": STRICT_WARNING_FLAGS,
            "sanitizers": SANITIZER_FLAGS,
        },
        "stage_order": stage_order,
        "stages": stage_results,
        "artifacts": {
            "run_dir": _rel(run_dir),
            "logs": _rel(logs_dir),
            "metrics": _rel(metrics_dir),
            "hyperfine": _rel(hf_dir),
        },
    }
    if hyperfine_path:
        summary["_meta"]["hyperfine"] = {"path": hyperfine_path}
    if ccache_stats:
        summary["ccache"] = ccache_stats
    if parallelism:
        summary["parallelism"] = parallelism
    if slow_units:
        summary["slow_translation_units"] = slow_units

    summary_path = run_dir / "bench.json"
    write_json(summary_path, summary)
    write_json(perf_dir / "bench.json", summary)
    print(f"📊 Benchmark summary: {summary_path}")
    return exit_code


def cmd_stderr_thin(args):
    diagnostics_dir = EXPORTS / "diagnostics"
    diagnostics_dir.mkdir(parents=True, exist_ok=True)

    budget = _DEFAULT_CONTEXT_BUDGETS.get(args.level, 6000)
    if args.context_budget and args.context_budget > 0:
        budget = args.context_budget

    json_out = pathlib.Path(args.json) if args.json else diagnostics_dir / "stderr-thin.json"
    text_out = pathlib.Path(args.text) if args.text else diagnostics_dir / "stderr-thin.txt"
    raw_out = pathlib.Path(args.raw) if args.raw else diagnostics_dir / "stderr-raw.txt"

    raw_stderr = ""
    stdout_text = ""
    run_info = {
        "mode": None,
        "command": None,
        "returncode": None,
        "duration_seconds": None,
    }

    compile_db_path = None
    structured_source = None

    if args.log:
        log_path = pathlib.Path(args.log)
        if not log_path.exists():
            print(f"Error: log file not found: {log_path}")
            return 1
        raw_stderr = log_path.read_text()
        run_info.update({
            "mode": "log",
            "log_path": str(log_path.resolve()),
        })
    elif args.compile is not None or args.compile_index is not None:
        compile_db_path = _find_compile_database(args.compile_db)
        if not compile_db_path or not compile_db_path.exists():
            hint = args.compile_db or "exports/compile_commands.json"
            print(f"Error: compile_commands.json not found (looked for {hint})")
            return 1
        try:
            compile_entries = json.loads(compile_db_path.read_text())
        except Exception as exc:
            print(f"Error: failed to read compile database: {exc}")
            return 1
        if not isinstance(compile_entries, list):
            print("Error: compile_commands.json has unexpected format")
            return 1

        entry = None
        selected_index = None
        if args.compile_index is not None:
            if 0 <= args.compile_index < len(compile_entries):
                entry = compile_entries[args.compile_index]
                selected_index = args.compile_index
            else:
                print(f"Error: compile index {args.compile_index} is out of range (size={len(compile_entries)})")
                return 1
        else:
            needle = (args.compile or "").lower()
            for idx, candidate in enumerate(compile_entries):
                haystack = " ".join(filter(None, [candidate.get("file"), candidate.get("command"), candidate.get("directory")]))
                if needle in haystack.lower():
                    entry = candidate
                    selected_index = idx
                    break
            if entry is None:
                print(f"Error: no compile command matched '{args.compile}'")
                return 1

        command = _prepare_compile_command(entry)
        if not command:
            print("Error: unable to prepare compile command arguments")
            return 1

        cwd = pathlib.Path(entry.get("directory") or pathlib.Path.cwd())
        if not cwd.exists():
            cwd = pathlib.Path.cwd()
        start = time.perf_counter()
        result = run(command, cwd=str(cwd), check=False)
        duration = time.perf_counter() - start
        raw_stderr = (result.stderr or "")
        stdout_text = result.stdout or ""
        run_info.update({
            "mode": "compile_command",
            "command": list(command),
            "returncode": result.returncode,
            "duration_seconds": round(duration, 6),
            "directory": str(cwd),
            "compile_db": str(compile_db_path.resolve()),
            "compile_entry_index": selected_index,
            "compile_file": entry.get("file"),
        })
    elif args.cmd:
        command = [c for c in args.cmd if c]
        if command and command[0] == "--":
            command = command[1:]
        if not command:
            print("Error: no command provided for stderr capture")
            return 1
        cwd = pathlib.Path(args.cwd).resolve() if args.cwd else pathlib.Path.cwd()
        if not cwd.exists():
            print(f"Error: working directory does not exist: {cwd}")
            return 1
        start = time.perf_counter()
        result = run(command, cwd=str(cwd), check=False)
        duration = time.perf_counter() - start
        raw_stderr = result.stderr or ""
        stdout_text = result.stdout or ""
        run_info.update({
            "mode": "command",
            "command": list(command),
            "returncode": result.returncode,
            "duration_seconds": round(duration, 6),
            "directory": str(cwd),
        })
    else:
        if sys.stdin and not sys.stdin.isatty():
            raw_stderr = sys.stdin.read()
            run_info.update({"mode": "stdin"})
        else:
            print("Error: provide --log, --compile/--compile-index, a command, or pipe stderr to STDIN")
            return 1

    lines = raw_stderr.splitlines()
    collapsed_lines, template_omitted = _collapse_template_traces(lines)

    diagnostics = _parse_structured_diagnostics(raw_stderr)
    if diagnostics:
        structured_source = "clang-json"
    else:
        diagnostics = _parse_text_diagnostics(lines)
        structured_source = "text-regex" if diagnostics else None

    highlights = _format_highlights(diagnostics) if diagnostics else []
    if not highlights:
        severity_lines = [line for line in collapsed_lines if _SEVERITY_REGEX.search(line)]
        highlights = _dedupe_preserve(severity_lines[:20])

    view_lines = []
    if args.level == "summary":
        view_lines = highlights or collapsed_lines[:20]
    elif args.level == "focused":
        view_lines = _focused_view_lines(collapsed_lines)
    else:
        view_lines = collapsed_lines

    if highlights and args.level != "detailed":
        view_lines = _dedupe_preserve(highlights + view_lines)

    view_lines = [line for line in view_lines if line is not None and line.strip()]

    context_text, used_len, full_len, truncated = _apply_context_budget(view_lines, budget)

    counts = _collect_counts(diagnostics) if diagnostics else {"error": 0, "warning": 0, "note": 0, "remark": 0, "other": 0}

    text_out.parent.mkdir(parents=True, exist_ok=True)
    raw_out.parent.mkdir(parents=True, exist_ok=True)
    text_out.write_text((context_text + "\n") if context_text else "")
    raw_out.write_text(raw_stderr)

    report = {
        "_meta": {
            "generated_at": datetime.datetime.now(datetime.UTC).isoformat(),
            "level": args.level,
            "context_budget": budget,
            "structured_source": structured_source or "none",
        },
        "run": run_info,
        "counts": counts,
        "view": {
            "path": str(text_out.resolve()),
            "level": args.level,
            "context_budget": budget,
            "context_used": used_len,
            "context_full": full_len,
            "context_truncated": truncated,
        },
        "raw": {
            "stderr_path": str(raw_out.resolve()),
            "stderr_chars": len(raw_stderr),
            "stdout_chars": len(stdout_text),
            "stdout_preview": (stdout_text.splitlines()[:5] if stdout_text else []),
        },
        "highlights": highlights,
        "diagnostics": diagnostics,
        "omissions": {
            "template_frames": template_omitted,
        },
    }

    json_out.parent.mkdir(parents=True, exist_ok=True)
    write_json(json_out, report)

    print(f"Thinned diagnostics written to {text_out}")
    print(str(json_out))


def cmd_lsp_bridge(args):
    """Bridge to clangd LSP for structured diagnostics with filtering."""
    try:
        from lsp_bridge import collect_diagnostics
    except ImportError:
        print("Error: LSP bridge module not available")
        return 1

    diagnostics_dir = EXPORTS / "diagnostics"
    diagnostics_dir.mkdir(parents=True, exist_ok=True)

    # Set up defaults similar to stderr-thin
    level = getattr(args, 'level', 'focused')
    budget = getattr(args, 'context_budget', None)
    if budget is None:
        budget = _DEFAULT_CONTEXT_BUDGETS.get(level, 6000)

    # Output file paths
    json_out = pathlib.Path(args.json) if getattr(args, 'json', None) else diagnostics_dir / "lsp-bridge.json"
    text_out = pathlib.Path(args.text) if getattr(args, 'text', None) else diagnostics_dir / "lsp-bridge.txt"

    # Determine files to analyze
    files = getattr(args, 'files', []) or []
    if not files:
        # Auto-discover from compile_commands.json
        compile_db_path = _find_compile_database(getattr(args, 'compile_db', None))
        if compile_db_path and compile_db_path.exists():
            try:
                compile_entries = json.loads(compile_db_path.read_text())
                files = [entry.get("file") for entry in compile_entries if entry.get("file")]
                files = [f for f in files if f and pathlib.Path(f).suffix in ['.cpp', '.cc', '.cxx', '.c', '.hpp', '.hh', '.hxx', '.h']]
                files = files[:20]  # Limit to first 20 files to avoid overwhelming LSP
            except Exception as e:
                print(f"Warning: Failed to read compile database: {e}")

        if not files:
            print("Error: No files specified and no compile_commands.json found")
            return 1

    # Collect diagnostics via LSP
    try:
        timeout = float(getattr(args, 'timeout', 30))
        server_path = getattr(args, 'server_path', None)

        print(f"Analyzing {len(files)} files via clangd LSP...")
        lsp_results = collect_diagnostics(
            files=files,
            compile_db_path=getattr(args, 'compile_db', None),
            server_path=server_path,
            timeout=timeout
        )

        # Apply filtering similar to stderr-thin
        all_diagnostics = lsp_results["diagnostics"]

        # Filter by severity if specified
        filter_severity = getattr(args, 'filter_severity', None)
        if filter_severity:
            severity_order = {"error": 1, "warning": 2, "information": 3, "hint": 4}
            min_level = severity_order.get(filter_severity.lower(), 4)
            all_diagnostics = [
                d for d in all_diagnostics
                if severity_order.get(d.get("level", "hint"), 4) <= min_level
            ]

        # Filter by category if specified
        filter_category = getattr(args, 'filter_category', None)
        if filter_category:
            all_diagnostics = [
                d for d in all_diagnostics
                if filter_category.lower() in (d.get("category") or "").lower() or
                   filter_category.lower() in (d.get("option") or "").lower()
            ]

        # Generate highlights using existing stderr-thin logic
        highlights = _format_highlights(all_diagnostics)

        # Apply view level filtering
        view_lines = []
        if level == "summary":
            view_lines = highlights[:20]
        elif level == "focused":
            # For LSP, focused view includes errors and warnings with context
            priority_diags = [d for d in all_diagnostics if d.get("level") in ["error", "warning"]]
            view_lines = _format_highlights(priority_diags)
        else:  # detailed
            view_lines = highlights

        # Apply context budget
        context_text, used_len, full_len, truncated = _apply_context_budget(view_lines, budget)

        # Recompute counts after filtering
        filtered_counts = _collect_counts(all_diagnostics)

        # Build final report
        report = {
            "_meta": {
                "generated_at": datetime.datetime.now(datetime.UTC).isoformat(),
                "server_version": lsp_results["server_info"].get("version"),
                "level": level,
                "context_budget": budget,
                "filter_severity": filter_severity,
                "filter_category": filter_category,
                "files_analyzed": len(lsp_results["files"]),
                "timeout_seconds": timeout
            },
            "server_info": lsp_results["server_info"],
            "files": lsp_results["files"],
            "counts": {
                # Map stderr-thin format to LSP format
                "error": filtered_counts.get("error", 0),
                "warning": filtered_counts.get("warning", 0),
                "information": filtered_counts.get("information", 0),
                "hint": filtered_counts.get("hint", 0),
                "total": sum(filtered_counts.values())
            },
            "diagnostics": all_diagnostics,
            "highlights": highlights,
            "view": {
                "path": str(text_out.resolve()),
                "level": level,
                "context_budget": budget,
                "context_used": used_len,
                "context_full": full_len,
                "context_truncated": truncated
            }
        }

        # Write outputs
        text_out.parent.mkdir(parents=True, exist_ok=True)
        json_out.parent.mkdir(parents=True, exist_ok=True)

        text_out.write_text((context_text + "\n") if context_text else "")
        write_json(json_out, report)

        print(f"LSP diagnostics written to {text_out}")
        print(str(json_out))

        return 0

    except Exception as e:
        print(f"Error during LSP analysis: {e}")
        return 1


def cmd_tidy(args):
    """Run clang-tidy with optional fix application."""
    compile_db = EXPORTS / "compile_commands.json"
    paths = args.paths or []

    clang_tidy = shutil.which("clang-tidy")
    if not clang_tidy:
        print("Error: clang-tidy not found in PATH")
        return 1

    if not compile_db.exists():
        print("Error: compile_commands.json not found. Run 'llmtk context export' first.")
        return 1

    # Collect files to analyze
    files_to_analyze = []
    if compile_db.exists():
        try:
            data = json.loads(compile_db.read_text())
            all_files = [pathlib.Path(entry.get("file", "")) for entry in data if entry.get("file")]

            if paths:
                # Filter by provided paths
                for file_path in all_files:
                    for path_filter in paths:
                        filter_path = pathlib.Path(path_filter).resolve()
                        try:
                            if filter_path.is_dir():
                                if file_path.is_relative_to(filter_path):
                                    files_to_analyze.append(file_path)
                            elif filter_path.is_file() and file_path.resolve() == filter_path:
                                files_to_analyze.append(file_path)
                        except Exception:
                            # Fallback for path matching
                            if str(file_path).startswith(str(filter_path)):
                                files_to_analyze.append(file_path)
            else:
                files_to_analyze = all_files

        except Exception as e:
            print(f"Error reading compile_commands.json: {e}")
            return 1

    if not files_to_analyze:
        print("No files found to analyze")
        return 0

    # Deduplicate while preserving order
    seen = set()
    unique_files = []
    for f in files_to_analyze:
        if str(f) not in seen:
            seen.add(str(f))
            unique_files.append(f)

    print(f"Running clang-tidy on {len(unique_files)} files...")

    # Build clang-tidy command
    cmd = [clang_tidy, "-p", str(compile_db.parent)]

    if args.checks:
        cmd.extend(["-checks", args.checks])

    if args.apply:
        cmd.append("--fix")
        print("Applying fixes automatically...")

    # Add files
    cmd.extend([str(f) for f in unique_files[:100]])  # Limit to prevent command line too long

    try:
        result = run(cmd, check=False)
        print(result.stdout)
        if result.stderr:
            print("Warnings/Errors:", file=sys.stderr)
            print(result.stderr, file=sys.stderr)

        if args.apply:
            print("Fixes applied. Review changes before committing.")

        return result.returncode
    except Exception as e:
        print(f"Error running clang-tidy: {e}")
        return 1

def cmd_format(args):
    """Run clang-format with check or apply options."""
    paths = args.paths or ["."]

    clang_format = shutil.which("clang-format")
    if not clang_format:
        print("Error: clang-format not found in PATH")
        return 1

    # Collect C++ files to format
    cpp_extensions = {".c", ".cc", ".cpp", ".cxx", ".c++", ".h", ".hh", ".hpp", ".hxx", ".inl"}
    files_to_format = []

    for path_str in paths:
        path = pathlib.Path(path_str)
        if path.is_file():
            if path.suffix in cpp_extensions:
                files_to_format.append(path)
        elif path.is_dir():
            for file_path in path.rglob("*"):
                if file_path.is_file() and file_path.suffix in cpp_extensions:
                    files_to_format.append(file_path)

    if not files_to_format:
        print("No C++ files found to format")
        return 0

    print(f"Found {len(files_to_format)} files to format")

    # Build clang-format command
    cmd = [clang_format]

    if args.style:
        cmd.extend(["-style", args.style])
    else:
        # Look for .clang-format file, default to LLVM if not found
        if pathlib.Path(".clang-format").exists():
            cmd.extend(["-style", "file"])
        else:
            cmd.extend(["-style", "LLVM"])

    if args.check:
        cmd.append("--dry-run")
        cmd.append("--Werror")
        print("Checking formatting (dry run)...")
    elif args.apply:
        cmd.append("-i")  # In-place modification
        print("Applying formatting changes...")
    else:
        # Default to check mode if neither specified
        cmd.append("--dry-run")
        cmd.append("--Werror")
        print("Checking formatting (dry run)... Use --apply to format files")

    # Run clang-format on files in batches to avoid command line length limits
    batch_size = 50
    total_errors = 0

    for i in range(0, len(files_to_format), batch_size):
        batch = files_to_format[i:i + batch_size]
        batch_cmd = cmd + [str(f) for f in batch]

        try:
            result = run(batch_cmd, check=False)

            if result.returncode != 0:
                total_errors += 1
                if args.check:
                    print(f"Formatting issues found in batch {i//batch_size + 1}")
                    if result.stdout:
                        print(result.stdout)
                    if result.stderr:
                        print(result.stderr)
                else:
                    print(f"Error formatting batch {i//batch_size + 1}: {result.stderr}")

        except Exception as e:
            print(f"Error running clang-format on batch {i//batch_size + 1}: {e}")
            total_errors += 1

    if args.check:
        if total_errors == 0:
            print("✅ All files are properly formatted")
            return 0
        else:
            print(f"❌ Found formatting issues in {total_errors} batches")
            return 1
    elif args.apply:
        if total_errors == 0:
            print("✅ Formatting applied successfully")
        else:
            print(f"⚠️ Completed with {total_errors} errors")
        return total_errors
    else:
        return total_errors

def cmd_gate(args):
    """Enforce SARIF severity budgets for CI gating."""
    sarif_file = pathlib.Path(args.sarif_file)

    if not sarif_file.exists():
        print(f"Error: SARIF file not found: {sarif_file}")
        return 1

    # Load configuration if provided
    config = {
        "max_errors": args.max_errors,
        "max_warnings": args.max_warnings,
        "max_notes": args.max_notes
    }

    if args.config:
        config_file = pathlib.Path(args.config)
        if config_file.exists():
            try:
                config_data = load_yaml(config_file)
                if config_data:
                    config.update(config_data)
                print(f"Loaded configuration from {config_file}")
            except Exception as e:
                print(f"Warning: Failed to load config file {config_file}: {e}")

    # Load and analyze SARIF file
    try:
        with open(sarif_file) as f:
            sarif_doc = json.load(f)
    except Exception as e:
        print(f"Error: Failed to read SARIF file: {e}")
        return 1

    # Count results by severity
    counts = {"error": 0, "warning": 0, "note": 0, "unknown": 0}
    total_results = 0

    for run in sarif_doc.get("runs", []):
        for result in run.get("results", []):
            level = result.get("level", "warning").lower()
            counts[level] = counts.get(level, 0) + 1
            total_results += 1

    # Report findings
    print(f"📊 SARIF Analysis Results")
    print(f"=" * 40)
    print(f"Total results: {total_results}")
    print(f"Errors:        {counts['error']:3d} (limit: {config['max_errors']})")
    print(f"Warnings:      {counts['warning']:3d} (limit: {config['max_warnings']})")
    print(f"Notes:         {counts['note']:3d} (limit: {config['max_notes']})")
    print(f"Unknown:       {counts['unknown']:3d}")

    # Check against limits
    violations = []
    if counts["error"] > config["max_errors"]:
        violations.append(f"Errors: {counts['error']} > {config['max_errors']}")
    if counts["warning"] > config["max_warnings"]:
        violations.append(f"Warnings: {counts['warning']} > {config['max_warnings']}")
    if counts["note"] > config["max_notes"]:
        violations.append(f"Notes: {counts['note']} > {config['max_notes']}")

    if violations:
        print(f"\n❌ GATE FAILED - Budget violations:")
        for violation in violations:
            print(f"   • {violation}")
        return 1
    else:
        print(f"\n✅ GATE PASSED - All severity budgets within limits")
        return 0

def cmd_reduce(args):
    repros = EXPORTS/"repros"; repros.mkdir(exist_ok=True)
    cvise = shutil.which("cvise")
    creduce = shutil.which("creduce")
    reducer = None
    reducer_name = None
    if cvise:
        reducer = cvise
        reducer_name = "cvise"
    elif creduce:
        reducer = creduce
        reducer_name = "creduce"

    report = {
        "input": args.input,
        "test_cmd": args.test_cmd,
        "cvise_available": bool(cvise),
        "creduce_available": bool(creduce),
        "reducer_used": reducer_name,
        "timeout": args.timeout,
        "sanitizer": args.sanitizer
    }

    if not reducer:
        report["note"] = "No reducer found; please run `llmtk install cvise` or `llmtk install creduce`"
        write_json(repros/"report.json", report)
        print(str(repros/"report.json"))
        return

    # Build the command
    cmd = [reducer]
    if args.timeout:
        cmd.extend(["--timeout", str(args.timeout)])
    cmd.extend([args.input, "--", "bash", "-lc"])

    test_cmd = args.test_cmd
    if args.sanitizer:
        if args.sanitizer == "asan":
            test_cmd = f"ASAN_OPTIONS=detect_leaks=0 {test_cmd}"
        elif args.sanitizer == "ubsan":
            test_cmd = f"UBSAN_OPTIONS=print_stacktrace=1 {test_cmd}"
        elif args.sanitizer == "tsan":
            test_cmd = f"TSAN_OPTIONS=report_atomic_races=1 {test_cmd}"
    cmd.append(test_cmd)

    # Minimal shell-out; users can expand
    try:
        result = run(cmd, check=False)
        report["note"] = f"{reducer_name} run completed"
        report["returncode"] = result.returncode
        
        post_mortem_path = repros/"post-mortem.txt"
        with open(post_mortem_path, "w") as f:
            f.write(f"# {reducer_name} Post-Mortem\n\n")
            f.write(f"## Configuration\n")
            f.write(f"- Input: {args.input}\n")
            f.write(f"- Test Command: {args.test_cmd}\n")
            f.write(f"- Timeout: {args.timeout}\n")
            f.write(f"- Sanitizer: {args.sanitizer}\n\n")
            f.write(f"## Output\n")
            f.write(f"### STDOUT\n```\n{result.stdout}\n```\n\n")
            f.write(f"### STDERR\n```\n{result.stderr}\n```\n")
        report["post_mortem"] = str(post_mortem_path)

    except Exception as e:
        report["error"] = str(e)
    write_json(repros/"report.json", report)
    print(str(repros/"report.json"))

def detect_package_manager():
    """Detect available package manager"""
    managers = [
        ("apt", ["apt-get", "apt"]),
        ("dnf", ["dnf"]),
        ("pacman", ["pacman"]),
        ("brew", ["brew"]),
        ("nix", ["nix-env"])
    ]

    for name, commands in managers:
        for cmd in commands:
            if shutil.which(cmd):
                return name
    return None

def install_tool_with_package_manager(tool_name, tool_config, pm):
    """Install a tool using system package manager"""
    if "install" not in tool_config or pm not in tool_config["install"]:
        return False

    packages = tool_config["install"][pm]
    if not packages:
        return False

    print(f"  📦 Installing {tool_name} via {pm}...")

    # Build install command based on package manager
    if pm == "apt":
        cmd = ["sudo", "apt-get", "update", "&&", "sudo", "apt-get", "install", "-y"] + packages
    elif pm == "dnf":
        cmd = ["sudo", "dnf", "install", "-y"] + packages
    elif pm == "pacman":
        cmd = ["sudo", "pacman", "-S", "--noconfirm"] + packages
    elif pm == "brew":
        cmd = ["brew", "install"] + packages
    elif pm == "nix":
        cmd = ["nix-env", "-iA"] + [f"nixpkgs.{pkg}" for pkg in packages]
    else:
        return False

    try:
        if pm == "apt":
            # Handle apt's compound command - suppress normal output but show errors
            update_result = subprocess.run(["sudo", "apt-get", "update"],
                                         text=True, capture_output=True)
            if update_result.returncode != 0:
                # Only show if it's a real error, not just GPG warnings
                if "E:" in update_result.stderr and "NO_PUBKEY" not in update_result.stderr:
                    print(f"    ⚠️ apt-get update warning (continuing): {update_result.stderr.strip()}")

            result = subprocess.run(["sudo", "apt-get", "install", "-y"] + packages,
                                  check=True, text=True, capture_output=True)
        elif pm in ["dnf", "pacman"]:
            result = subprocess.run(cmd, check=True, text=True, capture_output=True)
        else:
            result = subprocess.run(cmd, check=True, text=True, capture_output=True)

        print(f"    ✅ {tool_name} installed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"    ❌ Failed to install {tool_name}")

        # Show specific error for debugging
        if e.stderr and e.stderr.strip():
            # Extract key error messages, skip verbose output
            error_lines = [line.strip() for line in e.stderr.split('\n')
                          if line.strip() and ('E:' in line or 'Error:' in line or 'unable to locate' in line.lower())]
            if error_lines:
                print(f"       Error: {error_lines[0]}")

        return False

def install_tool_locally(tool_name, tool_config, local_bin):
    """Install a tool locally"""
    if "local_install" not in tool_config:
        print(f"    ❌ No local install method for {tool_name}")
        return False

    print(f"  🔧 Installing {tool_name} locally...")
    local_config = tool_config["local_install"]

    if "github_repo" in local_config:
        success = install_from_github(tool_name, tool_config, local_bin)
        if success:
            print(f"    ✅ {tool_name} installed locally")
        else:
            print(f"    ❌ Failed to install {tool_name} locally")
        return success

    return False

def install_tool_basic(tool_name, local_bin):
    """Basic fallback installation for specific tools"""
    if tool_name == "cppcheck":
        script = MODULES / "simple-install.sh"
        if script.exists():
            result = run([str(script)], check=False)
            return result.returncode == 0
    elif tool_name == "include-what-you-use":
        script = MODULES / "simple-install.sh"
        if script.exists():
            result = run([str(script)], check=False)
            return result.returncode == 0

    print(f"No basic installation method available for {tool_name}")
    return False

def install_from_github(tool_name, config, local_bin):
    """Install tool from GitHub releases using manifest configuration"""
    local_config = config.get("local_install", {})
    repo = local_config.get("github_repo")

    if not repo:
        print(f"No GitHub repo specified for {tool_name}")
        return False

    print(f"Installing {tool_name} locally from {repo}")

    # Use the enhanced local installer with manifest data
    enhanced_installer = MODULES / "enhanced-install.sh"
    if enhanced_installer.exists():
        # Pass manifest data as environment variables
        env = os.environ.copy()
        env.update({
            "LLMTK_TOOL_NAME": tool_name,
            "LLMTK_GITHUB_REPO": repo,
            "LLMTK_RELEASE_PATTERN": local_config.get("release_pattern", ""),
            "LLMTK_BINARY_PATH": local_config.get("binary_path", ""),
            "LLMTK_BUILD_METHOD": local_config.get("build_method", ""),
            "LLMTK_LOCAL_BIN": str(local_bin),
            "LLMTK_MANIFEST_DATA": json.dumps(local_config)
        })

        # Add checksums as JSON
        if "checksums" in local_config:
            env["LLMTK_CHECKSUMS"] = json.dumps(local_config["checksums"])

        # Add version tag if specified
        if "version_tag" in local_config:
            env["LLMTK_VERSION_TAG"] = local_config["version_tag"]

        result = subprocess.run([str(enhanced_installer), tool_name], env=env, text=True, capture_output=True)

        # Show stderr output for user feedback
        if result.stderr:
            for line in result.stderr.strip().split('\n'):
                if line.strip():
                    print(line)

        return result.returncode == 0

    # Fallback to basic installation if enhanced installer not available
    print(f"Enhanced installer not found, using basic method for {tool_name}")
    return install_tool_basic(tool_name, local_bin)

def cmd_install(args):
    """Install missing tools using manifest-driven approach"""
    tools_manifest = ROOT / "manifest" / "tools.yaml"
    if not tools_manifest.exists():
        print(f"Error: Tools manifest not found at {tools_manifest}", file=sys.stderr)
        return 1

    # Load tools manifest
    tools_config = load_yaml(tools_manifest)
    if not tools_config or "tools" not in tools_config:
        print("Error: Invalid tools manifest", file=sys.stderr)
        return 1

    # Determine installation method
    use_local = getattr(args, 'local', False)
    pm = None if use_local else detect_package_manager()

    if not use_local and not pm:
        print("No package manager detected, falling back to local installation")
        use_local = True

    # Prepare local bin directory
    local_bin = ROOT / ".llmtk" / "bin"
    local_bin.mkdir(parents=True, exist_ok=True)

    # Install missing tools
    tools_to_install = []
    if hasattr(args, 'tools') and args.tools:
        tools_to_install = args.tools
    else:
        # Install all core and recommended tools that are missing
        for tool_name, tool_config in tools_config["tools"].items():
            if tool_config.get("role") in ["core", "recommended"]:
                if not shutil.which(tool_name):
                    tools_to_install.append(tool_name)

    if not tools_to_install:
        print("All tools are already installed")
        cmd_doctor(None)
        return 0

    print(f"🚀 Installing {len(tools_to_install)} missing tools...")
    print(f"   Method: {'local' if use_local else f'package manager ({pm})'}")
    print()

    installed = []
    failed = []
    skipped = []

    for tool_name in tools_to_install:
        if tool_name not in tools_config["tools"]:
            print(f"⚠️ {tool_name} not found in manifest, skipping")
            skipped.append(tool_name)
            continue

        tool_config = tools_config["tools"][tool_name]

        if use_local:
            if install_tool_locally(tool_name, tool_config, local_bin):
                installed.append(tool_name)
            else:
                # Fall back to simple installer for specific tools
                if tool_name in ["cppcheck", "include-what-you-use"]:
                    script = MODULES / "simple-install.sh"
                    if script.exists():
                        print(f"  🔄 Using fallback installer for {tool_name}")
                        result = run([str(script)], check=False)
                        if result.returncode == 0:
                            installed.append(tool_name)
                        else:
                            failed.append(tool_name)
                    else:
                        failed.append(tool_name)
                else:
                    failed.append(tool_name)
        else:
            if install_tool_with_package_manager(tool_name, tool_config, pm):
                installed.append(tool_name)
            else:
                print(f"  🔄 Falling back to local install for {tool_name}")
                if install_tool_locally(tool_name, tool_config, local_bin):
                    installed.append(tool_name)
                else:
                    failed.append(tool_name)

    # Update PATH for doctor check
    if local_bin.exists():
        old_path = os.environ.get("PATH", "")
        os.environ["PATH"] = f"{local_bin}:{old_path}"

    # Print comprehensive summary
    print()
    print("=" * 60)
    print("📊 INSTALLATION SUMMARY")
    print("=" * 60)

    if installed:
        print(f"✅ Successfully installed ({len(installed)}):")
        for tool in installed:
            print(f"   • {tool}")

    if failed:
        print(f"\n❌ Failed to install ({len(failed)}):")
        for tool in failed:
            print(f"   • {tool}")

    if skipped:
        print(f"\n⚠️ Skipped ({len(skipped)}):")
        for tool in skipped:
            print(f"   • {tool}")

    print(f"\n📈 Total: {len(installed)}/{len(tools_to_install)} tools successfully installed")

    # Generate updated doctor report
    print("\n🔍 Running health check...")
    # Create a dummy args object to signal we're being called from install
    class DummyArgs:
        _from_install = True
    cmd_doctor(DummyArgs())

    # Show next steps
    if failed:
        print("\n🛠️ NEXT STEPS:")
        print("   • Run 'llmtk install --local' to try local installation")
        print("   • Check 'llmtk doctor' output for specific issues")
        print("   • Install failed tools manually")

    if use_local or any(local_bin.glob("*")):
        print(f"\n📁 Local tools directory: {local_bin}")
        print(f"   Add to your shell: export PATH=\"{local_bin}:$PATH\"")

    return 0 if installed else 1

def cmd_cache(args):
    """Manage the analysis cache."""
    cache_dir = ROOT / ".llmtk" / "cache" / "analyze"
    if args.cache_cmd == "clear":
        if cache_dir.exists():
            shutil.rmtree(cache_dir)
            print(f"Cache cleared at {cache_dir}")
        else:
            print("Cache is already empty.")
    elif args.cache_cmd == "show":
        if cache_dir.exists():
            print(f"Cache contents at {cache_dir}:")
            for item in cache_dir.iterdir():
                print(f"- {item.name}")
        else:
            print("Cache is empty.")

def cmd_kb(args):
    """Query the error knowledge base."""
    kb_file = ROOT / "manifest" / "knowledge_base" / "diagnostics.yaml"
    if kb_file.exists():
        kb_data = load_yaml(kb_file)
        if kb_data:
            print(json.dumps(kb_data, indent=2))
        else:
            print("Error: Could not load knowledge base.")
    else:
        print("Error: Knowledge base not found.")


class AgentOperationError(Exception):
    """Raised when an agent request cannot be fulfilled."""


def _resolve_agent_path(
    path_str: Optional[str],
    *,
    base: Optional[pathlib.Path] = None,
    allow_nonexistent: bool = False,
    expect_directory: Optional[bool] = None,
) -> pathlib.Path:
    if not path_str:
        raise AgentOperationError("Missing path parameter")

    base_path = (base or PROJECT_ROOT).resolve()
    candidate = pathlib.Path(path_str)
    if not candidate.is_absolute():
        candidate = (base_path / candidate).resolve()
    else:
        candidate = candidate.resolve()

    if candidate != base_path and base_path not in candidate.parents:
        raise AgentOperationError("Access is restricted to the current working directory")

    if not allow_nonexistent and not candidate.exists():
        raise AgentOperationError(f"Path not found: {candidate}")

    if candidate.exists() and expect_directory is not None:
        if expect_directory and not candidate.is_dir():
            raise AgentOperationError(f"Expected a directory path: {candidate}")
        if not expect_directory and candidate.is_dir():
            raise AgentOperationError(f"Expected a file path: {candidate}")

    if allow_nonexistent:
        parent = candidate.parent
        if parent and parent != base_path and base_path not in parent.parents:
            raise AgentOperationError("Parent directory is outside the project scope")

    return candidate


def _agent_read_file(params: Dict[str, Any]) -> Dict[str, Any]:
    file_path = _resolve_agent_path(params.get("path"), expect_directory=False)
    try:
        content = file_path.read_text()
    except Exception as exc:
        raise AgentOperationError(str(exc)) from exc
    return {"path": str(file_path), "content": content}


def _agent_list_directory(params: Dict[str, Any]) -> Dict[str, Any]:
    dir_path = _resolve_agent_path(params.get("path", "."), expect_directory=True)
    entries = []
    try:
        for entry in sorted(dir_path.iterdir(), key=lambda p: p.name.lower()):
            info: Dict[str, Any] = {
                "name": entry.name,
                "path": str(entry),
                "is_dir": entry.is_dir(),
            }
            if entry.is_file():
                try:
                    info["size"] = entry.stat().st_size
                except Exception:
                    info["size"] = None
            entries.append(info)
    except Exception as exc:
        raise AgentOperationError(str(exc)) from exc
    return {"path": str(dir_path), "entries": entries}


def _agent_write_file(params: Dict[str, Any]) -> Dict[str, Any]:
    file_path = _resolve_agent_path(
        params.get("path"),
        allow_nonexistent=True,
        expect_directory=False,
    )
    if "content" not in params:
        raise AgentOperationError("Missing content parameter")
    content = params.get("content")
    try:
        file_path.parent.mkdir(parents=True, exist_ok=True)
        file_path.write_text(content)
    except Exception as exc:
        raise AgentOperationError(str(exc)) from exc
    return {"path": str(file_path)}


def _agent_delete_file(params: Dict[str, Any]) -> Dict[str, Any]:
    file_path = _resolve_agent_path(params.get("path"), expect_directory=False)
    if not file_path.exists():
        raise AgentOperationError(f"File not found: {file_path}")
    try:
        file_path.unlink()
    except Exception as exc:
        raise AgentOperationError(str(exc)) from exc
    return {"path": str(file_path)}


def _agent_get_capabilities(params: Dict[str, Any]) -> Dict[str, Any]:
    target = params.get("output")
    if target:
        out_path = _resolve_agent_path(target, allow_nonexistent=True, expect_directory=False)
    else:
        out_path = EXPORTS / "capabilities.json"
    generated = generate_capabilities_json(out_path)
    try:
        data = json.loads(generated.read_text())
    except Exception as exc:
        raise AgentOperationError(f"Failed to read capabilities: {exc}") from exc
    return {"path": str(generated), "capabilities": data}


def _agent_list_exports(params: Dict[str, Any]) -> Dict[str, Any]:
    subpath = params.get("path")
    base = EXPORTS
    if subpath:
        base = _resolve_agent_path(subpath, base=EXPORTS, expect_directory=True)

    pattern = params.get("glob")
    if pattern:
        matches = sorted(base.glob(pattern))
    else:
        matches = sorted(base.iterdir()) if base.exists() else []

    entries: List[Dict[str, Any]] = []
    for item in matches:
        if EXPORTS not in item.parents and item != EXPORTS:
            continue
        rel = item.relative_to(EXPORTS)
        info: Dict[str, Any] = {
            "name": item.name,
            "path": str(item),
            "relative": str(rel),
            "is_dir": item.is_dir(),
        }
        if item.is_file():
            try:
                info["size"] = item.stat().st_size
            except Exception:
                info["size"] = None
        entries.append(info)
    return {"root": str(base), "entries": entries}


def _agent_expand_context(params: Dict[str, Any]) -> Dict[str, Any]:
    build_dir = params.get("build", "build")
    preview = bool(params.get("preview"))
    require_cmake = bool(params.get("require_cmake"))
    level = params.get("level")
    deep = bool(params.get("deep"))
    if not deep and isinstance(level, str):
        deep = level.lower() in {"deep", "full"}

    args = argparse.Namespace(
        build=build_dir,
        deep=deep,
        preview=preview,
        require_cmake=require_cmake,
    )

    buffer = io.StringIO()
    with contextlib.redirect_stdout(buffer):
        result = cmd_context_export(args)
    rc = 0 if result in (None, 0) else result
    stdout_log = buffer.getvalue().strip()

    summary_path = EXPORTS / "context.json"
    summary: Optional[Dict[str, Any]] = None
    if summary_path.exists():
        try:
            summary = json.loads(summary_path.read_text())
        except Exception as exc:
            raise AgentOperationError(f"Context summary unreadable: {exc}") from exc
    elif not preview:
        raise AgentOperationError("Context export did not produce exports/context.json")

    payload = {
        "return_code": rc,
        "stdout": stdout_log,
        "preview": preview,
        "context_json": str(summary_path) if summary_path.exists() else None,
        "summary": summary,
    }

    if rc not in (0, None):
        raise AgentOperationError(json.dumps(payload))
    return payload


def _agent_dispatch(req: Dict[str, Any]) -> Dict[str, Any]:
    kind = req.get("kind")
    params = req.get("params", {})
    if kind == "read_file":
        return _agent_read_file(params)
    if kind == "list_directory":
        return _agent_list_directory(params)
    if kind == "write_file":
        return _agent_write_file(params)
    if kind == "delete_file":
        return _agent_delete_file(params)
    if kind == "get_capabilities":
        return _agent_get_capabilities(params)
    if kind == "list_exports":
        return _agent_list_exports(params)
    if kind == "expand_context":
        return _agent_expand_context(params)
    raise AgentOperationError(f"Unknown request kind: {kind}")


def _process_agent_requests(requests: Iterable[Dict[str, Any]]) -> List[Dict[str, Any]]:
    responses: List[Dict[str, Any]] = []
    for req in requests:
        req_id = req.get("id")
        try:
            payload = _agent_dispatch(req)
            responses.append({"id": req_id, "status": "success", "payload": payload})
        except AgentOperationError as exc:
            responses.append({"id": req_id, "status": "error", "payload": {"message": str(exc)}})
        except Exception as exc:
            traceback.print_exc()
            responses.append({
                "id": req_id,
                "status": "error",
                "payload": {"message": f"Unhandled exception: {exc}"},
            })
    return responses


MCP_PROTOCOL_VERSION = "2024-11-05"
_MCP_TOOL_DEFINITIONS = {
    "llmtk.read_file": {
        "kind": "read_file",
        "description": "Read a file from the current workspace.",
        "input_schema": {
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "File path (absolute or relative to workspace)"}
            },
            "required": ["path"],
            "additionalProperties": False,
        },
    },
    "llmtk.list_directory": {
        "kind": "list_directory",
        "description": "List directory entries within the workspace.",
        "input_schema": {
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "Directory path (default: current directory)"}
            },
            "additionalProperties": False,
        },
    },
    "llmtk.write_file": {
        "kind": "write_file",
        "description": "Write content to a file inside the workspace.",
        "input_schema": {
            "type": "object",
            "properties": {
                "path": {"type": "string"},
                "content": {"type": "string"},
            },
            "required": ["path", "content"],
            "additionalProperties": False,
        },
    },
    "llmtk.delete_file": {
        "kind": "delete_file",
        "description": "Delete a file from the workspace.",
        "input_schema": {
            "type": "object",
            "properties": {
                "path": {"type": "string"}
            },
            "required": ["path"],
            "additionalProperties": False,
        },
    },
    "llmtk.get_capabilities": {
        "kind": "get_capabilities",
        "description": "Generate and return manifest-driven capabilities metadata.",
        "input_schema": {
            "type": "object",
            "properties": {
                "output": {"type": "string", "description": "Optional override for output path"}
            },
            "additionalProperties": False,
        },
    },
    "llmtk.list_exports": {
        "kind": "list_exports",
        "description": "Enumerate files under the exports/ directory.",
        "input_schema": {
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "Subdirectory under exports/"},
                "glob": {"type": "string", "description": "Optional glob pattern"},
            },
            "additionalProperties": False,
        },
    },
    "llmtk.expand_context": {
        "kind": "expand_context",
        "description": "Run context export to gather build intelligence for agents.",
        "input_schema": {
            "type": "object",
            "properties": {
                "build": {"type": "string", "description": "Build directory (default: build)"},
                "deep": {"type": "boolean", "description": "Whether to perform a deep export"},
                "level": {"type": "string", "enum": ["basic", "focused", "deep", "full"]},
                "preview": {"type": "boolean"},
                "require_cmake": {"type": "boolean"},
            },
            "additionalProperties": False,
        },
    },
}

def cmd_agent(args):
    """Handle agent requests from JSON input."""
    raw = args.json_request
    if raw == "-":
        payload = sys.stdin.read()
    elif raw.startswith("@"):
        payload = pathlib.Path(raw[1:]).read_text()
    else:
        payload = raw

    try:
        request_data = json.loads(payload)
    except json.JSONDecodeError as exc:
        print(json.dumps({"version": 1, "error": f"Invalid JSON request: {exc}"}), file=sys.stderr)
        return 1

    requests = request_data.get("requests")
    if requests is None:
        print(json.dumps({"version": 1, "error": "Request payload must include a 'requests' array"}), file=sys.stderr)
        return 1

    responses = _process_agent_requests(requests)
    print(json.dumps({
        "version": 1,
        "generated_at": datetime.datetime.now(datetime.UTC).isoformat(),
        "responses": responses,
    }, indent=2))
    return 0

def cmd_agent_serve(args):
    """Start a persistent server for agent requests."""
    from http.server import BaseHTTPRequestHandler, HTTPServer

    class AgentRequestHandler(BaseHTTPRequestHandler):
        def do_POST(self):
            if self.path != '/':
                self.send_error(404)
                return

            content_length = int(self.headers.get('Content-Length', 0))
            raw = self.rfile.read(content_length).decode('utf-8') if content_length else ''
            try:
                request_payload = json.loads(raw or "{}")
            except json.JSONDecodeError as exc:
                body = json.dumps({"error": f"Invalid JSON: {exc}"}).encode('utf-8')
                self.send_response(400)
                self.send_header('Content-Type', 'application/json')
                self.end_headers()
                self.wfile.write(body)
                return

            requests = request_payload.get("requests") or []
            responses = _process_agent_requests(requests)
            response_body = json.dumps({
                "version": 1,
                "generated_at": datetime.datetime.now(datetime.UTC).isoformat(),
                "responses": responses,
            }).encode('utf-8')

            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.send_header('Content-Length', str(len(response_body)))
            self.end_headers()
            self.wfile.write(response_body)

    host = args.host
    port = args.port
    server_address = (host, port)
    httpd = HTTPServer(server_address, AgentRequestHandler)
    
    print(f"Starting agent server on http://{host}:{port}")
    try:
        httpd.serve_forever()
    except KeyboardInterrupt:
        print("\nShutting down server.")
        httpd.server_close()


def cmd_agent_mcp(_args):
    """Expose agent operations over the Model Context Protocol (stdio)."""

    def send(message: Dict[str, Any]) -> None:
        try:
            sys.stdout.write(json.dumps(message) + "\n")
            sys.stdout.flush()
        except Exception:
            traceback.print_exc()

    tool_list = [
        {
            "name": name,
            "description": meta["description"],
            "inputSchema": meta["input_schema"],
        }
        for name, meta in _MCP_TOOL_DEFINITIONS.items()
    ]

    while True:
        raw = sys.stdin.readline()
        if not raw:
            break
        raw = raw.strip()
        if not raw:
            continue
        try:
            message = json.loads(raw)
        except json.JSONDecodeError:
            send({
                "jsonrpc": "2.0",
                "id": None,
                "error": {"code": -32700, "message": "Failed to decode JSON"},
            })
            continue

        method = message.get("method")
        msg_id = message.get("id")
        params = message.get("params", {})

        if method == "initialize":
            send({
                "jsonrpc": "2.0",
                "id": msg_id,
                "result": {
                    "protocolVersion": MCP_PROTOCOL_VERSION,
                    "serverInfo": {"name": "llmtk", "version": get_version()},
                    "capabilities": {
                        "tools": {
                            "list": True,
                            "call": True,
                        }
                    },
                },
            })
        elif method == "initialized":
            # Notification acknowledgement; no response required.
            continue
        elif method == "tools/list":
            send({
                "jsonrpc": "2.0",
                "id": msg_id,
                "result": {"tools": tool_list},
            })
        elif method == "tools/call":
            tool_name = params.get("name")
            arguments = params.get("arguments") or {}
            tool_meta = _MCP_TOOL_DEFINITIONS.get(tool_name)
            if not tool_meta:
                send({
                    "jsonrpc": "2.0",
                    "id": msg_id,
                    "error": {"code": -32001, "message": f"Unknown tool: {tool_name}"},
                })
                continue

            request = {"kind": tool_meta["kind"], "params": arguments}
            try:
                payload = _agent_dispatch(request)
            except AgentOperationError as exc:
                send({
                    "jsonrpc": "2.0",
                    "id": msg_id,
                    "error": {"code": -32002, "message": str(exc)},
                })
            except Exception as exc:
                traceback.print_exc()
                send({
                    "jsonrpc": "2.0",
                    "id": msg_id,
                    "error": {"code": -32603, "message": f"Unhandled exception: {exc}"},
                })
            else:
                send({
                    "jsonrpc": "2.0",
                    "id": msg_id,
                    "result": {
                        "content": [
                            {
                                "type": "application/json",
                                "data": payload,
                            }
                        ]
                    },
                })
        elif method == "ping":
            send({"jsonrpc": "2.0", "id": msg_id, "result": {"ok": True}})
        elif method == "shutdown":
            send({"jsonrpc": "2.0", "id": msg_id, "result": {}})
            break
        else:
            if msg_id is not None:
                send({
                    "jsonrpc": "2.0",
                    "id": msg_id,
                    "error": {"code": -32601, "message": f"Unknown method: {method}"},
                })
def main():
    ap = argparse.ArgumentParser(prog="llmtk", description="LLM-friendly C++/CMake toolkit")
    ap.add_argument("--version", action="version", version=f"llmtk {get_version()}")
    sub = ap.add_subparsers(dest="cmd", required=True)
    doctor_parser = sub.add_parser("doctor", help="Check tool availability and CMake compliance")
    doctor_parser.add_argument("--cmake", action="store_true", help="Focus on CMake compliance only")
    doctor_parser.set_defaults(fn=cmd_doctor)

    cx = sub.add_parser("context", help="context commands").add_subparsers(dest="sub", required=True)
    cx_exp = cx.add_parser("export", help="Export build context for LLMs")
    cx_exp.add_argument("--build", default="build", help="Build directory path")
    cx_exp.add_argument("--require-cmake", action="store_true", help="Require CMAKE_GUIDANCE.md compliance")
    cx_exp.add_argument("--deep", action="store_true", help="Perform a deep context export, gathering more detailed information.")
    cx_exp.add_argument("--preview", action="store_true", help="Show planned context export operations without running them")
    cx_exp.set_defaults(fn=cmd_context_export)
    cx_pack = cx.add_parser("pack", help="Pack context export into a tarball")
    cx_pack.add_argument("--redact", action="store_true", help="Redact sensitive information from the export")
    cx_pack.set_defaults(fn=cmd_context_pack)

    an = sub.add_parser("analyze", help="Run static analysis tools")
    an.add_argument("paths", nargs="*", help="Paths to analyze")
    an.add_argument("--sarif", action="store_true", help="Output results in SARIF format")
    an.add_argument("--incremental", action="store_true", help="Analyze only changed files")
    an.add_argument("--cache-key", help="Manual cache key for analysis results")
    an.set_defaults(fn=cmd_analyze)

    test_parser = sub.add_parser("test", help="Run tests via CTest with structured exports")
    test_parser.add_argument("--build-dir", "-B", default="build", help="CMake build directory containing CTest files")
    test_parser.add_argument("--config", help="CTest build configuration (Debug, Release, etc.)")
    test_parser.add_argument("--regex", "-R", help="Run tests matching this regular expression")
    test_parser.add_argument("--exclude", "-E", help="Exclude tests matching this regular expression")
    test_parser.add_argument("--label", "-L", help="Run tests with a matching label regex")
    test_parser.add_argument("--parallel", "-j", type=int, help="Number of parallel test jobs")
    test_parser.add_argument("--timeout", type=int, help="Timeout in seconds for each test")
    test_parser.add_argument("--rerun-failed", action="store_true", help="Re-run tests that failed in the previous run")
    test_parser.add_argument("--stop-on-failure", action="store_true", help="Stop execution after the first failing test")
    test_parser.add_argument("--json", nargs="?", const="", help="Write JSON summary to this path (defaults to exports/tests/ctest_results.json)")
    test_parser.add_argument("--sarif", nargs="?", const="", help="Write SARIF report to this path (defaults to exports/tests/ctest_results.sarif)")
    test_parser.add_argument("--preview", action="store_true", help="List tests that would run without executing them")
    test_parser.add_argument("--dry-run", dest="preview", action="store_true", help=argparse.SUPPRESS)
    test_parser.add_argument("--list", dest="preview", action="store_true", help=argparse.SUPPRESS)
    test_parser.set_defaults(fn=cmd_test)

    bench_parser = sub.add_parser("bench", help="Benchmark configure/build/test workflows and gather performance metrics")
    bench_parser.add_argument("--build-dir", default="build", help="CMake build directory")
    bench_parser.add_argument("--std", default=STRICT_DEFAULT_STD, help="C++ standard for configure")
    bench_parser.add_argument("--build-type", default=STRICT_DEFAULT_BUILD_TYPE, help="CMake build type")
    bench_parser.add_argument("--generator", default=STRICT_DEFAULT_GENERATOR, help="CMake generator (default: Ninja)")
    bench_parser.add_argument("--runs", type=int, default=1, help="Number of hyperfine runs per stage")
    bench_parser.add_argument("--warmup", type=int, default=0, help="Number of warmup runs for hyperfine")
    bench_parser.add_argument("--target", help="Specific build target")
    bench_parser.add_argument("--jobs", type=int, help="Parallel build jobs")
    bench_parser.add_argument("--label", help="CTest label filter")
    bench_parser.add_argument("--regex", help="CTest regex filter")
    bench_parser.add_argument("--ctest-jobs", type=int, dest="ctest_jobs", help="Parallel jobs for CTest")
    bench_parser.add_argument("--cmake-arg", action="append", dest="cmake_arg", help="Additional -D style definitions for configure")
    bench_parser.add_argument("--cmake-extra", action="append", dest="cmake_extra", help="Extra raw arguments passed to cmake configure")
    bench_parser.add_argument("--build-arg", action="append", dest="build_arg", help="Extra arguments forwarded to the build tool")
    bench_parser.add_argument("--ctest-arg", action="append", dest="ctest_arg", help="Additional arguments forwarded to ctest")
    bench_parser.add_argument("--skip-configure", action="store_true", help="Skip configure stage")
    bench_parser.add_argument("--skip-build", action="store_true", help="Skip build stage")
    bench_parser.add_argument("--skip-test", action="store_true", help="Skip test stage")
    bench_parser.add_argument("--fresh", action="store_true", help="Remove the build directory before configuring")
    bench_parser.add_argument("--keep-output", action="store_true", help="Stream full command output instead of filtered highlights")
    bench_parser.add_argument("--no-ccache", action="store_true", help="Disable ccache launcher injection")
    bench_parser.add_argument("--no-tidy", action="store_true", help="Disable clang-tidy integration during configure")
    bench_parser.add_argument("--tidy-checks", default=STRICT_DEFAULT_TIDY_CHECKS, help="clang-tidy checks list")
    bench_parser.add_argument("--top-slowest", type=int, default=10, help="Number of slow translation units to report")
    bench_parser.set_defaults(fn=cmd_bench)

    stderr_parser = sub.add_parser("stderr-thin", help="Collapse compiler stderr into budget-aware highlights")
    stderr_parser.add_argument("--log", help="Path to an existing stderr log to thin")
    stderr_parser.add_argument("--compile", help="Substring to select compile command from compile_commands.json")
    stderr_parser.add_argument("--compile-index", type=int, help="Index in compile_commands.json to run")
    stderr_parser.add_argument("--compile-db", help="Path to compile_commands.json to use")
    stderr_parser.add_argument("--level", choices=["summary", "focused", "detailed"], default="focused", help="Detail level for the thinned output")
    stderr_parser.add_argument("--context-budget", type=int, help="Maximum characters to keep in the thinned view")
    stderr_parser.add_argument("--json", help="Custom path for JSON summary output")
    stderr_parser.add_argument("--text", help="Custom path for thinned text output")
    stderr_parser.add_argument("--raw", help="Custom path for captured raw stderr")
    stderr_parser.add_argument("--cwd", help="Working directory when executing a command")
    stderr_parser.add_argument("cmd", nargs=argparse.REMAINDER, help="Command to execute (use -- to separate)")
    stderr_parser.set_defaults(fn=cmd_stderr_thin)

    # Add lsp-bridge command
    lsp_parser = sub.add_parser("lsp-bridge", help="Bridge to clangd LSP for structured diagnostics")
    lsp_parser.add_argument("files", nargs="*", help="Source files to analyze (defaults to compile_commands.json entries)")
    lsp_parser.add_argument("--compile-db", help="Path to compile_commands.json")
    lsp_parser.add_argument("--server-path", help="Path to clangd executable")
    lsp_parser.add_argument("--level", choices=["summary", "focused", "detailed"], default="focused", help="Detail level")
    lsp_parser.add_argument("--context-budget", type=int, help="Maximum characters in filtered output")
    lsp_parser.add_argument("--filter-severity", choices=["error", "warning", "information", "hint"], help="Minimum severity to include")
    lsp_parser.add_argument("--filter-category", help="Filter by diagnostic category/check")
    lsp_parser.add_argument("--timeout", type=float, default=30.0, help="LSP server timeout in seconds")
    lsp_parser.add_argument("--json", help="Custom path for JSON output")
    lsp_parser.add_argument("--text", help="Custom path for text output")
    lsp_parser.set_defaults(fn=cmd_lsp_bridge)

    # Add tidy command
    tidy_parser = sub.add_parser("tidy", help="Run clang-tidy with fix options")
    tidy_parser.add_argument("paths", nargs="*", help="Paths to tidy")
    tidy_parser.add_argument("--apply", action="store_true", help="Apply suggested fixes")
    tidy_parser.add_argument("--checks", help="Specify which checks to run")
    tidy_parser.set_defaults(fn=cmd_tidy)

    # Add format command
    format_parser = sub.add_parser("format", help="Run clang-format on code")
    format_parser.add_argument("paths", nargs="*", help="Paths to format")
    format_parser.add_argument("--check", action="store_true", help="Check if files are formatted (dry run)")
    format_parser.add_argument("--apply", action="store_true", help="Apply formatting changes")
    format_parser.add_argument("--style", help="Formatting style (file, LLVM, Google, etc.)")
    format_parser.set_defaults(fn=cmd_format)

    # Add gate command for SARIF severity budgets
    gate_parser = sub.add_parser("gate", help="Enforce SARIF severity budgets for CI")
    gate_parser.add_argument("sarif_file", help="Path to SARIF file to check")
    gate_parser.add_argument("--max-errors", type=int, default=0, help="Maximum allowed errors")
    gate_parser.add_argument("--max-warnings", type=int, default=10, help="Maximum allowed warnings")
    gate_parser.add_argument("--max-notes", type=int, default=50, help="Maximum allowed notes")
    gate_parser.add_argument("--config", help="Path to gate configuration file")
    gate_parser.set_defaults(fn=cmd_gate)

    rd = sub.add_parser("reduce"); rd.add_argument("input"); rd.add_argument("test_cmd")
    rd.add_argument("--timeout", type=int, help="Timeout for the reduction process")
    rd.add_argument("--sanitizer", help="Sanitizer to use during testing")
    rd.set_defaults(fn=cmd_reduce)

    sub.add_parser("docs").set_defaults(fn=cmd_docs)
    sub.add_parser("capabilities").set_defaults(fn=cmd_capabilities)

    init_parser = sub.add_parser("init", help="Initialize LLM-optimized C++ project")
    init_parser.add_argument("name", nargs="?", help="Project name (uses current directory if not specified)")
    init_parser.add_argument("--force", action="store_true", help="Overwrite existing CMakeLists.txt")
    init_parser.add_argument("--existing", action="store_true", help="Adopt an existing project without overwriting CMake files")

    # Customization options
    init_parser.add_argument("--std", choices=["17", "20", "23", "26"], default="23", help="C++ standard version")
    init_parser.add_argument("--cmake-min", default="3.28", help="Minimum CMake version required")
    init_parser.add_argument("--pic", action="store_true", help="Enable position independent code by default")
    init_parser.add_argument("--no-sanitizers", action="store_true", help="Disable sanitizer options")
    # Get available presets dynamically
    available_presets = get_available_presets()
    init_parser.add_argument("--preset", choices=available_presets, default="full",
                           help=f"Project template preset. Available: {', '.join(available_presets)}")

    # Add toggle arguments for fine-grained control
    init_parser.add_argument("--no-rtti", action="store_true", help="Disable C++ RTTI")
    init_parser.add_argument("--no-exceptions", action="store_true", help="Disable C++ exceptions")
    init_parser.add_argument("--enable-simd", action="store_true", help="Enable SIMD optimizations")
    init_parser.add_argument("--openmp", action="store_true", help="Enable OpenMP support")
    init_parser.add_argument("--static-linking", action="store_true", help="Use static linking")

    init_parser.set_defaults(fn=cmd_init)

    install_parser = sub.add_parser("install", help="Install missing tools")
    install_parser.add_argument("--local", action="store_true", help="Force local installation (no sudo)")
    install_parser.add_argument("tools", nargs="*", help="Specific tools to install")
    install_parser.set_defaults(fn=cmd_install)

    cache_parser = sub.add_parser("cache", help="Manage analysis cache")
    cache_subparsers = cache_parser.add_subparsers(dest="cache_cmd", required=True)
    cache_subparsers.add_parser("clear", help="Clear the analysis cache").set_defaults(fn=cmd_cache)
    cache_subparsers.add_parser("show", help="Show cache contents").set_defaults(fn=cmd_cache)

    kb_parser = sub.add_parser("knowledge-base", help="Query the error knowledge base")
    kb_subparsers = kb_parser.add_subparsers(dest="kb_cmd", required=True)
    kb_subparsers.add_parser("show", help="Show knowledge base contents").set_defaults(fn=cmd_kb)

    agent_parser = sub.add_parser("agent", help="Handle agent feedback loop requests")
    agent_subparsers = agent_parser.add_subparsers(dest="agent_cmd", required=True)
    
    agent_request_parser = agent_subparsers.add_parser("request", help="Send a single request to the agent")
    agent_request_parser.add_argument("json_request", help="JSON string containing agent requests")
    agent_request_parser.set_defaults(fn=cmd_agent)

    agent_serve_parser = agent_subparsers.add_parser("serve", help="Start a persistent server for agent requests")
    agent_serve_parser.add_argument("--host", default="localhost", help="Host to bind the server to")
    agent_serve_parser.add_argument("--port", type=int, default=8080, help="Port to bind the server to")
    agent_serve_parser.set_defaults(fn=cmd_agent_serve)

    agent_mcp_parser = agent_subparsers.add_parser("mcp", help="Expose agent operations over Model Context Protocol")
    agent_mcp_parser.set_defaults(fn=cmd_agent_mcp)

    args = ap.parse_args()
    # map nested subcommand
    if getattr(args, "sub", None):
        return args.fn(args)
    return args.fn(args)

if __name__ == "__main__":
    sys.exit(main())
