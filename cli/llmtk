#!/usr/bin/env python3
import argparse, json, os, shutil, subprocess, sys, pathlib, datetime, re, tempfile, textwrap, tarfile, time, shlex, hashlib

# Import template engine
sys.path.insert(0, str(pathlib.Path(__file__).parent.parent / "modules"))
try:
    from template_engine import TemplateEngine
    TEMPLATE_ENGINE_AVAILABLE = True
except ImportError:
    TEMPLATE_ENGINE_AVAILABLE = False

ROOT = pathlib.Path(__file__).resolve().parent.parent

def get_available_presets():
    """Get list of available presets from template engine or fallback to legacy"""
    if TEMPLATE_ENGINE_AVAILABLE:
        try:
            template_dir = ROOT / "templates"
            manifest_dir = ROOT / "manifest"
            if template_dir.exists() and manifest_dir.exists():
                engine = TemplateEngine(template_dir, manifest_dir)
                return engine.get_available_presets()
        except Exception as e:
            print(f"Warning: Failed to load template engine: {e}")

    # Fallback to legacy presets
    return ["minimal", "full", "library"]

def get_version() -> str:
    try:
        vfile = ROOT / "VERSION"
        if vfile.exists():
            return vfile.read_text().strip()
    except Exception:
        pass
    try:
        res = subprocess.run(["git", "-C", str(ROOT), "describe", "--tags", "--always"], text=True, capture_output=True)
        if res.returncode == 0 and res.stdout.strip():
            return res.stdout.strip()
    except Exception:
        pass
    return "0.0.0+unknown"
# Always write artifacts under the current working directory
EXPORTS = pathlib.Path.cwd() / "exports"
MODULES = ROOT / "modules"
EXPORTS.mkdir(exist_ok=True)

def run(cmd, cwd=None, check=True, env=None):
    return subprocess.run(cmd, cwd=cwd, text=True, capture_output=True, check=check, env=env)

def write_json(path: pathlib.Path, data):
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, indent=2))

def validate_cmake_guidance(project_dir=None):
    """Validate CMakeLists.txt compliance with CMAKE_GUIDANCE.md patterns"""
    if project_dir is None:
        project_dir = pathlib.Path.cwd()
    else:
        project_dir = pathlib.Path(project_dir)

    cmake_file = project_dir / "CMakeLists.txt"

    result = {
        "cmake_file_exists": cmake_file.exists(),
        "compliance": {
            "compile_commands_export": False,
            "project_warnings_interface": False,
            "project_sanitizers_interface": False,
            "json_diagnostics": False,
            "cxx_standard_set": False,
            "lint_target": False
        },
        "suggestions": [],
        "overall_score": 0
    }

    if not cmake_file.exists():
        result["suggestions"].append("Create CMakeLists.txt with 'llmtk init'")
        return result

    try:
        cmake_content = cmake_file.read_text()

        # Check for required patterns from CMAKE_GUIDANCE.md
        patterns = {
            "compile_commands_export": "CMAKE_EXPORT_COMPILE_COMMANDS ON",
            "project_warnings_interface": "add_library(project_warnings INTERFACE)",
            "project_sanitizers_interface": "add_library(project_sanitizers INTERFACE)",
            "json_diagnostics": "fdiagnostics-format=json",
            "cxx_standard_set": "CMAKE_CXX_STANDARD",
            "lint_target": "add_custom_target(lint"
        }

        for key, pattern in patterns.items():
            if pattern in cmake_content:
                result["compliance"][key] = True
            else:
                # Add specific suggestions based on missing patterns
                if key == "compile_commands_export":
                    result["suggestions"].append("Add 'set(CMAKE_EXPORT_COMPILE_COMMANDS ON)' for clangd support")
                elif key == "project_warnings_interface":
                    result["suggestions"].append("Create project_warnings INTERFACE library for consistent warnings")
                elif key == "json_diagnostics":
                    result["suggestions"].append("Add JSON diagnostics support for LLM-friendly error output")
                elif key == "lint_target":
                    result["suggestions"].append("Add custom lint target for syntax-only checking")

        # Calculate overall compliance score
        compliant_count = sum(result["compliance"].values())
        total_checks = len(result["compliance"])
        result["overall_score"] = (compliant_count / total_checks) * 100

    except Exception as e:
        result["suggestions"].append(f"Error reading CMakeLists.txt: {e}")

    return result

def load_yaml(path: pathlib.Path):
    # Try PyYAML, then yq, else return None
    try:
        import yaml  # type: ignore
        with open(path, 'r') as f:
            return yaml.safe_load(f)
    except Exception:
        pass
    if shutil.which("yq"):
        try:
            conv = subprocess.run(["yq", "-o=json", str(path)], text=True, capture_output=True)
            if conv.returncode == 0 and conv.stdout:
                return json.loads(conv.stdout)
        except Exception:
            pass
    return None

def generate_reference_md(out_path: pathlib.Path):
    tools_manifest = ROOT/"manifest"/"tools.yaml"
    commands_manifest = ROOT/"manifest"/"commands.yaml"
    tools = load_yaml(tools_manifest)
    commands = load_yaml(commands_manifest)

    now = datetime.datetime.now(datetime.UTC).isoformat()
    lines = []
    lines.append("# Toolkit Reference")
    lines.append("")
    lines.append(f"Generated from manifests on {now}.")
    lines.append("")

    # Tools
    lines.append("## Tools")
    if tools and isinstance(tools, dict) and "tools" in tools:
        for name in sorted(tools["tools"].keys()):
            t = tools["tools"][name] or {}
            version = t.get("version")
            provides = t.get("provides") or []
            check = None
            if isinstance(t.get("check"), dict):
                check = " ".join(map(str, t["check"].get("cmd", [])))
            lines.append(f"- {name}")
            if version: lines.append(f"  - version: {version}")
            if provides: lines.append(f"  - provides: {', '.join(provides)}")
            if check: lines.append(f"  - check: `{check}`")
    else:
        # Fallback: embed raw YAML for visibility
        try:
            raw = tools_manifest.read_text()
            lines.append("````yaml")
            lines.append(raw.rstrip())
            lines.append("````")
        except Exception:
            lines.append("(tools manifest not found)")

    lines.append("")
    lines.append("## Commands")
    if commands and isinstance(commands, dict) and "commands" in commands:
        for name in sorted(commands["commands"].keys()):
            c = commands["commands"][name] or {}
            lines.append(f"- {name}")
            if c.get("description"): lines.append(f"  - description: {c['description']}")
            args = c.get("args") or []
            if args:
                arg_summaries = []
                for a in args:
                    if isinstance(a, dict):
                        nm = a.get("name")
                        req = a.get("required", False)
                        var = a.get("variadic", False)
                        frag = nm or "arg"
                        if req: frag += " (required)"
                        if var: frag += " (variadic)"
                        arg_summaries.append(frag)
                if arg_summaries:
                    lines.append(f"  - args: {', '.join(arg_summaries)}")
            runs = c.get("runs") or []
            if runs: lines.append(f"  - runs: {', '.join(runs)}")
            outs = c.get("outputs") or []
            if outs:
                for o in outs:
                    if isinstance(o, str):
                        lines.append(f"  - output: {o}")
                    elif isinstance(o, dict):
                        p = o.get("path") or o.get("file") or "(unknown)"
                        lines.append(f"  - output: {p}")
                        schema = o.get("schema")
                        if schema is not None:
                            try:
                                rendered = json.dumps(schema, indent=2)
                            except Exception:
                                rendered = str(schema)
                            lines.append("    schema:")
                            lines.append("    ```json")
                            for l in rendered.splitlines():
                                lines.append("    " + l)
                            lines.append("    ```")
            if c.get("json_summary"): lines.append(f"  - json_summary: {c['json_summary']}")
    else:
        try:
            raw = commands_manifest.read_text()
            lines.append("````yaml")
            lines.append(raw.rstrip())
            lines.append("````")
        except Exception:
            lines.append("(commands manifest not found)")

    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("\n".join(lines) + "\n")



def generate_capabilities_json(out_path: pathlib.Path):
    """Emit a machine-readable capabilities summary for agents."""
    tools_manifest = ROOT/"manifest"/"tools.yaml"
    commands_manifest = ROOT/"manifest"/"commands.yaml"
    tools = load_yaml(tools_manifest) or {}
    commands = load_yaml(commands_manifest) or {}

    data = {
        "$schema": f"https://llmtk.ai/schemas/capabilities-v1.json",
        "_meta": {
            "generated_at": datetime.datetime.now(datetime.UTC).isoformat(),
            "toolkit_version": get_version(),
            "tools_manifest": str(tools_manifest),
            "commands_manifest": str(commands_manifest),
        },
        "tools": {},
        "commands": {},
    }

    tools_section = tools.get("tools") if isinstance(tools, dict) else None
    if isinstance(tools_section, dict):
        for name, entry in tools_section.items():
            entry = entry or {}
            data["tools"][name] = {
                "version": entry.get("version"),
                "min_version": entry.get("min_version"),
                "provides": entry.get("provides") or [],
                "role": entry.get("role", "optional"),
                "invocation": entry.get("invocation") or {},
                "install": entry.get("install") or {},
                "check": entry.get("check") or {},
                "fallbacks": entry.get("fallbacks") or [],
                "local_install": entry.get("local_install") or None,
            }

    commands_section = commands.get("commands") if isinstance(commands, dict) else None
    if isinstance(commands_section, dict):
        for name, entry in commands_section.items():
            entry = entry or {}
            data["commands"][name] = {
                "description": entry.get("description"),
                "args": entry.get("args") or [],
                "runs": entry.get("runs") or [],
                "outputs": entry.get("outputs") or [],
                "json_summary": entry.get("json_summary"),
                "examples": entry.get("examples") or [],
            }

    write_json(out_path, data)
    return out_path


_TEMPLATE_CHAIN_PATTERNS = [
    re.compile(r"\b(in|note: in) instantiation of", re.IGNORECASE),
    re.compile(r"\brequired from", re.IGNORECASE),
    re.compile(r"\binstantiated from", re.IGNORECASE),
    re.compile(r"\bsubstitution failure", re.IGNORECASE),
]

_SEVERITY_REGEX = re.compile(r"\b(error|warning|note|remark|fatal error)\b", re.IGNORECASE)

_DEFAULT_CONTEXT_BUDGETS = {
    "summary": 1500,
    "focused": 6000,
    "detailed": 16000,
}


def _collapse_template_traces(lines):
    collapsed = []
    omitted = 0
    i = 0
    total = len(lines)
    while i < total:
        line = lines[i]
        if any(pat.search(line) for pat in _TEMPLATE_CHAIN_PATTERNS):
            group = [line]
            i += 1
            while i < total and any(pat.search(lines[i]) for pat in _TEMPLATE_CHAIN_PATTERNS):
                group.append(lines[i])
                i += 1
            if len(group) > 3:
                collapsed.append(group[0])
                collapsed.append(f"... ({len(group) - 2} template frames trimmed)")
                collapsed.append(group[-1])
                omitted += len(group) - 3
            else:
                collapsed.extend(group)
            continue
        collapsed.append(line)
        i += 1
    return collapsed, omitted


def _dedupe_preserve(seq):
    seen = set()
    out = []
    for item in seq:
        if item not in seen and item is not None:
            out.append(item)
            seen.add(item)
    return out


def _normalize_note(note):
    if isinstance(note, str):
        return {"message": note, "file": None, "line": None, "column": None, "level": "note"}
    if isinstance(note, dict):
        loc = note.get("location") or {}
        return {
            "message": note.get("message") or note.get("text") or "",
            "file": note.get("file") or note.get("path") or loc.get("file"),
            "line": note.get("line") or loc.get("line"),
            "column": note.get("column") or loc.get("column"),
            "level": note.get("level") or note.get("severity") or "note",
        }
    return {"message": str(note), "file": None, "line": None, "column": None, "level": "note"}


def _normalize_diag(diag):
    message = diag.get("message") or diag.get("text") or diag.get("description")
    loc = diag.get("location") or {}
    return {
        "level": diag.get("level") or diag.get("severity") or loc.get("level"),
        "message": message or "",
        "file": diag.get("file") or diag.get("path") or loc.get("file"),
        "line": diag.get("line") or loc.get("line"),
        "column": diag.get("column") or loc.get("column"),
        "category": diag.get("category") or diag.get("option"),
        "option": diag.get("option"),
        "notes": [_normalize_note(n) for n in diag.get("notes") or diag.get("children") or []],
        "raw": diag,
    }


def _extract_json_objects(blob):
    decoder = json.JSONDecoder()
    idx = 0
    length = len(blob)
    results = []
    while idx < length:
        ch = blob[idx]
        if ch not in "{[":
            idx += 1
            continue
        try:
            obj, end = decoder.raw_decode(blob, idx)
        except json.JSONDecodeError:
            idx += 1
            continue
        results.append(obj)
        idx = end
    return results


def _parse_structured_diagnostics(text):
    diagnostics = []
    for obj in _extract_json_objects(text):
        if isinstance(obj, dict):
            if "diag" in obj and isinstance(obj["diag"], dict):
                diagnostics.append(_normalize_diag(obj["diag"]))
            elif "diagnostics" in obj and isinstance(obj["diagnostics"], list):
                for item in obj["diagnostics"]:
                    if isinstance(item, dict):
                        diagnostics.append(_normalize_diag(item))
            elif {"level", "message"}.issubset(obj.keys()):
                diagnostics.append(_normalize_diag(obj))
        elif isinstance(obj, list):
            for entry in obj:
                if isinstance(entry, dict) and {"level", "message"}.issubset(entry.keys()):
                    diagnostics.append(_normalize_diag(entry))
    return diagnostics


_TEXT_DIAG_PATTERN = re.compile(
    r"^(?P<file>[^:\n]+)?(?::(?P<line>\d+))?(?::(?P<column>\d+))?:\s*(?P<level>error|warning|note|remark|fatal error)\s*:\s*(?P<message>.*)",
    re.IGNORECASE,
)


def _parse_text_diagnostics(lines):
    diagnostics = []
    for line in lines:
        match = _TEXT_DIAG_PATTERN.match(line.strip())
        if not match:
            continue
        gd = match.groupdict()
        try:
            line_no = int(gd.get("line")) if gd.get("line") else None
        except ValueError:
            line_no = None
        try:
            col_no = int(gd.get("column")) if gd.get("column") else None
        except ValueError:
            col_no = None
        diagnostics.append({
            "level": (gd.get("level") or "").lower(),
            "message": gd.get("message") or "",
            "file": gd.get("file") or None,
            "line": line_no,
            "column": col_no,
            "category": None,
            "option": None,
            "notes": [],
            "raw": line,
        })
    return diagnostics


def _format_location(file, line, column):
    if not file:
        return None
    location = str(file)
    if line is not None:
        location += f":{line}"
        if column is not None:
            location += f":{column}"
    return location


def _format_highlights(diagnostics):
    highlights = []
    for diag in diagnostics:
        level = (diag.get("level") or "info").upper()
        location = _format_location(diag.get("file"), diag.get("line"), diag.get("column"))
        category = diag.get("category") or diag.get("option")
        base = f"{level}: {diag.get('message', '').strip()}"
        if location:
            base = f"{base} ({location})"
        if category:
            base = f"{base} [{category}]"
        highlights.append(base.strip())
        for note in diag.get("notes", []):
            note_level = (note.get("level") or "note").lower()
            note_loc = _format_location(note.get("file"), note.get("line"), note.get("column"))
            note_msg = note.get("message", "").strip()
            frag = f"  {note_level}: {note_msg}" if note_msg else f"  {note_level}"
            if note_loc:
                frag = f"{frag} ({note_loc})"
            highlights.append(frag.strip())
    return _dedupe_preserve(highlights)


def _focused_view_lines(collapsed_lines):
    focus = []
    for idx, line in enumerate(collapsed_lines):
        if _SEVERITY_REGEX.search(line):
            if idx > 0:
                focus.append(collapsed_lines[idx - 1])
            focus.append(line)
            if idx + 1 < len(collapsed_lines):
                focus.append(collapsed_lines[idx + 1])
    focus = _dedupe_preserve(focus)
    return focus if focus else collapsed_lines[:200]


def _apply_context_budget(lines, budget):
    text = "\n".join(line.rstrip() for line in lines if line is not None)
    text = text.strip()
    full_len = len(text)
    if budget and full_len > budget:
        trimmed = text[:budget]
        cut = trimmed.rfind("\n")
        if cut > budget * 0.6:
            trimmed = trimmed[:cut]
        truncated = full_len - len(trimmed)
        text = trimmed.rstrip() + f"\n... [truncated {truncated} chars]"
        used = len(text)
        return text, used, full_len, truncated
    return text, full_len, full_len, 0


def _collect_counts(diagnostics):
    counts = {"error": 0, "warning": 0, "note": 0, "remark": 0, "other": 0}
    for diag in diagnostics:
        level = (diag.get("level") or "other").lower()
        if level in counts:
            counts[level] += 1
        else:
            counts["other"] += 1
    return counts


def _find_compile_database(path_hint=None):
    if path_hint:
        candidate = pathlib.Path(path_hint)
        if candidate.exists():
            return candidate
        return candidate  # allow caller to report missing path
    candidates = [
        EXPORTS / "compile_commands.json",
        pathlib.Path.cwd() / "compile_commands.json",
    ]
    for cand in candidates:
        if cand.exists():
            return cand
    common_build_dirs = ["build", "cmake-build-debug", "cmake-build-release"]
    for build_dir in common_build_dirs:
        candidate = pathlib.Path(build_dir) / "compile_commands.json"
        if candidate.exists():
            return candidate
    return None


def _prepare_compile_command(entry):
    args = []
    if isinstance(entry.get("arguments"), list):
        args = list(entry["arguments"])
    elif entry.get("command"):
        args = shlex.split(entry["command"])
    sanitized = []
    skip_next = False
    drop_prefixes = ("-o", "-MF", "-MT", "-MQ")
    for part in args:
        if skip_next:
            skip_next = False
            continue
        if part in drop_prefixes:
            skip_next = True
            continue
        if any(part.startswith(prefix) for prefix in drop_prefixes):
            continue
        sanitized.append(part)
    if "-fsyntax-only" not in sanitized:
        sanitized.append("-fsyntax-only")
    if not any(arg.startswith("-fdiagnostics-format") for arg in sanitized):
        sanitized.append("-fdiagnostics-format=json")
    if not any(arg.startswith("-fdiagnostics-color") for arg in sanitized) and not any(arg.startswith("-fcolor-diagnostics") for arg in sanitized):
        sanitized.append("-fdiagnostics-color=never")
    if "-fno-caret-diagnostics" not in sanitized:
        sanitized.append("-fno-caret-diagnostics")
    return sanitized


def cmd_docs(_):
    out = ROOT/"docs"/"REFERENCE.md"
    generate_reference_md(out)
    print(str(out))

def cmd_capabilities(_):
    path = generate_capabilities_json(EXPORTS/"capabilities.json")
    print(str(path))

def adopt_existing_project(project_dir: pathlib.Path, project_name: str) -> int:
    """Generate an adoption report for an existing project without mutating sources."""
    print(f"🤝 Adopting existing project in {project_dir}")

    cmake_validation = validate_cmake_guidance(project_dir)
    compile_db = project_dir / "compile_commands.json"
    exported_compile_db = None
    compile_db_copy_error = None
    build_dir_candidates = []
    for candidate in ["build", "cmake-build-debug", "cmake-build-release"]:
        bdir = project_dir / candidate
        if bdir.exists() and bdir.is_dir():
            build_dir_candidates.append(str(bdir.resolve()))

    preset_names = [".clang-tidy", ".clang-format", "cmake-format.yaml", "pre-commit-config.yaml"]
    presets_status = []
    recommendations = list(cmake_validation.get("suggestions", []))
    for preset in preset_names:
        present = (project_dir / preset).exists()
        presets_status.append({"name": preset, "present": present})
        if not present:
            recommendations.append(f"Consider copying preset '{preset}' from llm-cpp-toolkit/presets")

    if not compile_db.exists():
        recommendations.append("Generate compile_commands.json via 'llmtk context export --build <build-dir>'")
    else:
        dest = EXPORTS / "compile_commands.json"
        try:
            dest.parent.mkdir(parents=True, exist_ok=True)
            if dest.exists() and dest.resolve() == compile_db.resolve():
                exported_compile_db = str(dest.resolve())
            else:
                shutil.copy2(compile_db, dest)
                exported_compile_db = str(dest.resolve())
        except Exception as exc:  # best effort copy
            compile_db_copy_error = str(exc)
            recommendations.append(
                "Copy existing compile_commands.json into exports/ (e.g. 'cp compile_commands.json exports/') for agents"
            )

    deduped_recs = []
    seen = set()
    for item in recommendations:
        if item and item not in seen:
            deduped_recs.append(item)
            seen.add(item)

    adoption_summary = {
        "mode": "existing",
        "project_name": project_name,
        "project_dir": str(project_dir.resolve()),
        "timestamp": datetime.datetime.now(datetime.UTC).isoformat(),
        "cmake_validation": cmake_validation,
        "artifacts": {
            "compile_commands": str(compile_db.resolve()) if compile_db.exists() else None,
            "build_directories": build_dir_candidates,
            "exports_compile_commands": exported_compile_db,
        },
        "presets": presets_status,
        "recommendations": deduped_recs,
    }

    out = EXPORTS / "init-existing.json"
    write_json(out, adoption_summary)
    capabilities_path = generate_capabilities_json(EXPORTS/"capabilities.json")

    print("📄 Adoption summary written to", out)
    print("📚 Capabilities manifest available at", capabilities_path)
    if exported_compile_db:
        print(f"📦 Copied existing compile_commands.json to {exported_compile_db}")
    elif compile_db_copy_error:
        print(f"⚠️ Failed to copy compile_commands.json: {compile_db_copy_error}")
    if cmake_validation["overall_score"] >= 80:
        print(f"✅ Existing CMake setup scores {cmake_validation['overall_score']:.0f}% against CMAKE_GUIDANCE.md")
    else:
        print(f"💡 CMake compliance is {cmake_validation['overall_score']:.0f}% – see recommendations above.")

    print("\n🚀 Suggested follow-ups:")
    for rec in deduped_recs[:5]:
        print(f"   • {rec}")
    if len(deduped_recs) > 5:
        print(f"   • ...and {len(deduped_recs) - 5} more (see adoption summary)")

    return 0


def cmd_init(args):
    """Initialize a new C++ project with CMAKE_GUIDANCE.md compliance"""
    project_name = args.name or pathlib.Path.cwd().name
    project_dir = pathlib.Path.cwd() if not args.name else pathlib.Path(args.name)

    # Create directory if it doesn't exist
    if args.name:
        if project_dir.exists():
            print(f"📂 Using existing project directory: {project_dir}")
        else:
            project_dir.mkdir(parents=True, exist_ok=True)
            print(f"📁 Created project directory: {project_dir}")

    # Check if CMakeLists.txt already exists
    cmake_file = project_dir / "CMakeLists.txt"
    if args.existing and not cmake_file.exists():
        print("❌ --existing specified but no CMakeLists.txt found in the target directory.")
        return 1

    if cmake_file.exists() and not args.force:
        return adopt_existing_project(project_dir, project_name)

    # Use template engine if available, otherwise fall back to legacy generation
    if TEMPLATE_ENGINE_AVAILABLE:
        return init_with_template_engine(args, project_name, project_dir)
    else:
        return init_with_legacy_generation(args, project_name, project_dir)

def init_with_template_engine(args, project_name, project_dir):
    """Initialize project using the template engine"""
    try:
        template_dir = ROOT / "templates"
        manifest_dir = ROOT / "manifest"
        engine = TemplateEngine(template_dir, manifest_dir)

        # Build user overrides from command line arguments
        user_overrides = {}

        # Handle legacy arguments
        if getattr(args, 'pic', False):
            user_overrides['pic'] = True
        if getattr(args, 'no_sanitizers', False):
            user_overrides['sanitizers'] = False
        if getattr(args, 'no_rtti', False):
            user_overrides['rtti'] = False
        if getattr(args, 'no_exceptions', False):
            user_overrides['exceptions'] = False
        if getattr(args, 'enable_simd', False):
            user_overrides['simd'] = 'native'
        if getattr(args, 'openmp', False):
            user_overrides['concurrency'] = 'openmp'
        if getattr(args, 'static_linking', False):
            user_overrides['static_linking'] = True

        # Resolve template
        preset = getattr(args, 'preset', 'full')
        template = engine.resolve_template(preset, user_overrides)

        print(f"📋 Using template: {template.name}")
        if template.description:
            print(f"   {template.description}")

        # Generate CMakeLists.txt
        cmake_min_version = getattr(args, 'cmake_min', '3.28')
        cxx_standard = getattr(args, 'std', '23')

        cmake_content = engine.generate_cmake_content(
            template, project_name, cmake_min_version, cxx_standard
        )

        # Write CMakeLists.txt
        cmake_file = project_dir / "CMakeLists.txt"
        cmake_file.write_text(cmake_content)

        # Create additional files from template
        for file_spec in template.files:
            file_path = project_dir / file_spec['path']
            file_path.parent.mkdir(parents=True, exist_ok=True)
            try:
                content = file_spec['content'].format(project_name=project_name)
            except (KeyError, ValueError) as e:
                # Fallback: use simple replacement for problematic templates
                content = file_spec['content'].replace('{project_name}', project_name)
            file_path.write_text(content)
            print(f"📄 Created {file_spec['path']}")

        # Generate capabilities.json
        capabilities_path = project_dir / "exports" / "capabilities.json"
        capabilities_path.parent.mkdir(parents=True, exist_ok=True)
        generate_capabilities_json(capabilities_path)

        print(f"✅ Project '{project_name}' initialized successfully with preset '{preset}'")
        if template.documentation:
            print(f"\n📖 Template documentation:")
            print(textwrap.indent(template.documentation, "   "))

        return 0

    except Exception as e:
        print(f"❌ Failed to initialize with template engine: {e}")
        print("   Falling back to legacy generation...")
        return init_with_legacy_generation(args, project_name, project_dir)

def init_with_legacy_generation(args, project_name, project_dir):
    """Initialize project using legacy hardcoded generation"""
    # Generate CMAKE_GUIDANCE.md compliant CMakeLists.txt with user options
    cmake_min_version = getattr(args, 'cmake_min', '3.28')
    cxx_standard = getattr(args, 'std', '23')
    enable_pic = getattr(args, 'pic', False)
    disable_sanitizers = getattr(args, 'no_sanitizers', False)
    preset = getattr(args, 'preset', 'full')

    # Handle sanitizers section based on user preference
    sanitizers_section = ""
    if not disable_sanitizers:
        sanitizers_section = f'''
# 2) Sanitizer function for creating multiple sanitized variants
function(llmtk_add_sanitized_target base_target sanitizer_name sanitizer_flags)
  if (NOT TARGET "${{base_target}}")
    message(FATAL_ERROR "llmtk_add_sanitized_target: unknown target '${{base_target}}'")
  endif()

  set(sanitized "${{base_target}}_${{sanitizer_name}}")

  # Collect the base target's sources so we can clone the build
  get_target_property(srcs "${{base_target}}" SOURCES)
  if (NOT srcs)
    message(FATAL_ERROR "Target '${{base_target}}' has no SOURCES property; cannot clone for sanitizer build")
  endif()

  # Determine target type (executable vs library)
  get_target_property(target_type "${{base_target}}" TYPE)
  if(target_type STREQUAL "EXECUTABLE")
    add_executable("${{sanitized}}" EXCLUDE_FROM_ALL ${{srcs}})
  elseif(target_type STREQUAL "STATIC_LIBRARY" OR target_type STREQUAL "SHARED_LIBRARY")
    add_library("${{sanitized}}" EXCLUDE_FROM_ALL ${{srcs}})
  else()
    message(FATAL_ERROR "Unsupported target type '${{target_type}}' for sanitizer cloning")
  endif()

  # Keep the strict warnings and any other link deps from the original target
  target_link_libraries("${{sanitized}}" PRIVATE
    $<TARGET_PROPERTY:${{base_target}},LINK_LIBRARIES>
    project_warnings
  )

  # Mirror include dirs / compile definitions / options so the TU is identical
  target_include_directories("${{sanitized}}" PRIVATE
    $<TARGET_PROPERTY:${{base_target}},INCLUDE_DIRECTORIES>
  )
  target_compile_definitions("${{sanitized}}" PRIVATE
    $<TARGET_PROPERTY:${{base_target}},COMPILE_DEFINITIONS>
  )
  target_compile_options("${{sanitized}}" PRIVATE
    $<TARGET_PROPERTY:${{base_target}},COMPILE_OPTIONS>
    ${{sanitizer_flags}}
  )
  target_link_options("${{sanitized}}" PRIVATE ${{sanitizer_flags}})

  # Optional: share the same output directory, and make the sanitized target depend on the base one
  set_target_properties("${{sanitized}}" PROPERTIES
    RUNTIME_OUTPUT_DIRECTORY $<TARGET_PROPERTY:${{base_target}},RUNTIME_OUTPUT_DIRECTORY>
  )
  add_dependencies("${{sanitized}}" "${{base_target}}")
endfunction()

# Create sanitized variants for non-MSVC compilers
if (NOT MSVC)
  # AddressSanitizer + UBSan combo (most common for development)
  llmtk_add_sanitized_target({project_name} asan_ubsan "-fsanitize=address,undefined")

  # ThreadSanitizer (mutually exclusive with AddressSanitizer)
  llmtk_add_sanitized_target({project_name} tsan "-fsanitize=thread")

  # MemorySanitizer (requires special build setup, uncomment if needed)
  # llmtk_add_sanitized_target({project_name} msan "-fsanitize=memory")
endif()

# Legacy compatibility: empty interface library for existing code
add_library(project_sanitizers INTERFACE)'''
    else:
        sanitizers_section = '''
# 2) Sanitizers disabled per user preference
add_library(project_sanitizers INTERFACE)'''

    # Handle different presets for target and lint sections
    if preset == "minimal":
        target_section = f'''
# 3) Your target(s) - replace with your actual source files
add_executable({project_name} main.cpp)
target_link_libraries({project_name} PRIVATE project_warnings project_sanitizers)'''
        lint_section = ""
    elif preset == "library":
        target_section = f'''
# 3) Library target - adjust as needed
add_library({project_name} src/{project_name}.cpp)
target_include_directories({project_name} PUBLIC include)
target_link_libraries({project_name} PRIVATE project_warnings project_sanitizers)

# Optional: also build example executable
add_executable({project_name}_example examples/main.cpp)
target_link_libraries({project_name}_example PRIVATE {project_name} project_warnings project_sanitizers)'''

        # For library preset, add sanitized variants of both library and example
        if not disable_sanitizers:
            target_section += f'''

# Create sanitized variants of library and example (non-MSVC only)
if (NOT MSVC)
  # Library sanitized variants
  llmtk_add_sanitized_target({project_name} asan_ubsan "-fsanitize=address,undefined")
  llmtk_add_sanitized_target({project_name} tsan "-fsanitize=thread")

  # Example sanitized variants
  llmtk_add_sanitized_target({project_name}_example asan_ubsan "-fsanitize=address,undefined")
  llmtk_add_sanitized_target({project_name}_example tsan "-fsanitize=thread")
endif()'''
        lint_section = f'''
# 4) LLM-focused "lint" that compiles TUs with tight diagnostics (no linking)
get_target_property(LIB_SOURCES {project_name} SOURCES)
get_target_property(LIB_INCLUDES {project_name} INCLUDE_DIRECTORIES)
get_target_property(LIB_DEFS     {project_name} COMPILE_DEFINITIONS)
get_target_property(LIB_OPTS     {project_name} COMPILE_OPTIONS)

# Compose include and define flags portably
set(_lint_includes "")
if(LIB_INCLUDES)
  foreach(inc ${{LIB_INCLUDES}})
    list(APPEND _lint_includes -I${{inc}})
  endforeach()
endif()
set(_lint_defines "")
if(LIB_DEFS)
  foreach(def ${{LIB_DEFS}})
    list(APPEND _lint_defines -D${{def}})
  endforeach()
endif()

# Build a response file so the command stays short
set(_lint_rsp "${{CMAKE_BINARY_DIR}}/lint_args.rsp")
file(WRITE  "${{_lint_rsp}}" "")
if(LIB_OPTS)
  foreach(opt ${{LIB_OPTS}})
    file(APPEND "${{_lint_rsp}}" "${{opt}}\\n")
  endforeach()
endif()
foreach(def ${{_lint_defines}})
  file(APPEND "${{_lint_rsp}}" "${{def}}\\n")
endforeach()
foreach(inc ${{_lint_includes}})
  file(APPEND "${{_lint_rsp}}" "${{inc}}\\n")
endforeach()

# Pick flags per compiler
if (CMAKE_CXX_COMPILER_ID STREQUAL "Clang")
  set(LINT_CORE_FLAGS
    -std=c++{cxx_standard} -fsyntax-only
    -Wfatal-errors -ferror-limit=1
    -ftemplate-backtrace-limit=6 -fconstexpr-backtrace-limit=3 -fmacro-backtrace-limit=2
    -fno-caret-diagnostics -fdiagnostics-color=never -fdiagnostics-show-option
    -fdiagnostics-format=json
  )
elseif (CMAKE_CXX_COMPILER_ID STREQUAL "GNU")
  set(LINT_CORE_FLAGS
    -std=c++{cxx_standard} -fsyntax-only
    -Wfatal-errors -fmax-errors=1
    -fconcepts-diagnostics-depth=2
    -fno-diagnostics-show-caret -fdiagnostics-color=never
    -fdiagnostics-format=json
  )
else()
  # MSVC: fall back to /analyze JSON? For now: no-op lint with success.
  set(LINT_CORE_FLAGS "")
endif()

# One lint target that iterates all TU's; writes a single JSON per TU into build/lint/
add_custom_target(lint
  COMMENT "Running tight JSON diagnostics per translation unit..."
)
foreach(src ${{LIB_SOURCES}})
  get_filename_component(src_name "${{src}}" NAME_WE)
  set(out_json "${{CMAKE_BINARY_DIR}}/lint/${{src_name}}.json")
  file(MAKE_DIRECTORY "${{CMAKE_BINARY_DIR}}/lint")
  add_custom_command(TARGET lint POST_BUILD
    COMMAND ${{CMAKE_CXX_COMPILER}} "@${{_lint_rsp}}" ${{LINT_CORE_FLAGS}} "${{src}}" 2> "${{out_json}}"
    BYPRODUCTS "${{out_json}}"
    COMMENT "Lint ${{src_name}}"
    VERBATIM
  )
endforeach()'''
    else:  # preset == "full"
        target_section = f'''
# 3) Your target(s) - replace with your actual source files
add_executable({project_name} main.cpp)
target_link_libraries({project_name} PRIVATE project_warnings project_sanitizers)'''
        lint_section = f'''
# 4) LLM-focused "lint" that compiles TUs with tight diagnostics (no linking)
#    Important: use the *same* include dirs/defs/opts as {project_name} so headers resolve.
get_target_property(APP_SOURCES {project_name} SOURCES)
get_target_property(APP_INCLUDES {project_name} INCLUDE_DIRECTORIES)
get_target_property(APP_DEFS     {project_name} COMPILE_DEFINITIONS)
get_target_property(APP_OPTS     {project_name} COMPILE_OPTIONS)

# Compose include and define flags portably for non-MSVC compilers.
set(_lint_includes "")
if(APP_INCLUDES)
  foreach(inc ${{APP_INCLUDES}})
    list(APPEND _lint_includes -I${{inc}})
  endforeach()
endif()
set(_lint_defines "")
if(APP_DEFS)
  foreach(def ${{APP_DEFS}})
    list(APPEND _lint_defines -D${{def}})
  endforeach()
endif()

# Build a response file so the command stays short (token-cheap logs)
set(_lint_rsp "${{CMAKE_BINARY_DIR}}/lint_args.rsp")
file(WRITE  "${{_lint_rsp}}" "")
if(APP_OPTS)
  foreach(opt ${{APP_OPTS}})
    file(APPEND "${{_lint_rsp}}" "${{opt}}\\n")
  endforeach()
endif()
foreach(def ${{_lint_defines}})
  file(APPEND "${{_lint_rsp}}" "${{def}}\\n")
endforeach()
foreach(inc ${{_lint_includes}})
  file(APPEND "${{_lint_rsp}}" "${{inc}}\\n")
endforeach()

# Pick flags per compiler
if (CMAKE_CXX_COMPILER_ID STREQUAL "Clang")
  set(LINT_CORE_FLAGS
    -std=c++{cxx_standard} -fsyntax-only
    -Wfatal-errors -ferror-limit=1
    -ftemplate-backtrace-limit=6 -fconstexpr-backtrace-limit=3 -fmacro-backtrace-limit=2
    -fno-caret-diagnostics -fdiagnostics-color=never -fdiagnostics-show-option
    -fdiagnostics-format=json
  )
elseif (CMAKE_CXX_COMPILER_ID STREQUAL "GNU")
  set(LINT_CORE_FLAGS
    -std=c++{cxx_standard} -fsyntax-only
    -Wfatal-errors -fmax-errors=1
    -fconcepts-diagnostics-depth=2
    -fno-diagnostics-show-caret -fdiagnostics-color=never
    -fdiagnostics-format=json
  )
else()
  # MSVC: fall back to /analyze JSON? For now: no-op lint with success.
  set(LINT_CORE_FLAGS "")
endif()

# One lint target that iterates all TU's; writes a single JSON per TU into build/lint/
add_custom_target(lint
  COMMENT "Running tight JSON diagnostics per translation unit..."
)
foreach(src ${{APP_SOURCES}})
  get_filename_component(src_name "${{src}}" NAME_WE)
  set(out_json "${{CMAKE_BINARY_DIR}}/lint/${{src_name}}.json")
  file(MAKE_DIRECTORY "${{CMAKE_BINARY_DIR}}/lint")
  add_custom_command(TARGET lint POST_BUILD
    COMMAND ${{CMAKE_CXX_COMPILER}} "@${{_lint_rsp}}" ${{LINT_CORE_FLAGS}} "${{src}}" 2> "${{out_json}}"
    BYPRODUCTS "${{out_json}}"
    COMMENT "Lint ${{src_name}}"
    VERBATIM
  )
endforeach()'''

    # Handle Position Independent Code
    pic_section = ""
    if enable_pic:
        pic_section = "\n# Enable position independent code\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\n"

    cmake_template = f'''cmake_minimum_required(VERSION {cmake_min_version})
project({project_name} LANGUAGES CXX)

# Always declare the language level once
set(CMAKE_CXX_STANDARD {cxx_standard})
set(CMAKE_CXX_STANDARD_REQUIRED ON)
set(CMAKE_CXX_EXTENSIONS OFF){pic_section}

# Export compile_commands.json (handy for your reducer or tools)
set(CMAKE_EXPORT_COMPILE_COMMANDS ON)

# 1) Warnings-as-errors & friends — per-compiler, per-config, target-attachable
add_library(project_warnings INTERFACE)

if (MSVC)
  target_compile_options(project_warnings INTERFACE
    /W4 /WX /permissive- /Zc:preprocessor /Zc:__cplusplus /EHsc
    # optional noise trims:
    # /wd4244 /wd4267
  )
else()
  target_compile_options(project_warnings INTERFACE
    -Wall -Wextra -Wconversion -Wshadow -Werror
    -Wnon-virtual-dtor -Woverloaded-virtual -Wimplicit-fallthrough
    -fdiagnostics-show-option
  )
endif()

# 2) Sanitizers toggles (don't hardcode CMAKE_CXX_FLAGS_*; compose options instead)
option(ENABLE_ASAN "Enable AddressSanitizer" $<NOT:$<CXX_COMPILER_ID:MSVC>>)
option(ENABLE_UBSAN "Enable UBSan"           $<NOT:$<CXX_COMPILER_ID:MSVC>>)
add_library(project_sanitizers INTERFACE)
if (NOT MSVC)
  if (ENABLE_ASAN)
    target_compile_options(project_sanitizers INTERFACE -fsanitize=address)
    target_link_options(project_sanitizers    INTERFACE -fsanitize=address)
  endif()
  if (ENABLE_UBSAN)
    target_compile_options(project_sanitizers INTERFACE -fsanitize=undefined)
    target_link_options(project_sanitizers    INTERFACE -fsanitize=undefined)
  endif()
endif()

# 3) Your target(s) - replace with your actual source files
add_executable({project_name} main.cpp)
target_link_libraries({project_name} PRIVATE project_warnings project_sanitizers)

# 4) LLM-focused "lint" that compiles TUs with tight diagnostics (no linking)
#    Important: use the *same* include dirs/defs/opts as {project_name} so headers resolve.
get_target_property(APP_SOURCES {project_name} SOURCES)
get_target_property(APP_INCLUDES {project_name} INCLUDE_DIRECTORIES)
get_target_property(APP_DEFS     {project_name} COMPILE_DEFINITIONS)
get_target_property(APP_OPTS     {project_name} COMPILE_OPTIONS)

# Compose include and define flags portably for non-MSVC compilers.
set(_lint_includes "")
if(APP_INCLUDES)
  foreach(inc ${{APP_INCLUDES}})
    list(APPEND _lint_includes -I${{inc}})
  endforeach()
endif()

set(_lint_defines "")
if(APP_DEFS)
  foreach(def ${{APP_DEFS}})
    list(APPEND _lint_defines -D${{def}})
  endforeach()
endif()

# Build a response file so the command stays short (token-cheap logs)
set(_lint_rsp "${{CMAKE_BINARY_DIR}}/lint_args.rsp")
file(WRITE  "${{_lint_rsp}}" "")
if(APP_OPTS)
  foreach(opt ${{APP_OPTS}})
    file(APPEND "${{_lint_rsp}}" "${{opt}}\\n")
  endforeach()
endif()
foreach(def ${{_lint_defines}})
  file(APPEND "${{_lint_rsp}}" "${{def}}\\n")
endforeach()
foreach(inc ${{_lint_includes}})
  file(APPEND "${{_lint_rsp}}" "${{inc}}\\n")
endforeach()

# Pick flags per compiler
if (CMAKE_CXX_COMPILER_ID STREQUAL "Clang")
  set(LINT_CORE_FLAGS
    -std=c++23 -fsyntax-only
    -Wfatal-errors -ferror-limit=1
    -ftemplate-backtrace-limit=6 -fconstexpr-backtrace-limit=3 -fmacro-backtrace-limit=2
    -fno-caret-diagnostics -fdiagnostics-color=never -fdiagnostics-show-option
    -fdiagnostics-format=json
  )
elseif (CMAKE_CXX_COMPILER_ID STREQUAL "GNU")
  set(LINT_CORE_FLAGS
    -std=c++20 -fsyntax-only
    -Wfatal-errors -fmax-errors=1
    -fconcepts-diagnostics-depth=2
    -fno-diagnostics-show-caret -fdiagnostics-color=never
    -fdiagnostics-format=json
  )
else()
  # MSVC: fall back to /analyze JSON? For now: no-op lint with success.
  set(LINT_CORE_FLAGS "")
endif()

# One lint target that iterates all TU's; writes a single JSON per TU into build/lint/
add_custom_target(lint
  COMMENT "Running tight JSON diagnostics per translation unit..."
)
foreach(src ${{APP_SOURCES}})
  get_filename_component(src_name "${{src}}" NAME_WE)
  set(out_json "${{CMAKE_BINARY_DIR}}/lint/${{src_name}}.json")
  file(MAKE_DIRECTORY "${{CMAKE_BINARY_DIR}}/lint")
  add_custom_command(TARGET lint POST_BUILD
    COMMAND ${{CMAKE_CXX_COMPILER}} "@${{_lint_rsp}}" ${{LINT_CORE_FLAGS}} "${{src}}" 2> "${{out_json}}"
    BYPRODUCTS "${{out_json}}"
    COMMENT "Lint ${{src_name}}"
    VERBATIM
  )
endforeach()
'''

    # Write CMakeLists.txt
    cmake_file.write_text(cmake_template)
    print(f"✅ Created CMAKE_GUIDANCE.md compliant CMakeLists.txt")

    # Create main.cpp if it doesn't exist
    main_file = project_dir / "main.cpp"
    if not main_file.exists():
        main_template = f'''#include <iostream>

int main() {{
    std::cout << "Hello from {project_name}!\\n";
    return 0;
}}
'''
        main_file.write_text(main_template)
        print(f"✅ Created sample main.cpp")

    # Validate the created setup
    cmake_validation = validate_cmake_guidance(project_dir)
    print(f"🏗️ CMake compliance: ✅ {cmake_validation['overall_score']:.0f}% compliant with CMAKE_GUIDANCE.md")

    print(f"\n🚀 Next steps:")
    print(f"   • cd {project_dir if args.name else '.'}")
    print(f"   • llmtk context export    # Generate build context")
    print(f"   • cmake -B build -G Ninja  # Configure build")
    print(f"   • cmake --build build      # Build project")
    print(f"   • cmake --build build --target lint  # Run LLM-optimized linting")

    capabilities_path = generate_capabilities_json(EXPORTS/"capabilities.json")
    print(f"\n📚 Capabilities manifest available at {capabilities_path}")

    return 0

def cmd_doctor(args):
    # Load from manifest to get complete tool list
    tools_manifest = ROOT / "manifest" / "tools.yaml"
    if tools_manifest.exists():
        tools_config = load_yaml(tools_manifest)
        if tools_config and "tools" in tools_config:
            # Get all tools from manifest, prioritizing core and recommended
            all_tools = tools_config["tools"]
            tools = []
            for name, config in all_tools.items():
                role = config.get("role", "optional")
                if role in ["core", "recommended"]:
                    tools.append(name)
            # Add any remaining tools
            for name in all_tools.keys():
                if name not in tools:
                    tools.append(name)
        else:
            # Fallback list
            tools = [
                "cmake","ninja","clangd","clang-tidy","clang-format",
                "include-what-you-use","cppcheck","rg","fd","jq","yq","bear","ccache","mold"
            ]
    else:
        # Fallback list
        tools = [
            "cmake","ninja","clangd","clang-tidy","clang-format",
            "include-what-you-use","cppcheck","rg","fd","jq","yq","bear","ccache","mold"
        ]

    report = {"_meta": {"generated_at": datetime.datetime.now(datetime.UTC).isoformat()}}

    found_tools = []
    missing_core = []
    missing_recommended = []
    missing_optional = []

    # Add local bin to PATH for tool discovery (use same path as install command)
    local_bin = ROOT / ".llmtk" / "bin"
    old_path = os.environ.get("PATH", "")
    if local_bin.exists():
        os.environ["PATH"] = f"{local_bin}:{old_path}"

    for t in tools:
        # Get the actual command to check from the manifest
        actual_cmd = t
        if tools_manifest.exists() and tools_config and "tools" in tools_config:
            tool_config = tools_config["tools"].get(t, {})
            check_config = tool_config.get("check", {})
            if isinstance(check_config, dict) and "cmd" in check_config:
                actual_cmd = check_config["cmd"][0]  # First element is the command name

        path = shutil.which(actual_cmd)
        info = {"found": bool(path), "path": path or None}
        if path:
            try:
                out = run([actual_cmd, "--version"], check=False).stdout.splitlines()
                info["version_line"] = out[0] if out else None
            except Exception:
                info["version_line"] = None
            found_tools.append(t)
        else:
            # Categorize missing tools by role
            if tools_manifest.exists() and tools_config and "tools" in tools_config:
                tool_config = tools_config["tools"].get(t, {})
                role = tool_config.get("role", "optional")
                if role == "core":
                    missing_core.append(t)
                elif role == "recommended":
                    missing_recommended.append(t)
                else:
                    missing_optional.append(t)
            else:
                missing_core.append(t)  # Default to core for fallback
        report[t] = info

    # Restore original PATH
    os.environ["PATH"] = old_path

    # Add CMake validation to the report
    cmake_validation = validate_cmake_guidance()
    report["_cmake"] = cmake_validation

    # Add summary to report
    report["_summary"] = {
        "total_tools": len(tools),
        "found": len(found_tools),
        "missing": len(tools) - len(found_tools),
        "missing_core": missing_core,
        "missing_recommended": missing_recommended,
        "missing_optional": missing_optional,
        "cmake_compliance_score": cmake_validation["overall_score"]
    }

    out = EXPORTS / "doctor.json"
    write_json(out, report)

    # Print user-friendly summary if not being called from install
    if not hasattr(args, '_from_install'):
        # Check if this is CMake-focused or full health check
        cmake_only = hasattr(args, 'cmake') and args.cmake

        if cmake_only:
            print()
            print("🏗️ CMAKE COMPLIANCE CHECK")
            print("=" * 40)
        else:
            print()
            print("🏥 HEALTH CHECK SUMMARY")
            print("=" * 40)
            print(f"✅ Found: {len(found_tools)}/{len(tools)} tools")

        # Show different content based on mode
        if not cmake_only:
            if missing_core:
                print(f"\n❌ Missing core tools ({len(missing_core)}):")
                for tool in missing_core:
                    print(f"   • {tool}")

            if missing_recommended:
                print(f"\n⚠️ Missing recommended tools ({len(missing_recommended)}):")
                for tool in missing_recommended:
                    print(f"   • {tool}")

        # Show CMake compliance status (always shown)
        cmake_score = cmake_validation["overall_score"]
        if cmake_validation["cmake_file_exists"]:
            if cmake_score >= 80:
                print(f"\n🏗️ CMake setup: ✅ {cmake_score:.0f}% compliant with CMAKE_GUIDANCE.md")
            elif cmake_score >= 50:
                print(f"\n🏗️ CMake setup: ⚠️ {cmake_score:.0f}% compliant with CMAKE_GUIDANCE.md")
            else:
                print(f"\n🏗️ CMake setup: ❌ {cmake_score:.0f}% compliant with CMAKE_GUIDANCE.md")

            # Show detailed compliance breakdown in CMake-only mode
            if cmake_only:
                print(f"\n📋 Compliance Details:")
                for key, value in cmake_validation["compliance"].items():
                    status = "✅" if value else "❌"
                    readable_name = key.replace("_", " ").title()
                    print(f"   {status} {readable_name}")

            if cmake_validation["suggestions"]:
                suggestions_to_show = cmake_validation["suggestions"] if cmake_only else cmake_validation["suggestions"][:3]
                print("   CMake improvements needed:")
                for suggestion in suggestions_to_show:
                    print(f"   • {suggestion}")
        else:
            print(f"\n🏗️ CMake setup: ❌ No CMakeLists.txt found")
            print("   • Run 'llmtk init' to create LLM-optimized project structure")

        # Recommendations section
        needs_tools = missing_core or missing_recommended
        needs_cmake = cmake_score < 80

        if (not cmake_only and needs_tools) or needs_cmake:
            print(f"\n💡 RECOMMENDED ACTIONS:")
            if not cmake_only and needs_tools:
                print(f"   • Run 'llmtk install' to install missing tools")
                print(f"   • Use 'llmtk install --local' for non-sudo installation")
            if needs_cmake:
                print(f"   • Run 'llmtk init' to upgrade CMake setup for LLM workflows")
                print(f"   • See CMAKE_GUIDANCE.md for manual setup instructions")

        if not cmake_only:
            print(f"\n📄 Detailed report: {out}")

    print(str(out))

def cmd_context_export(args):
    # Check CMake compliance if requested
    if hasattr(args, 'require_cmake') and args.require_cmake:
        cmake_validation = validate_cmake_guidance()
        if not cmake_validation["cmake_file_exists"]:
            print("❌ Error: No CMakeLists.txt found. Use --require-cmake only in CMake projects.")
            print("💡 Run 'llmtk init' to create a compliant CMake project")
            return 1

        if cmake_validation["overall_score"] < 80:
            print(f"❌ Error: CMake compliance too low ({cmake_validation['overall_score']:.0f}% < 80%)")
            print("💡 Missing CMAKE_GUIDANCE.md patterns:")
            for suggestion in cmake_validation["suggestions"][:3]:
                print(f"   • {suggestion}")
            print("💡 Run 'llmtk init' to upgrade your CMake setup")
            return 1

        print(f"✅ CMake compliance verified ({cmake_validation['overall_score']:.0f}%)")

    build = pathlib.Path(args.build)
    build.mkdir(exist_ok=True)

    # Compile DB
    try:
        run(["cmake","-S",".","-B",str(build),"-G","Ninja","-DCMAKE_EXPORT_COMPILE_COMMANDS=ON"], check=True)
        if (build/"compile_commands.json").exists():
            (EXPORTS/"compile_commands.json").write_bytes((build/"compile_commands.json").read_bytes())
    except Exception as e:
        # If compile_commands missing, try bear as a fallback
        bear = shutil.which("bear")
        if bear:
            try:
                run([bear, "--", "cmake", "--build", str(build)], check=False)
                if (build/"compile_commands.json").exists():
                    (EXPORTS/"compile_commands.json").write_bytes((build/"compile_commands.json").read_bytes())
            except Exception:
                pass
    # If still missing but project root has a compile_commands.json, copy it
    if not (EXPORTS/"compile_commands.json").exists() and (pathlib.Path.cwd()/"compile_commands.json").exists():
        (EXPORTS/"compile_commands.json").write_bytes((pathlib.Path.cwd()/"compile_commands.json").read_bytes())

    # CMake File API codemodel
    q = build/".cmake"/"api"/"v1"/"query"; q.mkdir(parents=True, exist_ok=True)
    (q/"codemodel-v2").write_text("")
    if args.deep:
        (q/"cache-v2").write_text("")
        (q/"toolchains-v1").write_text("")
    run(["cmake","--build",str(build)], check=False)
    reply = build/".cmake"/"api"/"v1"/"reply"
    if reply.exists():
        (EXPORTS/"cmake-file-api").mkdir(exist_ok=True)
        for p in reply.iterdir():
            (EXPORTS/"cmake-file-api"/p.name).write_bytes(p.read_bytes())

    summary = {
        "deep_export": args.deep,
        "compile_commands": "exports/compile_commands.json" if (EXPORTS/"compile_commands.json").exists() else None,
        "cmake_file_api": {
            "dir": "exports/cmake-file-api/",
            "files": sorted([p.name for p in (EXPORTS/"cmake-file-api").iterdir()]) if (EXPORTS/"cmake-file-api").exists() else []
        } if (EXPORTS/"cmake-file-api").exists() else None,
        "generated_at": datetime.datetime.now(datetime.UTC).isoformat()
    }

    if args.deep and (EXPORTS/"cmake-file-api").exists():
        summary["deep_info"] = {}
        # Try to find the codemodel file
        codemodel_file = next((p for p in (EXPORTS/"cmake-file-api").iterdir() if p.name.startswith("codemodel-v2-")), None)
        if codemodel_file:
            try:
                codemodel = json.loads(codemodel_file.read_text())
                summary["deep_info"]["codemodel"] = {
                    "configurations": [conf.get("name") for conf in codemodel.get("configurations", [])],
                    "targets": [t.get("name") for conf in codemodel.get("configurations", []) for t in conf.get("targets", [])]
                }
            except Exception:
                pass # ignore malformed json

        # Try to find the cache file
        cache_file = next((p for p in (EXPORTS/"cmake-file-api").iterdir() if p.name.startswith("cache-v2-")), None)
        if cache_file:
            try:
                cache = json.loads(cache_file.read_text())
                summary["deep_info"]["cache"] = {
                    "CMAKE_CXX_COMPILER": next((item.get("value") for item in cache.get("entries", []) if item.get("name") == "CMAKE_CXX_COMPILER"), None),
                    "CMAKE_CXX_STANDARD": next((item.get("value") for item in cache.get("entries", []) if item.get("name") == "CMAKE_CXX_STANDARD"), None),
                }
            except Exception:
                pass

        # Try to find the toolchains file
        toolchains_file = next((p for p in (EXPORTS/"cmake-file-api").iterdir() if p.name.startswith("toolchains-v1-")), None)
        if toolchains_file:
            try:
                toolchains = json.loads(toolchains_file.read_text())
                if toolchains.get("toolchains"):
                    summary["deep_info"]["toolchains"] = toolchains.get("toolchains")
            except Exception:
                pass

    write_json(EXPORTS/"context.json", summary)
    print(str(EXPORTS/"context.json"))

def cmd_context_pack(args):
    """Pack the context export into a tarball."""
    if args.redact:
        print("Note: Redaction is not yet implemented.")

    output_filename = EXPORTS.parent / "context-export.tar.gz"
    with tarfile.open(output_filename, "w:gz") as tar:
        tar.add(EXPORTS, arcname=EXPORTS.name)

    print(f"Context export packed to: {output_filename}")

def cmd_analyze(args):
    reports = EXPORTS/"reports"; reports.mkdir(exist_ok=True)
    cache_dir = ROOT / ".llmtk" / "cache" / "analyze"; cache_dir.mkdir(parents=True, exist_ok=True)
    compile_db = EXPORTS/"compile_commands.json"
    paths = args.paths or []

    def get_file_hash(path):
        h = hashlib.sha256()
        with open(path, 'rb') as f:
            while True:
                data = f.read(65536)
                if not data:
                    break
                h.update(data)
        return h.hexdigest()

    def get_changed_files():
        try:
            result = run(["git", "diff", "--name-only", "HEAD"], check=True)
            return [pathlib.Path(p).resolve() for p in result.stdout.splitlines()]
        except Exception as e:
            print(f"Warning: Could not get changed files from git: {e}")
            return None

    def read_compile_db_files():
        files = []
        if compile_db.exists():
            try:
                data = json.loads(compile_db.read_text())
                files = [entry.get("file") for entry in data if entry.get("file")]
            except Exception:
                files = []
        
        # Convert to absolute paths for comparison
        files = [pathlib.Path(f).resolve() for f in files]

        # Filter by user-provided paths if any
        if paths:
            keep = []
            for f in files:
                for p in paths:
                    p_abs = pathlib.Path(p).resolve()
                    if p_abs.is_dir():
                        if f.is_relative_to(p_abs):
                            keep.append(f)
                            break
                    elif f == p_abs:
                        keep.append(f)
                        break
            files = keep
        
        # Incremental filtering
        if args.incremental:
            changed_files = get_changed_files()
            if changed_files is not None:
                files = [f for f in files if f in changed_files]

        # Dedup while preserving order
        seen = set(); uniq = []
        for f in files:
            if f not in seen:
                seen.add(f); uniq.append(str(f))
        return uniq

    files_in_db = read_compile_db_files()

    # Caching logic
    cache_key_str = args.cache_key or ""
    if files_in_db:
        files_hash = hashlib.sha256("".join(sorted(files_in_db)).encode()).hexdigest()
        content_hash = hashlib.sha256()
        for f in files_in_db:
            try:
                content_hash.update(get_file_hash(f).encode())
            except Exception:
                pass # Ignore files that can't be read
        cache_key_str += files_hash + content_hash.hexdigest()

    cache_key = hashlib.sha256(cache_key_str.encode()).hexdigest()
    cache_file = cache_dir / f"{cache_key}.json"

    if cache_file.exists():
        print(f"Using cached analysis results from {cache_file}")
        cached_data = json.loads(cache_file.read_text())
        for report_name, report_data in cached_data.items():
            write_json(reports/f"{report_name}.json", report_data)
        print(str(reports))
        return

    def run_proc(cmd, cwd=None):
        return subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)

    # clang-tidy with export-fixes
    clang_tidy_path = shutil.which("clang-tidy")
    clang_tidy_report = {"available": bool(clang_tidy_path), "diagnostics": [], "fixes": [], "version": None, "inputs": []}
    if clang_tidy_path:
        try:
            v = run_proc([clang_tidy_path, "--version"]).stdout.splitlines()
            clang_tidy_report["version"] = v[0] if v else None
        except Exception:
            pass
        # Choose files to analyze
        target_files = files_in_db
        clang_tidy_report["inputs"] = target_files[:50]  # cap in report
        tidy_diags = []
        tidy_fixes = []
        diag_re = re.compile(r"^(?P<file>[^:]+):(?P<line>\d+):(?!\d+:)\s*(?P<col>\d+): (?P<severity>warning|error|note): (?P<msg>.*?)(?: \[(?P<check>[A-Za-z0-9_.\-]+)\])?$")
        # If no compile DB, still try on explicit paths
        if not target_files and paths:
            # restrict to common C/C++ extensions
            exts = (".c", ".cc", ".cxx", ".cpp", ".h", ".hh", ".hpp", ".hxx")
            for p in paths:
                pp = pathlib.Path(p)
                if pp.is_file() and pp.suffix in exts:
                    target_files.append(str(pp))
                elif pp.is_dir():
                    for f in pp.rglob("*"):
                        if f.suffix in exts:
                            target_files.append(str(f))
        # Analyze (best-effort, sequential to keep simple)
        for f in target_files[:200]:  # prevent runaway
            fixes_yaml = pathlib.Path(tempfile.gettempdir())/f"llmtk_tidy_{abs(hash(f))}.yaml"
            if fixes_yaml.exists():
                try: fixes_yaml.unlink()
                except Exception: pass
            cmd = [clang_tidy_path, f, "-quiet", f"-export-fixes={fixes_yaml}"]
            if compile_db.exists():
                # clang-tidy accepts directory or file to -p; pass dir containing the DB
                cmd.extend(["-p", str(compile_db.parent.resolve())])
            res = run_proc(cmd)
            for line in (res.stdout + "\n" + res.stderr).splitlines():
                m = diag_re.match(line.strip())
                if m:
                    d = m.groupdict()
                    d["line"] = int(d["line"]); d["col"] = int(d["col"])
                    tidy_diags.append(d)
            # Parse export-fixes YAML (if any)
            if fixes_yaml.exists():
                content = load_yaml(fixes_yaml)
                if content and isinstance(content, dict):
                    # content has Diagnostics: [{DiagnosticMessage:{Message,FilePath,FileOffset}, Replacements:[{FilePath,Offset,Length,ReplacementText}]}]
                    diags = content.get("Diagnostics") or []
                    for item in diags:
                        dm = item.get("DiagnosticMessage") or {}
                        repl = item.get("Replacements") or []
                        tidy_fixes.append({
                            "file": dm.get("FilePath"),
                            "message": dm.get("Message"),
                            "file_offset": dm.get("FileOffset"),
                            "replacements": [
                                {
                                    "file": r.get("FilePath"),
                                    "offset": r.get("Offset"),
                                    "length": r.get("Length"),
                                    "replacement": r.get("ReplacementText"),
                                } for r in repl if isinstance(r, dict)
                            ],
                        })
                try: fixes_yaml.unlink()
                except Exception: pass
        clang_tidy_report["diagnostics"] = tidy_diags
        clang_tidy_report["fixes"] = tidy_fixes
    write_json(reports/"clang-tidy.json", clang_tidy_report)

    # include-what-you-use (IWYU)
    iwyu_bin = shutil.which("include-what-you-use")
    iwyu_tool = shutil.which("iwyu-tool")
    iwyu_report = {"available": bool(iwyu_bin or iwyu_tool), "version": None, "suggestions": []}
    if iwyu_bin or iwyu_tool:
        # Try to get version line
        try:
            if iwyu_bin:
                vv = run_proc([iwyu_bin, "--version"]).stdout.splitlines()
                iwyu_report["version"] = vv[0] if vv else None
        except Exception:
            pass
        out = None
        if iwyu_tool and compile_db.exists():
            # Run across the compilation database
            res = run_proc([iwyu_tool, "-p", str(compile_db.parent.resolve())])
            out = res.stdout or res.stderr
        elif iwyu_bin and files_in_db:
            # Try first few files using compile DB path; IWYU will discover flags via -Xiwyu?
            # We fallback to plain invocation which may be noisy
            collected = []
            for f in files_in_db[:20]:
                res = run_proc([iwyu_bin, f])
                collected.append((res.stdout or "") + "\n" + (res.stderr or ""))
            out = "\n".join(collected)
        def parse_iwyu(text: str):
            suggestions = {}
            current = None
            mode = None  # 'add' or 'remove'
            for raw in text.splitlines():
                line = raw.strip()
                if not line:
                    mode = None
                    continue
                m_add = re.search(r"^(.*) should add these lines:", line)
                m_rem = re.search(r"^(.*) should remove these lines:", line)
                if m_add or m_rem:
                    current = (m_add or m_rem).group(1).strip()
                    entry = suggestions.setdefault(current, {"add": [], "remove": []})
                    mode = 'add' if m_add else 'remove'
                    continue
                if line.startswith("The full include-list for "):
                    current = None
                    mode = None
                    continue
                if mode in ('add','remove') and (line.startswith('#include') or line.startswith('namespace') or line.startswith('using')):
                    # IWYU annotates with comments; keep the include text as-is
                    suggestions.setdefault(current, {"add": [], "remove": []})[mode].append(line)
            return [{"file": f, **v} for f, v in suggestions.items()]
        if out:
            iwyu_report["suggestions"] = parse_iwyu(out)[:200]
    write_json(reports/"iwyu.json", iwyu_report)

    # cppcheck
    cppcheck_bin = shutil.which("cppcheck")
    cppcheck_report = {"available": bool(cppcheck_bin), "version": None, "diagnostics": []}
    if cppcheck_bin:
        try:
            vv = run_proc([cppcheck_bin, "--version"]).stdout.splitlines()
            cppcheck_report["version"] = vv[0] if vv else None
        except Exception:
            pass
        xml_tmp = pathlib.Path(tempfile.gettempdir())/"llmtk_cppcheck.xml"
        cmd = [cppcheck_bin, "--enable=all", "--inconclusive", "--quiet", "--xml", "--xml-version=2"]
        if compile_db.exists():
            cmd.extend(["--project", str(compile_db)])
        else:
            # Fall back to paths list or current dir
            search_paths = paths or ["."]
            cmd.extend(search_paths)
        res = subprocess.run(cmd, text=True, capture_output=True)
        # cppcheck writes XML to stderr
        xml_data = res.stderr
        diags = []
        try:
            import xml.etree.ElementTree as ET
            root = ET.fromstring(xml_data)
            for error in root.iterfind('.//errors/error'):
                ed = error.attrib
                locs = []
                for loc in error.iterfind('location'):
                    locs.append({
                        "file": loc.attrib.get("file"),
                        "line": int(loc.attrib.get("line", "0")),
                        "column": int(loc.attrib.get("column", "0"))
                    })
                diags.append({
                    "id": ed.get("id"),
                    "severity": ed.get("severity"),
                    "msg": ed.get("msg"),
                    "verbose": ed.get("verbose"),
                    "locations": locs
                })
        except Exception:
            # If XML parsing fails, include raw for inspection
            diags = [{"raw": xml_data[:200000]}]
        cppcheck_report["diagnostics"] = diags
    write_json(reports/"cppcheck.json", cppcheck_report)

    # Save results to cache
    all_reports = {
        "clang-tidy": clang_tidy_report,
        "iwyu": iwyu_report,
        "cppcheck": cppcheck_report
    }
    write_json(cache_file, all_reports)
    print(f"Saved analysis results to cache: {cache_file}")

    # Generate SARIF output if requested
    if getattr(args, 'sarif', False):
        try:
            # Import the SARIF converter
            sarif_converter_path = MODULES / "sarif_converter.py"
            if sarif_converter_path.exists():
                # Run the SARIF converter
                cmd = [
                    sys.executable, str(sarif_converter_path),
                    str(reports / "analysis.sarif"),
                    str(reports / "clang-tidy.json"),
                    str(reports / "cppcheck.json"),
                    str(reports / "iwyu.json")
                ]
                result = subprocess.run(cmd, capture_output=True, text=True, cwd=ROOT)
                if result.returncode == 0:
                    print(f"SARIF report: {reports / 'analysis.sarif'}")
                else:
                    print(f"Warning: SARIF conversion failed: {result.stderr}")
            else:
                print("Warning: SARIF converter not found")
        except Exception as e:
            print(f"Warning: SARIF conversion failed: {e}")

    print(str(reports))


def cmd_stderr_thin(args):
    diagnostics_dir = EXPORTS / "diagnostics"
    diagnostics_dir.mkdir(parents=True, exist_ok=True)

    budget = _DEFAULT_CONTEXT_BUDGETS.get(args.level, 6000)
    if args.context_budget and args.context_budget > 0:
        budget = args.context_budget

    json_out = pathlib.Path(args.json) if args.json else diagnostics_dir / "stderr-thin.json"
    text_out = pathlib.Path(args.text) if args.text else diagnostics_dir / "stderr-thin.txt"
    raw_out = pathlib.Path(args.raw) if args.raw else diagnostics_dir / "stderr-raw.txt"

    raw_stderr = ""
    stdout_text = ""
    run_info = {
        "mode": None,
        "command": None,
        "returncode": None,
        "duration_seconds": None,
    }

    compile_db_path = None
    structured_source = None

    if args.log:
        log_path = pathlib.Path(args.log)
        if not log_path.exists():
            print(f"Error: log file not found: {log_path}")
            return 1
        raw_stderr = log_path.read_text()
        run_info.update({
            "mode": "log",
            "log_path": str(log_path.resolve()),
        })
    elif args.compile is not None or args.compile_index is not None:
        compile_db_path = _find_compile_database(args.compile_db)
        if not compile_db_path or not compile_db_path.exists():
            hint = args.compile_db or "exports/compile_commands.json"
            print(f"Error: compile_commands.json not found (looked for {hint})")
            return 1
        try:
            compile_entries = json.loads(compile_db_path.read_text())
        except Exception as exc:
            print(f"Error: failed to read compile database: {exc}")
            return 1
        if not isinstance(compile_entries, list):
            print("Error: compile_commands.json has unexpected format")
            return 1

        entry = None
        selected_index = None
        if args.compile_index is not None:
            if 0 <= args.compile_index < len(compile_entries):
                entry = compile_entries[args.compile_index]
                selected_index = args.compile_index
            else:
                print(f"Error: compile index {args.compile_index} is out of range (size={len(compile_entries)})")
                return 1
        else:
            needle = (args.compile or "").lower()
            for idx, candidate in enumerate(compile_entries):
                haystack = " ".join(filter(None, [candidate.get("file"), candidate.get("command"), candidate.get("directory")]))
                if needle in haystack.lower():
                    entry = candidate
                    selected_index = idx
                    break
            if entry is None:
                print(f"Error: no compile command matched '{args.compile}'")
                return 1

        command = _prepare_compile_command(entry)
        if not command:
            print("Error: unable to prepare compile command arguments")
            return 1

        cwd = pathlib.Path(entry.get("directory") or pathlib.Path.cwd())
        if not cwd.exists():
            cwd = pathlib.Path.cwd()
        start = time.perf_counter()
        result = run(command, cwd=str(cwd), check=False)
        duration = time.perf_counter() - start
        raw_stderr = (result.stderr or "")
        stdout_text = result.stdout or ""
        run_info.update({
            "mode": "compile_command",
            "command": list(command),
            "returncode": result.returncode,
            "duration_seconds": round(duration, 6),
            "directory": str(cwd),
            "compile_db": str(compile_db_path.resolve()),
            "compile_entry_index": selected_index,
            "compile_file": entry.get("file"),
        })
    elif args.cmd:
        command = [c for c in args.cmd if c]
        if command and command[0] == "--":
            command = command[1:]
        if not command:
            print("Error: no command provided for stderr capture")
            return 1
        cwd = pathlib.Path(args.cwd).resolve() if args.cwd else pathlib.Path.cwd()
        if not cwd.exists():
            print(f"Error: working directory does not exist: {cwd}")
            return 1
        start = time.perf_counter()
        result = run(command, cwd=str(cwd), check=False)
        duration = time.perf_counter() - start
        raw_stderr = result.stderr or ""
        stdout_text = result.stdout or ""
        run_info.update({
            "mode": "command",
            "command": list(command),
            "returncode": result.returncode,
            "duration_seconds": round(duration, 6),
            "directory": str(cwd),
        })
    else:
        if sys.stdin and not sys.stdin.isatty():
            raw_stderr = sys.stdin.read()
            run_info.update({"mode": "stdin"})
        else:
            print("Error: provide --log, --compile/--compile-index, a command, or pipe stderr to STDIN")
            return 1

    lines = raw_stderr.splitlines()
    collapsed_lines, template_omitted = _collapse_template_traces(lines)

    diagnostics = _parse_structured_diagnostics(raw_stderr)
    if diagnostics:
        structured_source = "clang-json"
    else:
        diagnostics = _parse_text_diagnostics(lines)
        structured_source = "text-regex" if diagnostics else None

    highlights = _format_highlights(diagnostics) if diagnostics else []
    if not highlights:
        severity_lines = [line for line in collapsed_lines if _SEVERITY_REGEX.search(line)]
        highlights = _dedupe_preserve(severity_lines[:20])

    view_lines = []
    if args.level == "summary":
        view_lines = highlights or collapsed_lines[:20]
    elif args.level == "focused":
        view_lines = _focused_view_lines(collapsed_lines)
    else:
        view_lines = collapsed_lines

    if highlights and args.level != "detailed":
        view_lines = _dedupe_preserve(highlights + view_lines)

    view_lines = [line for line in view_lines if line is not None and line.strip()]

    context_text, used_len, full_len, truncated = _apply_context_budget(view_lines, budget)

    counts = _collect_counts(diagnostics) if diagnostics else {"error": 0, "warning": 0, "note": 0, "remark": 0, "other": 0}

    text_out.parent.mkdir(parents=True, exist_ok=True)
    raw_out.parent.mkdir(parents=True, exist_ok=True)
    text_out.write_text((context_text + "\n") if context_text else "")
    raw_out.write_text(raw_stderr)

    report = {
        "_meta": {
            "generated_at": datetime.datetime.now(datetime.UTC).isoformat(),
            "level": args.level,
            "context_budget": budget,
            "structured_source": structured_source or "none",
        },
        "run": run_info,
        "counts": counts,
        "view": {
            "path": str(text_out.resolve()),
            "level": args.level,
            "context_budget": budget,
            "context_used": used_len,
            "context_full": full_len,
            "context_truncated": truncated,
        },
        "raw": {
            "stderr_path": str(raw_out.resolve()),
            "stderr_chars": len(raw_stderr),
            "stdout_chars": len(stdout_text),
            "stdout_preview": (stdout_text.splitlines()[:5] if stdout_text else []),
        },
        "highlights": highlights,
        "diagnostics": diagnostics,
        "omissions": {
            "template_frames": template_omitted,
        },
    }

    json_out.parent.mkdir(parents=True, exist_ok=True)
    write_json(json_out, report)

    print(f"Thinned diagnostics written to {text_out}")
    print(str(json_out))

def cmd_tidy(args):
    """Run clang-tidy with optional fix application."""
    compile_db = EXPORTS / "compile_commands.json"
    paths = args.paths or []

    clang_tidy = shutil.which("clang-tidy")
    if not clang_tidy:
        print("Error: clang-tidy not found in PATH")
        return 1

    if not compile_db.exists():
        print("Error: compile_commands.json not found. Run 'llmtk context export' first.")
        return 1

    # Collect files to analyze
    files_to_analyze = []
    if compile_db.exists():
        try:
            data = json.loads(compile_db.read_text())
            all_files = [pathlib.Path(entry.get("file", "")) for entry in data if entry.get("file")]

            if paths:
                # Filter by provided paths
                for file_path in all_files:
                    for path_filter in paths:
                        filter_path = pathlib.Path(path_filter).resolve()
                        try:
                            if filter_path.is_dir():
                                if file_path.is_relative_to(filter_path):
                                    files_to_analyze.append(file_path)
                            elif filter_path.is_file() and file_path.resolve() == filter_path:
                                files_to_analyze.append(file_path)
                        except Exception:
                            # Fallback for path matching
                            if str(file_path).startswith(str(filter_path)):
                                files_to_analyze.append(file_path)
            else:
                files_to_analyze = all_files

        except Exception as e:
            print(f"Error reading compile_commands.json: {e}")
            return 1

    if not files_to_analyze:
        print("No files found to analyze")
        return 0

    # Deduplicate while preserving order
    seen = set()
    unique_files = []
    for f in files_to_analyze:
        if str(f) not in seen:
            seen.add(str(f))
            unique_files.append(f)

    print(f"Running clang-tidy on {len(unique_files)} files...")

    # Build clang-tidy command
    cmd = [clang_tidy, "-p", str(compile_db.parent)]

    if args.checks:
        cmd.extend(["-checks", args.checks])

    if args.apply:
        cmd.append("--fix")
        print("Applying fixes automatically...")

    # Add files
    cmd.extend([str(f) for f in unique_files[:100]])  # Limit to prevent command line too long

    try:
        result = run(cmd, check=False)
        print(result.stdout)
        if result.stderr:
            print("Warnings/Errors:", file=sys.stderr)
            print(result.stderr, file=sys.stderr)

        if args.apply:
            print("Fixes applied. Review changes before committing.")

        return result.returncode
    except Exception as e:
        print(f"Error running clang-tidy: {e}")
        return 1

def cmd_format(args):
    """Run clang-format with check or apply options."""
    paths = args.paths or ["."]

    clang_format = shutil.which("clang-format")
    if not clang_format:
        print("Error: clang-format not found in PATH")
        return 1

    # Collect C++ files to format
    cpp_extensions = {".c", ".cc", ".cpp", ".cxx", ".c++", ".h", ".hh", ".hpp", ".hxx", ".inl"}
    files_to_format = []

    for path_str in paths:
        path = pathlib.Path(path_str)
        if path.is_file():
            if path.suffix in cpp_extensions:
                files_to_format.append(path)
        elif path.is_dir():
            for file_path in path.rglob("*"):
                if file_path.is_file() and file_path.suffix in cpp_extensions:
                    files_to_format.append(file_path)

    if not files_to_format:
        print("No C++ files found to format")
        return 0

    print(f"Found {len(files_to_format)} files to format")

    # Build clang-format command
    cmd = [clang_format]

    if args.style:
        cmd.extend(["-style", args.style])
    else:
        # Look for .clang-format file, default to LLVM if not found
        if pathlib.Path(".clang-format").exists():
            cmd.extend(["-style", "file"])
        else:
            cmd.extend(["-style", "LLVM"])

    if args.check:
        cmd.append("--dry-run")
        cmd.append("--Werror")
        print("Checking formatting (dry run)...")
    elif args.apply:
        cmd.append("-i")  # In-place modification
        print("Applying formatting changes...")
    else:
        # Default to check mode if neither specified
        cmd.append("--dry-run")
        cmd.append("--Werror")
        print("Checking formatting (dry run)... Use --apply to format files")

    # Run clang-format on files in batches to avoid command line length limits
    batch_size = 50
    total_errors = 0

    for i in range(0, len(files_to_format), batch_size):
        batch = files_to_format[i:i + batch_size]
        batch_cmd = cmd + [str(f) for f in batch]

        try:
            result = run(batch_cmd, check=False)

            if result.returncode != 0:
                total_errors += 1
                if args.check:
                    print(f"Formatting issues found in batch {i//batch_size + 1}")
                    if result.stdout:
                        print(result.stdout)
                    if result.stderr:
                        print(result.stderr)
                else:
                    print(f"Error formatting batch {i//batch_size + 1}: {result.stderr}")

        except Exception as e:
            print(f"Error running clang-format on batch {i//batch_size + 1}: {e}")
            total_errors += 1

    if args.check:
        if total_errors == 0:
            print("✅ All files are properly formatted")
            return 0
        else:
            print(f"❌ Found formatting issues in {total_errors} batches")
            return 1
    elif args.apply:
        if total_errors == 0:
            print("✅ Formatting applied successfully")
        else:
            print(f"⚠️ Completed with {total_errors} errors")
        return total_errors
    else:
        return total_errors

def cmd_gate(args):
    """Enforce SARIF severity budgets for CI gating."""
    sarif_file = pathlib.Path(args.sarif_file)

    if not sarif_file.exists():
        print(f"Error: SARIF file not found: {sarif_file}")
        return 1

    # Load configuration if provided
    config = {
        "max_errors": args.max_errors,
        "max_warnings": args.max_warnings,
        "max_notes": args.max_notes
    }

    if args.config:
        config_file = pathlib.Path(args.config)
        if config_file.exists():
            try:
                config_data = load_yaml(config_file)
                if config_data:
                    config.update(config_data)
                print(f"Loaded configuration from {config_file}")
            except Exception as e:
                print(f"Warning: Failed to load config file {config_file}: {e}")

    # Load and analyze SARIF file
    try:
        with open(sarif_file) as f:
            sarif_doc = json.load(f)
    except Exception as e:
        print(f"Error: Failed to read SARIF file: {e}")
        return 1

    # Count results by severity
    counts = {"error": 0, "warning": 0, "note": 0, "unknown": 0}
    total_results = 0

    for run in sarif_doc.get("runs", []):
        for result in run.get("results", []):
            level = result.get("level", "warning").lower()
            counts[level] = counts.get(level, 0) + 1
            total_results += 1

    # Report findings
    print(f"📊 SARIF Analysis Results")
    print(f"=" * 40)
    print(f"Total results: {total_results}")
    print(f"Errors:        {counts['error']:3d} (limit: {config['max_errors']})")
    print(f"Warnings:      {counts['warning']:3d} (limit: {config['max_warnings']})")
    print(f"Notes:         {counts['note']:3d} (limit: {config['max_notes']})")
    print(f"Unknown:       {counts['unknown']:3d}")

    # Check against limits
    violations = []
    if counts["error"] > config["max_errors"]:
        violations.append(f"Errors: {counts['error']} > {config['max_errors']}")
    if counts["warning"] > config["max_warnings"]:
        violations.append(f"Warnings: {counts['warning']} > {config['max_warnings']}")
    if counts["note"] > config["max_notes"]:
        violations.append(f"Notes: {counts['note']} > {config['max_notes']}")

    if violations:
        print(f"\n❌ GATE FAILED - Budget violations:")
        for violation in violations:
            print(f"   • {violation}")
        return 1
    else:
        print(f"\n✅ GATE PASSED - All severity budgets within limits")
        return 0

def cmd_reduce(args):
    repros = EXPORTS/"repros"; repros.mkdir(exist_ok=True)
    cvise = shutil.which("cvise")
    creduce = shutil.which("creduce")
    reducer = None
    reducer_name = None
    if cvise:
        reducer = cvise
        reducer_name = "cvise"
    elif creduce:
        reducer = creduce
        reducer_name = "creduce"

    report = {
        "input": args.input,
        "test_cmd": args.test_cmd,
        "cvise_available": bool(cvise),
        "creduce_available": bool(creduce),
        "reducer_used": reducer_name,
        "timeout": args.timeout,
        "sanitizer": args.sanitizer
    }

    if not reducer:
        report["note"] = "No reducer found; please run `llmtk install cvise` or `llmtk install creduce`"
        write_json(repros/"report.json", report)
        print(str(repros/"report.json"))
        return

    # Build the command
    cmd = [reducer]
    if args.timeout:
        cmd.extend(["--timeout", str(args.timeout)])
    cmd.extend([args.input, "--", "bash", "-lc"])

    test_cmd = args.test_cmd
    if args.sanitizer:
        if args.sanitizer == "asan":
            test_cmd = f"ASAN_OPTIONS=detect_leaks=0 {test_cmd}"
        elif args.sanitizer == "ubsan":
            test_cmd = f"UBSAN_OPTIONS=print_stacktrace=1 {test_cmd}"
        elif args.sanitizer == "tsan":
            test_cmd = f"TSAN_OPTIONS=report_atomic_races=1 {test_cmd}"
    cmd.append(test_cmd)

    # Minimal shell-out; users can expand
    try:
        result = run(cmd, check=False)
        report["note"] = f"{reducer_name} run completed"
        report["returncode"] = result.returncode
        
        post_mortem_path = repros/"post-mortem.txt"
        with open(post_mortem_path, "w") as f:
            f.write(f"# {reducer_name} Post-Mortem\n\n")
            f.write(f"## Configuration\n")
            f.write(f"- Input: {args.input}\n")
            f.write(f"- Test Command: {args.test_cmd}\n")
            f.write(f"- Timeout: {args.timeout}\n")
            f.write(f"- Sanitizer: {args.sanitizer}\n\n")
            f.write(f"## Output\n")
            f.write(f"### STDOUT\n```\n{result.stdout}\n```\n\n")
            f.write(f"### STDERR\n```\n{result.stderr}\n```\n")
        report["post_mortem"] = str(post_mortem_path)

    except Exception as e:
        report["error"] = str(e)
    write_json(repros/"report.json", report)
    print(str(repros/"report.json"))

def detect_package_manager():
    """Detect available package manager"""
    managers = [
        ("apt", ["apt-get", "apt"]),
        ("dnf", ["dnf"]),
        ("pacman", ["pacman"]),
        ("brew", ["brew"]),
        ("nix", ["nix-env"])
    ]

    for name, commands in managers:
        for cmd in commands:
            if shutil.which(cmd):
                return name
    return None

def install_tool_with_package_manager(tool_name, tool_config, pm):
    """Install a tool using system package manager"""
    if "install" not in tool_config or pm not in tool_config["install"]:
        return False

    packages = tool_config["install"][pm]
    if not packages:
        return False

    print(f"  📦 Installing {tool_name} via {pm}...")

    # Build install command based on package manager
    if pm == "apt":
        cmd = ["sudo", "apt-get", "update", "&&", "sudo", "apt-get", "install", "-y"] + packages
    elif pm == "dnf":
        cmd = ["sudo", "dnf", "install", "-y"] + packages
    elif pm == "pacman":
        cmd = ["sudo", "pacman", "-S", "--noconfirm"] + packages
    elif pm == "brew":
        cmd = ["brew", "install"] + packages
    elif pm == "nix":
        cmd = ["nix-env", "-iA"] + [f"nixpkgs.{pkg}" for pkg in packages]
    else:
        return False

    try:
        if pm == "apt":
            # Handle apt's compound command - suppress normal output but show errors
            update_result = subprocess.run(["sudo", "apt-get", "update"],
                                         text=True, capture_output=True)
            if update_result.returncode != 0:
                # Only show if it's a real error, not just GPG warnings
                if "E:" in update_result.stderr and "NO_PUBKEY" not in update_result.stderr:
                    print(f"    ⚠️ apt-get update warning (continuing): {update_result.stderr.strip()}")

            result = subprocess.run(["sudo", "apt-get", "install", "-y"] + packages,
                                  check=True, text=True, capture_output=True)
        elif pm in ["dnf", "pacman"]:
            result = subprocess.run(cmd, check=True, text=True, capture_output=True)
        else:
            result = subprocess.run(cmd, check=True, text=True, capture_output=True)

        print(f"    ✅ {tool_name} installed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"    ❌ Failed to install {tool_name}")

        # Show specific error for debugging
        if e.stderr and e.stderr.strip():
            # Extract key error messages, skip verbose output
            error_lines = [line.strip() for line in e.stderr.split('\n')
                          if line.strip() and ('E:' in line or 'Error:' in line or 'unable to locate' in line.lower())]
            if error_lines:
                print(f"       Error: {error_lines[0]}")

        return False

def install_tool_locally(tool_name, tool_config, local_bin):
    """Install a tool locally"""
    if "local_install" not in tool_config:
        print(f"    ❌ No local install method for {tool_name}")
        return False

    print(f"  🔧 Installing {tool_name} locally...")
    local_config = tool_config["local_install"]

    if "github_repo" in local_config:
        success = install_from_github(tool_name, tool_config, local_bin)
        if success:
            print(f"    ✅ {tool_name} installed locally")
        else:
            print(f"    ❌ Failed to install {tool_name} locally")
        return success

    return False

def install_tool_basic(tool_name, local_bin):
    """Basic fallback installation for specific tools"""
    if tool_name == "cppcheck":
        script = MODULES / "simple-install.sh"
        if script.exists():
            result = run([str(script)], check=False)
            return result.returncode == 0
    elif tool_name == "include-what-you-use":
        script = MODULES / "simple-install.sh"
        if script.exists():
            result = run([str(script)], check=False)
            return result.returncode == 0

    print(f"No basic installation method available for {tool_name}")
    return False

def install_from_github(tool_name, config, local_bin):
    """Install tool from GitHub releases using manifest configuration"""
    local_config = config.get("local_install", {})
    repo = local_config.get("github_repo")

    if not repo:
        print(f"No GitHub repo specified for {tool_name}")
        return False

    print(f"Installing {tool_name} locally from {repo}")

    # Use the enhanced local installer with manifest data
    enhanced_installer = MODULES / "enhanced-install.sh"
    if enhanced_installer.exists():
        # Pass manifest data as environment variables
        env = os.environ.copy()
        env.update({
            "LLMTK_TOOL_NAME": tool_name,
            "LLMTK_GITHUB_REPO": repo,
            "LLMTK_RELEASE_PATTERN": local_config.get("release_pattern", ""),
            "LLMTK_BINARY_PATH": local_config.get("binary_path", ""),
            "LLMTK_BUILD_METHOD": local_config.get("build_method", ""),
            "LLMTK_LOCAL_BIN": str(local_bin),
            "LLMTK_MANIFEST_DATA": json.dumps(local_config)
        })

        # Add checksums as JSON
        if "checksums" in local_config:
            env["LLMTK_CHECKSUMS"] = json.dumps(local_config["checksums"])

        # Add version tag if specified
        if "version_tag" in local_config:
            env["LLMTK_VERSION_TAG"] = local_config["version_tag"]

        result = subprocess.run([str(enhanced_installer), tool_name], env=env, text=True, capture_output=True)

        # Show stderr output for user feedback
        if result.stderr:
            for line in result.stderr.strip().split('\n'):
                if line.strip():
                    print(line)

        return result.returncode == 0

    # Fallback to basic installation if enhanced installer not available
    print(f"Enhanced installer not found, using basic method for {tool_name}")
    return install_tool_basic(tool_name, local_bin)

def cmd_install(args):
    """Install missing tools using manifest-driven approach"""
    tools_manifest = ROOT / "manifest" / "tools.yaml"
    if not tools_manifest.exists():
        print(f"Error: Tools manifest not found at {tools_manifest}", file=sys.stderr)
        return 1

    # Load tools manifest
    tools_config = load_yaml(tools_manifest)
    if not tools_config or "tools" not in tools_config:
        print("Error: Invalid tools manifest", file=sys.stderr)
        return 1

    # Determine installation method
    use_local = getattr(args, 'local', False)
    pm = None if use_local else detect_package_manager()

    if not use_local and not pm:
        print("No package manager detected, falling back to local installation")
        use_local = True

    # Prepare local bin directory
    local_bin = ROOT / ".llmtk" / "bin"
    local_bin.mkdir(parents=True, exist_ok=True)

    # Install missing tools
    tools_to_install = []
    if hasattr(args, 'tools') and args.tools:
        tools_to_install = args.tools
    else:
        # Install all core and recommended tools that are missing
        for tool_name, tool_config in tools_config["tools"].items():
            if tool_config.get("role") in ["core", "recommended"]:
                if not shutil.which(tool_name):
                    tools_to_install.append(tool_name)

    if not tools_to_install:
        print("All tools are already installed")
        cmd_doctor(None)
        return 0

    print(f"🚀 Installing {len(tools_to_install)} missing tools...")
    print(f"   Method: {'local' if use_local else f'package manager ({pm})'}")
    print()

    installed = []
    failed = []
    skipped = []

    for tool_name in tools_to_install:
        if tool_name not in tools_config["tools"]:
            print(f"⚠️ {tool_name} not found in manifest, skipping")
            skipped.append(tool_name)
            continue

        tool_config = tools_config["tools"][tool_name]

        if use_local:
            if install_tool_locally(tool_name, tool_config, local_bin):
                installed.append(tool_name)
            else:
                # Fall back to simple installer for specific tools
                if tool_name in ["cppcheck", "include-what-you-use"]:
                    script = MODULES / "simple-install.sh"
                    if script.exists():
                        print(f"  🔄 Using fallback installer for {tool_name}")
                        result = run([str(script)], check=False)
                        if result.returncode == 0:
                            installed.append(tool_name)
                        else:
                            failed.append(tool_name)
                    else:
                        failed.append(tool_name)
                else:
                    failed.append(tool_name)
        else:
            if install_tool_with_package_manager(tool_name, tool_config, pm):
                installed.append(tool_name)
            else:
                print(f"  🔄 Falling back to local install for {tool_name}")
                if install_tool_locally(tool_name, tool_config, local_bin):
                    installed.append(tool_name)
                else:
                    failed.append(tool_name)

    # Update PATH for doctor check
    if local_bin.exists():
        old_path = os.environ.get("PATH", "")
        os.environ["PATH"] = f"{local_bin}:{old_path}"

    # Print comprehensive summary
    print()
    print("=" * 60)
    print("📊 INSTALLATION SUMMARY")
    print("=" * 60)

    if installed:
        print(f"✅ Successfully installed ({len(installed)}):")
        for tool in installed:
            print(f"   • {tool}")

    if failed:
        print(f"\n❌ Failed to install ({len(failed)}):")
        for tool in failed:
            print(f"   • {tool}")

    if skipped:
        print(f"\n⚠️ Skipped ({len(skipped)}):")
        for tool in skipped:
            print(f"   • {tool}")

    print(f"\n📈 Total: {len(installed)}/{len(tools_to_install)} tools successfully installed")

    # Generate updated doctor report
    print("\n🔍 Running health check...")
    # Create a dummy args object to signal we're being called from install
    class DummyArgs:
        _from_install = True
    cmd_doctor(DummyArgs())

    # Show next steps
    if failed:
        print("\n🛠️ NEXT STEPS:")
        print("   • Run 'llmtk install --local' to try local installation")
        print("   • Check 'llmtk doctor' output for specific issues")
        print("   • Install failed tools manually")

    if use_local or any(local_bin.glob("*")):
        print(f"\n📁 Local tools directory: {local_bin}")
        print(f"   Add to your shell: export PATH=\"{local_bin}:$PATH\"")

    return 0 if installed else 1

def cmd_cache(args):
    """Manage the analysis cache."""
    cache_dir = ROOT / ".llmtk" / "cache" / "analyze"
    if args.cache_cmd == "clear":
        if cache_dir.exists():
            shutil.rmtree(cache_dir)
            print(f"Cache cleared at {cache_dir}")
        else:
            print("Cache is already empty.")
    elif args.cache_cmd == "show":
        if cache_dir.exists():
            print(f"Cache contents at {cache_dir}:")
            for item in cache_dir.iterdir():
                print(f"- {item.name}")
        else:
            print("Cache is empty.")

def main():
    ap = argparse.ArgumentParser(prog="llmtk", description="LLM-friendly C++/CMake toolkit")
    ap.add_argument("--version", action="version", version=f"llmtk {get_version()}")
    sub = ap.add_subparsers(dest="cmd", required=True)
    doctor_parser = sub.add_parser("doctor", help="Check tool availability and CMake compliance")
    doctor_parser.add_argument("--cmake", action="store_true", help="Focus on CMake compliance only")
    doctor_parser.set_defaults(fn=cmd_doctor)

    cx = sub.add_parser("context", help="context commands").add_subparsers(dest="sub", required=True)
    cx_exp = cx.add_parser("export", help="Export build context for LLMs")
    cx_exp.add_argument("--build", default="build", help="Build directory path")
    cx_exp.add_argument("--require-cmake", action="store_true", help="Require CMAKE_GUIDANCE.md compliance")
    cx_exp.add_argument("--deep", action="store_true", help="Perform a deep context export, gathering more detailed information.")
    cx_exp.set_defaults(fn=cmd_context_export)
    cx_pack = cx.add_parser("pack", help="Pack context export into a tarball")
    cx_pack.add_argument("--redact", action="store_true", help="Redact sensitive information from the export")
    cx_pack.set_defaults(fn=cmd_context_pack)

    an = sub.add_parser("analyze", help="Run static analysis tools")
    an.add_argument("paths", nargs="*", help="Paths to analyze")
    an.add_argument("--sarif", action="store_true", help="Output results in SARIF format")
    an.add_argument("--incremental", action="store_true", help="Analyze only changed files")
    an.add_argument("--cache-key", help="Manual cache key for analysis results")
    an.set_defaults(fn=cmd_analyze)

    stderr_parser = sub.add_parser("stderr-thin", help="Collapse compiler stderr into budget-aware highlights")
    stderr_parser.add_argument("--log", help="Path to an existing stderr log to thin")
    stderr_parser.add_argument("--compile", help="Substring to select compile command from compile_commands.json")
    stderr_parser.add_argument("--compile-index", type=int, help="Index in compile_commands.json to run")
    stderr_parser.add_argument("--compile-db", help="Path to compile_commands.json to use")
    stderr_parser.add_argument("--level", choices=["summary", "focused", "detailed"], default="focused", help="Detail level for the thinned output")
    stderr_parser.add_argument("--context-budget", type=int, help="Maximum characters to keep in the thinned view")
    stderr_parser.add_argument("--json", help="Custom path for JSON summary output")
    stderr_parser.add_argument("--text", help="Custom path for thinned text output")
    stderr_parser.add_argument("--raw", help="Custom path for captured raw stderr")
    stderr_parser.add_argument("--cwd", help="Working directory when executing a command")
    stderr_parser.add_argument("cmd", nargs=argparse.REMAINDER, help="Command to execute (use -- to separate)")
    stderr_parser.set_defaults(fn=cmd_stderr_thin)

    # Add tidy command
    tidy_parser = sub.add_parser("tidy", help="Run clang-tidy with fix options")
    tidy_parser.add_argument("paths", nargs="*", help="Paths to tidy")
    tidy_parser.add_argument("--apply", action="store_true", help="Apply suggested fixes")
    tidy_parser.add_argument("--checks", help="Specify which checks to run")
    tidy_parser.set_defaults(fn=cmd_tidy)

    # Add format command
    format_parser = sub.add_parser("format", help="Run clang-format on code")
    format_parser.add_argument("paths", nargs="*", help="Paths to format")
    format_parser.add_argument("--check", action="store_true", help="Check if files are formatted (dry run)")
    format_parser.add_argument("--apply", action="store_true", help="Apply formatting changes")
    format_parser.add_argument("--style", help="Formatting style (file, LLVM, Google, etc.)")
    format_parser.set_defaults(fn=cmd_format)

    # Add gate command for SARIF severity budgets
    gate_parser = sub.add_parser("gate", help="Enforce SARIF severity budgets for CI")
    gate_parser.add_argument("sarif_file", help="Path to SARIF file to check")
    gate_parser.add_argument("--max-errors", type=int, default=0, help="Maximum allowed errors")
    gate_parser.add_argument("--max-warnings", type=int, default=10, help="Maximum allowed warnings")
    gate_parser.add_argument("--max-notes", type=int, default=50, help="Maximum allowed notes")
    gate_parser.add_argument("--config", help="Path to gate configuration file")
    gate_parser.set_defaults(fn=cmd_gate)

    rd = sub.add_parser("reduce"); rd.add_argument("input"); rd.add_argument("test_cmd")
    rd.add_argument("--timeout", type=int, help="Timeout for the reduction process")
    rd.add_argument("--sanitizer", help="Sanitizer to use during testing")
    rd.set_defaults(fn=cmd_reduce)

    sub.add_parser("docs").set_defaults(fn=cmd_docs)
    sub.add_parser("capabilities").set_defaults(fn=cmd_capabilities)

    init_parser = sub.add_parser("init", help="Initialize LLM-optimized C++ project")
    init_parser.add_argument("name", nargs="?", help="Project name (uses current directory if not specified)")
    init_parser.add_argument("--force", action="store_true", help="Overwrite existing CMakeLists.txt")
    init_parser.add_argument("--existing", action="store_true", help="Adopt an existing project without overwriting CMake files")

    # Customization options
    init_parser.add_argument("--std", choices=["17", "20", "23", "26"], default="23", help="C++ standard version")
    init_parser.add_argument("--cmake-min", default="3.28", help="Minimum CMake version required")
    init_parser.add_argument("--pic", action="store_true", help="Enable position independent code by default")
    init_parser.add_argument("--no-sanitizers", action="store_true", help="Disable sanitizer options")
    # Get available presets dynamically
    available_presets = get_available_presets()
    init_parser.add_argument("--preset", choices=available_presets, default="full",
                           help=f"Project template preset. Available: {', '.join(available_presets)}")

    # Add toggle arguments for fine-grained control
    init_parser.add_argument("--no-rtti", action="store_true", help="Disable C++ RTTI")
    init_parser.add_argument("--no-exceptions", action="store_true", help="Disable C++ exceptions")
    init_parser.add_argument("--enable-simd", action="store_true", help="Enable SIMD optimizations")
    init_parser.add_argument("--openmp", action="store_true", help="Enable OpenMP support")
    init_parser.add_argument("--static-linking", action="store_true", help="Use static linking")

    init_parser.set_defaults(fn=cmd_init)

    install_parser = sub.add_parser("install", help="Install missing tools")
    install_parser.add_argument("--local", action="store_true", help="Force local installation (no sudo)")
    install_parser.add_argument("tools", nargs="*", help="Specific tools to install")
    install_parser.set_defaults(fn=cmd_install)

    cache_parser = sub.add_parser("cache", help="Manage analysis cache")
    cache_subparsers = cache_parser.add_subparsers(dest="cache_cmd", required=True)
    cache_subparsers.add_parser("clear", help="Clear the analysis cache").set_defaults(fn=cmd_cache)
    cache_subparsers.add_parser("show", help="Show cache contents").set_defaults(fn=cmd_cache)

    args = ap.parse_args()
    # map nested subcommand
    if getattr(args, "sub", None):
        return args.fn(args)
    return args.fn(args)

if __name__ == "__main__":
    sys.exit(main())
